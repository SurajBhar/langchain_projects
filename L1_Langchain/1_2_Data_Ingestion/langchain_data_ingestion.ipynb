{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion- Document Loaders\n",
    "\n",
    "- Checkout the DocumentLoader Documentation from Langchain\n",
    "- (https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x7f9ad0771c30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "# Load the text file\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load entire speech.txt into one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='A text file (sometimes spelled textfile; \\nan old alternative name is flat file) is a kind of computer file \\nthat is structured as a sequence of lines of electronic text. \\nA text file exists stored as data within a computer file system.\\n\\nIn operating systems such as CP/M, where the operating system does \\nnot keep track of the file size in bytes, the end of a text file is \\ndenoted by placing one or more special characters, \\nknown as an end-of-file (EOF) marker, as padding after the last \\nline in a text file. In modern operating systems such as DOS, \\nMicrosoft Windows and Unix-like systems, \\ntext files do not contain any special EOF character, \\nbecause file systems on those operating systems \\nkeep track of the file size in bytes.\\n\\nSome operating systems, such as Multics, Unix-like systems, \\nCP/M, DOS, the classic Mac OS, and Windows, \\nstore text files as a sequence of bytes, \\nwith an end-of-line delimiter at the end of each line. \\nOther operating systems, such as OpenVMS and OS/360 \\nand its successors, have record-oriented filesystems, \\nin which text files are stored as a sequence either of \\nfixed-length records or of variable-length records with a \\nrecord-length value in the record header.\\n\\n\"Text file\" refers to a type of container, \\nwhile plain text refers to a type of content.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a text document using load function\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Raptor.pdf', 'page': 0}, page_content='Published as a conference paper at ICLR 2024\\nRAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING\\nFOR TREE -ORGANIZED RETRIEVAL\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning\\nStanford University\\npsarthi@cs.stanford.edu\\nABSTRACT\\nRetrieval-augmented language models can better adapt to changes in world state\\nand incorporate long-tail knowledge. However, most existing methods retrieve\\nonly short contiguous chunks from a retrieval corpus, limiting holistic under-\\nstanding of the overall document context. We introduce the novel approach of\\nrecursively embedding, clustering, and summarizing chunks of text, constructing\\na tree with differing levels of summarization from the bottom up. At inference\\ntime, our RAPTOR model retrieves from this tree, integrating information across\\nlengthy documents at different levels of abstraction. Controlled experiments show\\nthat retrieval with recursive summaries offers significant improvements over tra-\\nditional retrieval-augmented LMs on several tasks. On question-answering tasks\\nthat involve complex, multi-step reasoning, we show state-of-the-art results; for\\nexample, by coupling RAPTOR retrieval with the use of GPT-4, we can improve\\nthe best performance on the QuALITY benchmark by 20% in absolute accuracy.\\n1 I NTRODUCTION\\nLarge Language Models (LLMs) have emerged as transformative tools showing impressive perfor-\\nmance on many tasks. With the growing size of LLMs, they can serve standalone as very effective\\nknowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al., 2020;\\nTalmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al.,\\n2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream\\ntasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain-\\nspecific knowledge for particular tasks and the world continues to change, invalidating facts in the\\nLLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,\\nparticularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-\\nnative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\\nquestion as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,\\n2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to\\nsome domain and enabling easy interpretability and provenance tracking, whereas the parametric\\nknowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).\\nNevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that\\nmost existing methods retrieve only a few short, contiguous text chunks, which limits their ability\\nto represent and leverage large-scale discourse structure. This is particularly relevant for thematic\\nquestions that require integrating knowledge from multiple parts of a text, such as understanding\\nan entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tale of\\nCinderella, and the question “How did Cinderella reach her happy ending?”. The top- k retrieved\\nshort contiguous texts will not contain enough context to answer the question.\\nTo address this, we design an indexing and retrieval system that uses a tree structure to capture both\\nhigh-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters\\nchunks of text, generates text summaries of those clusters, and then repeats, generating a tree from\\nthe bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing\\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\\n1\\narXiv:2401.18059v1  [cs.CL]  31 Jan 2024'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 1}, page_content='Published as a conference paper at ICLR 2024\\n2\\n 3\\n 4\\n 5\\n1\\n1\\n 2\\n3\\n 3\\n4\\n 5\\n5\\n6\\n 8\\n7\\n Index #8\\nText:  summary of \\nnodes 2 and 3\\nChild Nodes: 2, 3\\nText Embedding\\nText chunks\\n3\\n.1\\n4\\n.1\\n5\\n2. Summarization \\nby LLM\\n1. Clustering\\n10\\n7\\n1\\n 2\\n8\\n4\\n3\\n 5\\n6\\n9\\nFormation of one tree layer\\nRoot layer\\nLeaf layer\\nContents of a nodeRAPTOR Tree \\nFigure 1: Tree construction process: RAPTOR recursively clusters chunks of text based on their\\nvector embeddings and generates text summaries of those clusters, constructing a tree from the\\nbottom up. Nodes clustered together are siblings; a parent node contains the text summary of that\\ncluster.\\nOur main contribution is the idea of using text summarization to allow retrieval augmentation of\\ncontext at different scales, and to show its effectiveness in experiments on collections of long doc-\\numents. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),\\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current\\nretrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni-\\nfiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books\\nand movies (NarrativeQA, Koˇcisk`y et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),\\nand multiple-choice questions based on medium-length passages (QuALITY , Pang et al. 2022).1\\n2 R ELATED WORK\\nWhy Retrieval? Recent advances in hardware and algorithms have indeed expanded the con-\\ntext lengths that models can handle, leading to questions about the need for retrieval systems (Dai\\net al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)\\nhave noted, models tend to underutilize long-range context and see diminishing performance as con-\\ntext length increases, especially when pertinent information is embedded within a lengthy context.\\nMoreover, practically, use of long contexts is expensive and slow. This suggests that selecting the\\nmost relevant information for knowledge-intensive tasks is still crucial.\\nRetrieval Methods Retrieval-augmented language models (RALMs) have seen improvements in\\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\\nhave transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and\\nBM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin\\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\\n2022; Sun et al., 2022). Research on the reader component includes Fusion-in-Decoder (FiD)\\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\\nindependently in the encoder andRETRO (Borgeaud et al., 2022; Wang et al., 2023), which utilizes\\ncross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.\\nEnd-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-\\ndecoder model in conjunction with the retriever;REALM (Guu et al., 2020), a bidirectional, masked\\nLM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-\\ntion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\\nretriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-\\ndecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi-\\nerarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements\\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).\\n1We will release the code of RAPTOR publicly here.\\n2'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 2}, page_content='Published as a conference paper at ICLR 2024\\nDespite a diversity in methods, the retrieving components of models predominantly rely on stan-\\ndard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this\\napproach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous seg-\\nmentation might not capture the complete semantic depth of the text. Reading extracted snippets\\nfrom technical or scientific documents may lack important context making them difficult to read or\\neven misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).\\nRecursive summarization as Context Summarization techniques provide a condensed view of\\ndocuments, enabling more focused engagement with the content (Angelidis & Lapata, 2018). The\\nsummarization/snippet model by Gao et al. (2023) uses summarizations and snippets of passages,\\nwhich improves correctness on most datasets but can sometimes be a lossy means of compression.\\nThe recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition\\nto summarize smaller text chunks, which are later integrated to form summaries of larger sections.\\nWhile this method is effective for capturing broader themes, it can miss granular details. LlamaIndex\\n(Liu, 2022) mitigates this issue by similarly summarizing adjacent text chunks but also retaining\\nintermediate nodes thus storing varying levels of detail, keeping granular details. However, both\\nmethods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still\\noverlook distant interdependencies within the text, which we can find and group with RAPTOR.\\n3 M ETHODS\\nOverview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi-\\ncal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic\\ndepth and connection in reading by building a recursive tree structure that balances broader thematic\\ncomprehension with granular details and which allows nodes to be grouped based on semantic sim-\\nilarity not just order in the text.\\nConstruction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous\\ntexts of length 100, similar to traditional retrieval augmentation techniques. If a sentence exceeds the\\n100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence.\\nThis preserves the contextual and semantic coherence of the text within each chunk. These texts\\nare then embedded using SBERT, a BERT-based encoder ( multi-qa-mpnet-base-cos-v1)\\n(Reimers & Gurevych, 2019). The chunks and their corresponding SBERT embeddings form the\\nleaf nodes of our tree structure.\\nTo group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\\nresulting in a structured, multi-layered tree representation of the original documents. An important\\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\\ncomprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A.\\nFor querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree.\\nThe tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant\\nnodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find\\nthe most relevant ones.\\nClustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text\\nsegments into cohesive groups. This step groups related content together, which helps the subse-\\nquent retrieval process.\\nOne of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\\nbelong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen-\\ntial because individual text segments often contain information relevant to various topics, thereby\\nwarranting their inclusion in multiple summaries.\\nOur clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers\\nboth flexibility and a probabilistic framework. GMMs assume that data points are generated from a\\nmixture of several Gaussian distributions.\\n3'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 3}, page_content='Published as a conference paper at ICLR 2024\\nGiven a set of N text segments, each represented as a d-dimensional dense vector embedding, the\\nlikelihood of a text vector, x, given its membership in the kth Gaussian distribution, is denoted by\\nP(x|k) = N(x; µk, Σk). The overall probability distribution is a weighted combination P(x) =PK\\nk=1 πkN(x; µk, Σk), where πk signifies the mixture weight for the kth Gaussian distribution.\\nThe high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-\\ntance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-\\ngarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection\\n(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The\\nnumber of nearest neighbors parameter, n neighbors, in UMAP determines the balance between\\nthe preservation of local and global structures. Our algorithm varies n neighbors to create a hierar-\\nchical clustering structure: it first identifies global clusters and then performs local clustering within\\nthese global clusters. This two-step clustering process captures a broad spectrum of relationships\\namong the text data, from broad themes to specific details.\\nShould a local cluster’s combined context ever exceed the summarization model’s token threshold,\\nour algorithm recursively applies clustering within the cluster, ensuring that the context remains\\nwithin the token threshold.\\nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\\nfor model selection. BIC not only penalizes model complexity but also rewards goodness of fit\\n(Schwarz, 1978). The BIC for a given GMM is BIC = ln(N)k − 2 ln(ˆL), where N is the number\\nof text segments (or data points), k is the number of model parameters, and ˆL is the maximized\\nvalue of the likelihood function of the model. In the context of GMM, the number of parameters k\\nis a function of the dimensionality of the input vectors and the number of clusters.\\nWith the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm\\nis then used to estimate the GMM parameters, namely the means, covariances, and mixture weights.\\nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\\noften exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\\neffective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\\ncontiguous chunks and provide details in Appendix B.\\nModel-Based Summarization After clustering the nodes using Gaussian Mixture Models, the\\nnodes in each cluster are sent to a language model for summarization. This step allows the model\\nto transform large chunks of text into concise, coherent summaries of the selected nodes. For our\\nexperiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-\\ndenses the potentially large volume of retrieved information into a manageable size. We provide\\nstatistics on the compression due to the summarization in Appendix C and the prompt used for\\nsummarization in Appendix D.\\nWhile the summarization model generally produces reliable summaries, a focused annotation study\\nrevealed that about 4% of the summaries contained minor hallucinations. These did not propagate\\nto parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis\\nof hallucinations, refer to the appendix E.\\nQuerying In this section, we elaborate on the two querying mechanisms employed by RAPTOR:\\ntree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered\\nRAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We\\nprovide the pseudocode of both methods in Appendix F. Note that we embed all nodes using SBERT.\\nThe tree traversal method first selects the top-k most relevant root nodes based on their cosine\\nsimilarity to the query embedding. The children of these selected nodes are considered at the next\\nlayer and the top-k nodes are selected from this pool again based on their cosine similarity to the\\nquery vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected\\nnodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:\\n1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the\\nquery embedding and the embeddings of all nodes present at this initial layer.\\n2. Choose the top- k nodes based on the highest cosine similarity scores, forming the set S1.\\n4'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 4}, page_content='Published as a conference paper at ICLR 2024\\nFigure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms.Tree traver-\\nsal starts at the root level of the tree and retrieves the top- k (here, top-1) node(s) based on cosine\\nsimilarity to the query vector. At each level, it retrieves the top- k node(s) from the child nodes of\\nthe previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The\\nnodes on which cosine similarity search is performed are highlighted in both illustrations.\\n3. Proceed to the child nodes of the elements in setS1. Compute the cosine similarity between\\nthe query vector and the vector embeddings of these child nodes.\\n4. Select the top k child nodes with the highest cosine similarity scores to the query, forming\\nthe set S2.\\n5. Continue this process recursively for d layers, producing sets S1, S2, . . . , Sd.\\n6. Concatenate sets S1 through Sd to assemble the relevant context to the query.\\nBy adjusting the depth d and the number of nodes k selected at each layer, the tree traversal method\\noffers control over the specificity and breadth of the information retrieved. The algorithm starts with\\na broad outlook by considering the top layers of the tree and progressively focuses on finer details\\nas it descends through the lower layers.\\nThe collapsed tree approach offers a simpler way to search for relevant information by considering\\nall nodes in the tree simultaneously, as depicted in Figure 2. Instead of going layer-by-layer, this\\nmethod flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\\nsame level for comparison. The steps for this method are outlined below:\\n1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted\\nas C, contains nodes from every layer of the original tree.\\n2. Next, calculate the cosine similarity between the query embedding and the embeddings of\\nall nodes present in the collapsed set C.\\n3. Finally, pick the top- k nodes that have the highest cosine similarity scores with the query.\\nKeep adding nodes to the result set until you reach a predefined maximum number of\\ntokens, ensuring you don’t exceed the model’s input limitations.\\nWe tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance\\nof tree traversal with different top- sizes and collapsed tree with different maximum token numbers.\\nThe collapsed tree approach consistently performs better. We believe collapsed tree retrieval is\\nbetter due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes\\nsimultaneously, it retrieves information that is at the correct level of granularity for a given question.\\nIn comparison, while using tree traversal with the same values of d and k, the ratio of nodes from\\neach level of the tree will be constant. So, the ratio of higher-order thematic information to granular\\ndetails will remain the same regardless of the question.\\n5'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 5}, page_content='Published as a conference paper at ICLR 2024\\nOne drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\\nbe performed on all nodes in the tree. However, this can be made more efficient with fast k-nearest\\nneighbor libraries such as FAISS (Johnson et al., 2019).\\nFigure 3: Comparison of querying methods.\\nResults on 20 stories from the QASPER dataset\\nusing tree traversal with different top-k values,\\nand collapsed tree with different context lengths.\\nCollapsed tree with 2000 tokens produces the best\\nresults, so we use this querying strategy for our\\nmain results.\\nOverall, given the collapsed tree approach’s\\ngreater flexibility and its superior performance\\non the subset of the QASPER dataset, this is\\nthe querying approach with which we proceed.\\nSpecifically, we use the collapsed tree with\\n2000 maximum tokens, which approximately\\nequates to retrieving the top-20 nodes. Using a\\ntoken-based approach ensures the context does\\nnot exceed model context constraints as token\\ncounts can vary across nodes. For experiments\\nwith the UnifiedQA model, we provide 400 to-\\nkens of context, as UnifiedQA has a max con-\\ntext length of 512 tokens. We provide the same\\namount of tokens of context to RAPTOR and to\\nthe baselines.\\nQualitative Study We conduct a qualitative\\nanalysis to understand the benefits of RAP-\\nTOR’s retrieval process compared to Dense\\nPassage Retrieval (DPR) methods. Our study\\nfocuses on thematic, multi-hop questions using\\na 1500-word Cinderella fairytale. As illustrated\\nin Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers,\\nmatching the question’s detail level. This approach often yields more relevant and comprehensive\\ninformation for downstream tasks than DPR. For a detailed discussion and examples, including the\\ntext retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\\n4 E XPERIMENTS\\nDatasets We measure RAPTOR’s performance across three question-answering datasets: Narra-\\ntiveQA, QASPER, and QuALITY .\\nNarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\\nand movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al., 2018; Wu et al., 2021). The\\nNarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\\nto accurately answer its questions, thus testing the model’s ability to comprehend longer texts in\\nthe literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\\nROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the Narra-\\ntiveQA evaluation script used in our experiments.\\nThe QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing\\nfor information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER\\nare categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive. Accuracy is\\nmeasured using standard F1.\\nLastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context\\npassages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for\\nreasoning over the entire document for QA tasks, enabling us to measure the performance of our re-\\ntrieval system on medium-length documents. The dataset includes a challenging subset, QuALITY-\\nHARD, which contains questions that a majority of human annotators answered incorrectly in a\\nspeed-setting. We report accuracies for both the entire test set and the HARD subset.\\nControlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA\\n3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree\\nstructure, on three datasets: QASPER, NarrativeQA, and QuALITY . As shown in Tables 1 and 2,\\n6'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 6}, page_content='Published as a conference paper at ICLR 2024\\nFigure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions\\nabout the Cinderella story: “What is the central theme of the story?” and “How did Cinderella find\\na happy ending?”. Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s\\nleaf nodes. Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either\\ndirectly or within higher-layer summaries.\\nour results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms\\nthe respective retriever across all datasets. 2\\nSince RAPTOR with SBERT has the best performance, we use it in all subsequent experiments.\\nWe now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and\\nUnifiedQA. As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all\\nthree Language Models on the QASPER dataset. RAPTOR’s F-1 Match scores are 53.1%, 55.7%,\\nand 36.6% when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by\\nmargins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the respective\\nLLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that\\nRAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract\\nthe top-k most similar raw chunks of text, which may not contain the correct response in isolation.\\nTable 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of\\nvarious retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\\ndataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-\\nspective retrieval method.\\nModel ROUGE BLEU-1 BLEU-4 METEOR\\nSBERT with RAPTOR 30.87% 23.50% 6.42% 19.20%\\nSBERT without RAPTOR 29.26% 22.56% 5.95% 18.15%\\nBM25 with RAPTOR 27.93% 21.17% 5.70% 17.03%\\nBM25 without RAPTOR 23.52% 17.73% 4.65% 13.98%\\nDPR with RAPTOR 30.94% 23.51% 6.45% 19.05%\\nDPR without RAPTOR 29.56% 22.84% 6.12% 18.44%\\nLikewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62.4%,\\nwhich is a 2% and 5.1% improvement over DPR and BM25. Similar trends are observed when Uni-\\nfiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2.7% and 6.7%, respectively.\\nFinally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met-\\nrics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other\\nmetrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins\\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\\nthe performance observed in Karpukhin et al. (2020), wheredpr-multiset-base showed superior results.\\n7'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 7}, page_content='Published as a conference paper at ICLR 2024\\nTable 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\\nparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\\nforms baselines of each respective retrieval method for both datasets.\\nModel Accuracy (QuALITY) Answer F1 (QASPER)\\nSBERT with RAPTOR 56.6% 36.70%\\nSBERT without RAPTOR 54.9% 36.23%\\nBM25 with RAPTOR 52.1% 27.00%\\nBM25 without RAPTOR 49.9% 26.47%\\nDPR with RAPTOR 54.7% 32.23%\\nDPR without RAPTOR 53.1% 31.70%\\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan-\\nguage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title +\\nAbstract” reflects performance when only the title and abstract of the papers are used for context.\\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\\nSpecifically, RAPTOR’s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\\nhigher than BM25.\\nRetriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\\nTitle + Abstract 25.2 22.2 17.5\\nBM25 46.6 50.2 26.4\\nDPR 51.3 53.0 32.1\\nRAPTOR 53.1 55.7 36.6\\nTable 4: Comparison of accuracies on the QuAL-\\nITY dev dataset for two different language mod-\\nels (GPT-3, UnifiedQA 3B) using various retrieval\\nmethods. RAPTOR outperforms the baselines of\\nBM25 and DPR by at least 2.0% in accuracy.\\nModel GPT-3 Acc. UnifiedQA Acc.\\nBM25 57.3 49.9\\nDPR 60.4 53.9\\nRAPTOR 62.4 56.6\\nTable 5: Results on F-1 Match scores of various\\nmodels on the QASPER dataset.\\nModel F-1 Match\\nLongT5 XL (Guo et al., 2022) 53.1\\nCoLT5 XL (Ainslie et al., 2023) 53.9\\nRAPTOR + GPT-4 55.7\\nComparison to State-of-the-art Systems\\nBuilding upon our controlled comparisons,\\nwe examine RAPTOR’s performance relative\\nto other state-of-the-art models. As shown\\nin Table 5, RAPTOR with GPT-4 sets a new\\nbenchmark on QASPER, with a 55.7% F-1\\nscore, surpassing the CoLT5 XL’s score of\\n53.9%.\\nIn the QuALITY dataset, as shown in Table 7,\\nRAPTOR paired with GPT-4 sets a new state-\\nof-the-art with an accuracy of 82.6%, surpass-\\ning the previous best result of 62.3%. In par-\\nticular, it outperforms CoLISA by 21.5% on\\nQuALITY-HARD, which represents questions\\nthat humans took unusually long to correctly\\nanswer, requiring rereading parts of the text,\\ndifficult reasoning, or both.\\nFor the NarrativeQA dataset, as represented in\\nTable 6, RAPTOR paired with UnifiedQA sets\\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by\\nWu et al. (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While\\nWu et al. (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR\\nbenefits from its intermediate layers and clustering approaches, which allows it to capture a range of\\ninformation, from general themes to specific details, contributing to its overall strong performance.\\n4.1 C ONTRIBUTION OF THE TREE STRUCTURE\\nWe examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy-\\npothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring\\na broader understanding of the text.\\n8'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 8}, page_content='Published as a conference paper at ICLR 2024\\nTable 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing\\non four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with Uni-\\nfiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of-\\nthe-art in the METEOR metric.\\nModel ROUGE-L BLEU-1 BLEU-4 METEOR\\nBiDAF (Koˇcisk`y et al., 2018) 6.2 5 .7 0 .3 3 .7\\nBM25 + BERT (Mou et al., 2020) 15.5 14 .5 1 .4 5 .0\\nRecursively Summarizing Books (Wu et al., 2021) 21.6 22 .3 4 .2 10 .6\\nRetriever + Reader (Izacard & Grave, 2022) 32.0 35.3 7.5 11.1\\nRAPTOR + UnifiedQA 30.8 23.5 6.4 19.1\\nTable 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging\\nhard subset. GPT-4 with RAPTOR sets a new state-of-the-art.\\nModel Accuracy\\nTest Set Hard Subset\\nLongformer-base (Beltagy et al., 2020) 39.5 35 .3\\nDPR and DeBERTaV3-large (Pang et al., 2022) 55.4 46 .1\\nCoLISA (DeBERTaV3-large) (Dong et al., 2023a) 62.3 54 .7\\nRAPTOR + GPT-4 82.6 76.2\\nTable 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL-\\nITY dataset. Columns represent different starting points (highest layer) and rows represent different\\nnumbers of layers queried.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\\n1 layer 57.9 57.8 57.9\\n2 layers - 52.6 63.15\\n3 layers - - 73.68\\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in\\nappendix G. To quantitatively understand the contribution of the upper-level nodes, we used stories\\nfrom the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in\\nSection 3. However, during retrieval, we limit the search to different subsets of layers. For example,\\nwe exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous\\nsubsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree\\nsearch, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.\\nThese findings highlight the importance of the full tree structure in RAPTOR. By providing both\\nthe original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider\\nrange of questions, from higher-order thematic queries to detail-oriented questions. Detailed results\\nfor additional stories and an ablation study on layer contributions can be found in Appendix I.\\n5 C ONCLUSION\\nIn this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\\nparametric knowledge of large language models with contextual information at various levels of\\nabstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\\nhierarchical tree structure that is capable of synthesizing information across various sections of the\\nretrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\\nretrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\\nretrieval methods but also sets new performance benchmarks on several question-answering tasks.\\n9'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 9}, page_content='Published as a conference paper at ICLR 2024\\n6 R EPRODUCIBILITY STATEMENT\\nLanguage Models for QA and Summarization Four language models are used in our RAPTOR\\nexperiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3,\\ngpt-4, and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,\\nwhich is used for QA tasks, is publicly available at Hugging Face.\\nEvaluation Datasets The three evaluation datasets used in our experiments—QuALITY,\\nQASPER, and NarrativeQA—are all publicly accessible. These datasets ensure that the retrieval\\nand QA tests conducted in this study can be replicated.\\nSource Code The source code for RAPTOR will be publicly available here.\\nREFERENCES\\nCharu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Dis-\\ntance Metrics in High Dimensional Space. In Database Theory—ICDT 2001: 8th International\\nConference London, UK, January 4–6, 2001 Proceedings 8, pp. 420–434. Springer, 2001. URL\\nhttps://link.springer.com/chapter/10.1007/3-540-44503-x_27 .\\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyan-\\nskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range\\ntransformers with conditional computation. arXiv preprint arXiv:2303.09752, 2023. URL\\nhttps://arxiv.org/abs/2303.09752.\\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\\nKelvin Guu. Towards tracing knowledge in language models back to the training data. In\\nFindings of the Association for Computational Linguistics: EMNLP 2022, pp. 2429–2446,\\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.\\nfindings-emnlp.180.\\nStefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment\\nprediction and they are both weakly supervised. arXiv preprint arXiv:1808.08858, 2018. URL\\nhttps://arxiv.org/abs/1808.08858.\\nManoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng\\nHuang. Hybrid hierarchical retrieval for open-domain question answering. In Anna Rogers,\\nJordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational\\nLinguistics: ACL 2023, pp. 10680–10689, Toronto, Canada, July 2023. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.\\norg/2023.findings-acl.679.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer,\\n2020. URL https://arxiv.org/abs/2004.05150. arXiv preprint arXiv:2004.05150.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\\nImproving language models by retrieving from trillions of tokens. In International conference on\\nmachine learning, pp. 2206–2240. PMLR, 2022. URL https://arxiv.org/abs/2112.\\n04426.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\\nZiegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\\nford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In\\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\\nral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc.,\\n10'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 10}, page_content='Published as a conference paper at ICLR 2024\\n2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/\\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\nS´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General\\nIntelligence: Early Experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. URL\\nhttps://arxiv.org/abs/2303.12712.\\nShuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long\\ndocument summarization. In Proceedings of the 60th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pp. 786–807, Dublin, Ireland, May 2022.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https:\\n//aclanthology.org/2022.acl-long.58.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pp. 1870–1879, Vancouver, Canada, July\\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:\\n//aclanthology.org/P17-1171.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\\nScaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022. URL\\nhttps://arxiv.org/abs/2204.02311.\\nArman Cohan and Nazli Goharian. Contextualizing citations for scientific summarization using\\nword embeddings and domain knowledge. In Proceedings of the 40th International ACM SIGIR\\nConference on Research and Development in Information Retrieval, pp. 1133–1136, 2017. URL\\nhttps://dl.acm.org/doi/abs/10.1145/3077136.3080740.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\\nTransformer-XL: Attentive language models beyond a fixed-length context. InProceedings of the\\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence,\\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\\nhttps://aclanthology.org/P19-1285.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-Awareness.Advances in Neural Information Processing\\nSystems, 35:16344–16359, 2022. URL https://arxiv.org/abs/2205.14135.\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset\\nof Information-Seeking Questions and Answers Anchored in Research Papers. In Proceed-\\nings of the 2021 Conference of the North American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies, pp. 4599–4610, Online, June 2021. Asso-\\nciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https:\\n//aclanthology.org/2021.naacl-main.365.\\nMengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive\\nLearning for Multi-choice Reading Comprehension. In Advances in Information Retrieval: 45th\\nEuropean Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023,\\nProceedings, Part I, pp. 264–278. Springer, 2023a. URL https://link.springer.com/\\nchapter/10.1007/978-3-031-28244-7_17 .\\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with\\ntransformers. arXiv preprint arXiv:2302.14502, 2023b. URL https://arxiv.org/abs/\\n2302.14502.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/\\nabs/2305.14627.\\n11'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 11}, page_content='Published as a conference paper at ICLR 2024\\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\\nYinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the\\nAssociation for Computational Linguistics: NAACL 2022, pp. 724–736, Seattle, United States,\\nJuly 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.\\nURL https://aclanthology.org/2022.findings-naacl.55.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented\\nLanguage Model Pre-Training. In International conference on machine learning, pp. 3929–3938.\\nPMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL\\nhttps://arxiv.org/abs/2203.15556.\\nGautier Izacard and Edouard Grave. Distilling Knowledge from Reader to Retriever for Ques-\\ntion Answering, 2022. URL https://arxiv.org/abs/2012.04584. arXiv preprint\\narXiv:2012.04584.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. URL https:\\n//arxiv.org/abs/2208.03299.\\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\\nmodels know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.\\nURL https://arxiv.org/abs/1911.12543.\\nJeff Johnson, Matthijs Douze, and Herv´e J´egou. Billion-Scale Similarity Search with GPUs. IEEE\\nTransactions on Big Data, 7(3):535–547, 2019. URL https://arxiv.org/abs/1702.\\n08734.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\\nModels struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-\\ning, pp. 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/\\nkandpal23a/kandpal23a.pdf.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In\\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP), pp. 6769–6781, Online, November 2020. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\\nemnlp-main.550.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\\nHannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907,\\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\\nfindings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.\\n171.\\nOmar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via con-\\ntextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR\\nconference on research and development in Information Retrieval, pp. 39–48, 2020. URL\\nhttps://arxiv.org/abs/2004.12832.\\nTom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´abor Melis,\\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions\\nof the Association for Computational Linguistics, 6:317–328, 2018. URL https://arxiv.\\norg/abs/1712.07040.\\n12'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 12}, page_content='Published as a conference paper at ICLR 2024\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-Augmented Gener-\\nation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems,\\n33:9459–9474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401.\\nJerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index.\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\\narXiv:2307.03172, 2023. URL https://arxiv.org/abs/2307.03172.\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense\\nhierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing\\nHuang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2021, pp. 188–200, Punta Cana, Dominican Republic, Novem-\\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19.\\nURL https://aclanthology.org/2021.findings-emnlp.19.\\nLeland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation\\nand Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.\\n03426. arXiv preprint arXiv:1802.03426.\\nSewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint\\npassage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang,\\nLucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing, pp. 6997–7008, Online and Punta Cana, Dominican\\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\\nemnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560.\\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\\nZettlemoyer. Nonparametric masked language modeling. In Findings of the Association for\\nComputational Linguistics: ACL 2023, pp. 2097–2118, Toronto, Canada, July 2023. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:\\n//aclanthology.org/2023.findings-acl.132.\\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.\\nMemory-based model editing at scale. In International Conference on Machine Learning,\\npp. 15817–15831. PMLR, 2022. URL https://proceedings.mlr.press/v162/\\nmitchell22a/mitchell22a.pdf.\\nXiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui\\nSu. Frustratingly hard evidence retrieval for QA over books. In Proceedings of the First Joint\\nWorkshop on Narrative Understanding, Storylines, and Events, pp. 108–113, Online, July 2020.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:\\n//aclanthology.org/2020.nuse-1.13.\\nInderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr-\\nishna Karanam, and Sumit Shekhar. A neural CRF-based hierarchical approach for lin-\\near text segmentation. In Findings of the Association for Computational Linguistics: EACL\\n2023, pp. 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.\\nfindings-eacl.65.\\nBenjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa-\\nbased framework for decontextualization.arXiv preprint arXiv:2305.14772, 2023. URL https:\\n//arxiv.org/pdf/2305.14772.pdf.\\nOpenAI. GPT-4 Technical Report. ArXiv, abs/2303.08774, 2023. URL https://arxiv.org/\\nabs/2303.08774.\\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\\nVishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\\n13'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 13}, page_content='Published as a conference paper at ICLR 2024\\nQuestion Answering with Long Input Texts, Yes! In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pp. 5336–5358, Seattle, United States, July 2022. Association for Computational\\nLinguistics. URL https://aclanthology.org/2022.naacl-main.391.\\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,\\n2019. URL https://arxiv.org/abs/1909.01066.\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\\nMethods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446, 2021.\\nURL https://arxiv.org/abs/2112.11446.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\\nBrown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint\\narXiv:2302.00083, 2023. URL https://arxiv.org/abs/2302.00083.\\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\\nD19-1410.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into\\nthe Parameters of a Language Model? In Proceedings of the 2020 Conference on Empir-\\nical Methods in Natural Language Processing (EMNLP), pp. 5418–5426, Online, November\\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\\nhttps://aclanthology.org/2020.emnlp-main.437.\\nStephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\\nBeyond. Foundations and Trends in Information Retrieval, 3(4):333–389, 2009. URL https:\\n//doi.org/10.1561/1500000019.\\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\\net al. Okapi at TREC-3. Nist Special Publication Sp, 109:109, 1995. URL https://www.\\nmicrosoft.com/en-us/research/publication/okapi-at-trec-3/ .\\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\\nZaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\\nsociation for Computational Linguistics, 11:600–616, 2023. doi: 10.1162/tacl a 00564. URL\\nhttps://aclanthology.org/2023.tacl-1.35.\\nGideon Schwarz. Estimating the Dimension of a Model. The annals of statistics, pp. 461–464,\\n1978. URL https://projecteuclid.org/journals/annals-of-statistics/\\nvolume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\\naos/1176344136.full.\\nKaren Sp ¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\\ntrieval. Journal of documentation, 28(1):11–21, 1972. URL https://doi.org/10.1108/\\neb026526.\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\\nmodels actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia\\nSpecia, and Scott Wen-tau Yih (eds.),Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, pp. 807–822, Online and Punta Cana, Dominican Republic,\\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\\n62. URL https://aclanthology.org/2021.emnlp-main.62.\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\\nmodels. arXiv preprint arXiv:2210.01296, 2022. URL https://arxiv.org/abs/2210.\\n01296.\\n14'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 14}, page_content='Published as a conference paper at ICLR 2024\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics– on what language\\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\\n743–758, 2020. URL https://arxiv.org/abs/1912.13283.\\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:\\n//arxiv.org/abs/2304.06762.\\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\\nChristiano. Recursively Summarizing Books with Human Feedback, 2021. URL https:\\n//arxiv.org/abs/2109.10862.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,\\nand Quoc V . Le. QANet: Combining Local Convolution with Global Self-Attention for Read-\\ning Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint\\narXiv:1804.09541.\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang\\nZhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are\\nstrong context generators, 2022. URL https://arxiv.org/abs/2209.10063.\\nShiyue Zhang, David Wan, and Mohit Bansal. Extractive is not faithful: An investigation of\\nbroad unfaithfulness problems in extractive summarization. In Anna Rogers, Jordan Boyd-\\nGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), pp. 2153–2174, Toronto, Canada, July\\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL\\nhttps://aclanthology.org/2023.acl-long.120.\\nA S CALABILITY AND COMPUTATIONAL EFFICIENCY OF THE\\nTREE -BUILDING PROCESS\\nTo assess the computational efficiency and cost-effectiveness of RAPTOR’s tree-building process,\\nwe conducted experiments on a consumer-grade laptop, specifically an Apple M1 Mac with 16GB\\nof RAM. These experiments aimed to demonstrate the scalability and feasibility of RAPTOR on\\ntypical hardware. We varied the context length from 12,500 to 78,000 tokens and measured both the\\ntoken expenditure and the time required to complete the tree-building process, from initial splitting\\nand embedding to the construction of the final root node.\\nFigure 5: Token cost as a function of document length for QASPER, NarrativeQA, and QuALITY .\\nRAPTOR tree construction costs scale linearly with document length for each of the datasets.\\nToken Expenditure We empirically investigated the relationship between the initial document\\nlength and the total number of tokens expended during the tree-building process, which includes\\nboth the prompt and completion tokens. The document lengths varied significantly across the three\\n15'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 15}, page_content='Published as a conference paper at ICLR 2024\\ndatasets examined: QuALITY , QASPER, and NarrativeQA. Figure 5 illustrates a clear linear corre-\\nlation between the initial document length and the total token expenditure, emphasizing that RAP-\\nTOR maintains a linear token scaling regardless of document complexity or length.\\nFigure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAP-\\nTOR tree construction time scales linearly with document length for each of the datasets.\\nBuild Time We also empirically observed a consistent linear trend between the document length\\nand the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of\\ntime, making it a viable solution for efficiently processing large corpora of varying lengths.\\nConclusion Overall, our empirical results indicate that RAPTOR scales both in terms of tokens\\nexpended and build time. Even as the complexity and volume of the input text grow, the cost of\\nconstructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa-\\ntionally efficient and well-suited for processing large and diverse corpora.\\nB A BLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR\\nTo assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted\\nan ablation study on the QuALITY dataset. This study compares RAPTOR’s performance with a\\nbalanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard\\nclustering method.\\nB.1 M ETHODOLOGY\\nBoth configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain\\nconsistency in retrieval. For RAPTOR, we employed our typical clustering and summarization\\nprocess. In contrast, the alternative setup involved creating a balanced tree by recursively encoding\\nand summarizing contiguous text chunks. We determined the window size for this setup based on\\nthe average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose\\na window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models.\\nB.2 R ESULTS & DISCUSSION\\nThe results of the ablation study are presented in table 9. The results from this ablation study clearly\\nindicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the\\nrecency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in\\nRAPTOR is more effective in capturing homogeneous content for summarization, thereby enhancing\\nthe overall retrieval performance.\\n16'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 16}, page_content='Published as a conference paper at ICLR 2024\\nTable 9: Ablation study results comparing RAPTOR with a recency-based tree approach\\nConfiguration Accuracy\\nRAPTOR + SBERT embeddings + UnifiedQA 56.6%\\nRecency-based tree + SBERT embeddings + UnifiedQA 55.8%\\nC D ATASET STATISTICS AND COMPRESSION RATIOS\\nThe average ratio of the summary length to the sum of child node lengths across all datasets is 0.28,\\nindicating a 72% compression rate. On average, the summary length is 131 tokens, and the average\\nchild node length is 86 tokens. Below are the detailed statistics for all three datasets:\\nTable 10: Statistics of Average Summary Length and Child Node Length Across Datasets\\nDataset Avg.\\nSummary\\nLength\\n(tokens)\\nAvg. Child\\nNode Text\\nLength\\n(tokens)\\nAvg. # of\\nChild Nodes\\nPer Parent\\nAvg.\\nCompression\\nRatio (%)\\nAll Datasets 131 85.6 6.7 .28\\nQuALITY 124.4 87.9 5.7 .28\\nNarrativeQA 129.7 85.5 6.8 .27\\nQASPER 145.9 86.2 5.7 .35\\nD S UMMARIZATION PROMPT\\nTable 11 shows the prompt used for summarization.\\nTable 11: Prompt for Summarization\\nRole Content\\nsystem You are a Summarizing Text Portal\\nuser Write a summary of the following, including as many key details as\\npossible: {context}:\\nE H ALLUCINATION ANALYSIS\\nTo assess the quality and accuracy of the summarizations within our RAPTOR model, we conducted\\nan analysis focusing on hallucinations in the generated summaries. The summaries were generated\\nby gpt-3.5-turbo and subsequently annotated to quantify the rates of hallucinations, to examine\\nwhether such inaccuracies propagate to parent nodes, and to evaluate their impact on question-\\nanswering (QA) tasks.\\nE.1 M ETHODOLOGY\\nWe randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This\\nsampling strategy provides a broad view of the model’s performance across different contexts. Each\\nnode was annotated by hand, and determined if it contained a hallucination.\\nE.2 F INDINGS\\nOut of the 150 nodes sampled, 4% (6 nodes) contained some form of hallucination. Most commonly,\\nthese hallucinations originated from the model adding minor information possibly from its training\\ndata that was not present in the text being summarized, or from incorrectly extrapolating some\\ninformation when creating the summary.\\n17'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 17}, page_content='Published as a conference paper at ICLR 2024\\nExample:\\nText of the child nodes:\\n”And you will come with me to my people? We may live here among them, and\\nyou will be a great warrior–oh, when Jor dies you may even be chief, for there is\\nnone so mighty as my warrior...”But your father will not permit it–Jor, my father,\\nHigh Chief of the Galus, will not permit it, for like me you are cos-ata-lo. Oh, Co-\\nTan, if we but could!... Bradley noticed that she spoke in English–broken English\\nlike Co-Tan’s but equally appealing.\\nSummary found in the parent of that node:\\nThe protagonist, Bradley, is being asked by Co-Tan to stay with her people and\\nbecome a great warrior, but he refuses and must return to his own country. Tom\\nBillings of Santa Monica arrives and tells them he came to search for a man named\\nBowen J. Tyler, Jr. Ajor, Co-Tan’s sister, is excited about the possibility of going\\nto Tom’s country to see strange and wonderful things...\\nThe hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not\\nexplicitly mention or imply this.\\nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers.\\nGenerally, the hallucinations were minor and did not alter the thematic interpretation of the text.\\nE.3 I MPACT ON QA TASKS\\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-\\ngests that hallucination is not a major concerns for the summarization component in our RAPTOR\\narchitecture.\\nF P SEUDOCODE FOR RETRIEVAL METHODS\\nAlgorithm 1 Tree Traversal Algorithm\\nfunction TRAVERSE TREE (tree, query, k)\\nScurrent ← tree.layer[0]\\nfor layer in range(tree.num layers) do\\ntopk ← []\\nfor node in Scurrent do\\nscore ← dot product(query, node)\\ntop k.append((node, score))\\nend for\\nSlayer ← sorted(top k)[:k].nodes\\nScurrent ← Slayer\\nend for\\nreturn S0 ∪ S1 ∪ S2 ∪ . . .∪ Sk\\nend function\\nG Q UALITATIVE ANALYSIS\\nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions\\nabout a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP-\\nTOR with the context retrieved by Dense Passage Retrieval (DPR). Figure 4 in the main paper details\\nthe retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR\\nselects for each question are highlighted, while the leaf nodes that DPR selects for the same question\\nare indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure.\\nRAPTOR selects nodes from different layers depending on the level of granularity required by the\\n18'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 18}, page_content='Published as a conference paper at ICLR 2024\\nAlgorithm 2 Collapsed Tree Algorithm\\nfunction COLLAPSED TREE (tree, query, k,max tokens)\\ntree ← flatten(tree) ▷ Flatten tree into 1D\\ntop nodes ← []\\nfor node in tree do\\ntop nodes.append((node, dot product(query, node))\\nend for\\ntop nodes ← sorted(top nodes)\\nresult ← []\\ntotal tokens ← 0\\nfor node in top nodes do\\nif total tokens + node.token size < max tokens then\\nresult.append(node)\\nend if\\ntotal tokens ← total tokens + node.token size\\nend for\\nreturn result\\nend function\\nQuestion: What is the central theme of the story?\\nRAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\\nto be glad that he had found the glass slipper.\\nDPR Two mice were turned into footmen; four grasshoppers into white horses. Next, the\\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\\nlace. . . . Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\\nQuestion: How does Cinderella find a happy ending?\\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\\nCinderella must return home before the clock strikes eleven or her dress will turn back\\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it\\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.\\nDPR the clock had struck Eleven. . . The Prince was very much surprised when he missed\\nCinderella again, and leaving the ball, went in search of her. . . Fairy touched Cin-\\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\\nto the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the\\nroom before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”\\nTable 12: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\\nfairytale Cinderella.\\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not\\nincluded in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a\\nsummary from a higher layer.\\n”The first question we examine is “How does Cinderella find a happy ending?”, a multi-hop question\\nbest answered by synthesizing information from various text segments. To control for the language\\nmodel’s potential familiarity with the Cinderella story, we instructed it to rely solely on the retrieved\\ninformation for its answers. Table 13 shows the text retrieved by both RAPTOR and DPR for this\\nquestion. RAPTOR’s context succinctly describes Cinderella’s journey to happiness, while DPR’s\\nleaf nodes primarily focus on her initial transformation. The difference in retrieved information\\n19'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 19}, page_content='Published as a conference paper at ICLR 2024\\nsignificantly impacts downstream tasks. When GPT-4 is provided with RAPTOR’s context, it gen-\\nerates a detailed answer: “Cinderella finds a happy ending when the Prince searches for the owner\\nof the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transform-\\ning Cinderella’s life for the better.” In contrast, using DPR’s context, GPT-4 states: “Based on the\\ngiven context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks\\ninformation about the story’s conclusion.”\\nThe second question we examine is “What is the central theme of the story?”, a thematic question\\nthat requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for\\nthis question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of\\nall the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of\\na narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance\\nof GPT-4 when answering the question. Given DPR’s context, it outputs “The central theme of\\nthe story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is\\nmagically transformed into a beautiful princess, capturing the attention and admiration of the Prince\\nand others at the ball.” This answer only takes into account the first portion of the story, up until\\nCinderella first meets the prince. In contrast, given RAPTOR’s context, GPT-4 outputs “The central\\ntheme of the story is transformation and overcoming adversity, as Cinderella, with the help of her\\nFairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident\\nyoung woman who ultimately finds happiness and love with the Prince.” This is a more complete\\nanswer, demonstrating a comprehensive understanding of the story.\\nThis qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because\\nthe information that it retrieves is more relevant and exhaustive, allowing for better performance on\\ndownstream tasks.\\nWe also created a 2600-word story along with questions about its narrative and theme. An excerpt\\nfrom the story is present below and the full PDF of this story is linked here. For questions like “What\\nis the central theme of the story?”, an upper-level node is retrieved which includes the sentence:\\n“This story is about the power of human connection... inspiring and uplifting each other as they\\npursued their passions.” This summary, not explicitly present in the original text, almost directly\\nanswers the question.\\nExcerpt from ”The Eager Writer”:\\n”Ethan’s passion for writing had always been a part of him. As a child, he would\\noften scribble stories and poems in his notebook, and as he grew older, his love\\nfor writing only intensified. His evenings were often spent in the dim light of his\\nroom, typing away at his laptop. He had recently taken a job as a content writer\\nfor an online marketing firm to pay the bills, but his heart still longed for the\\nworld of storytelling. However, like many aspiring writers, he struggled to find a\\nfoothold in the industry. He took a job as a content writer for an online marketing\\nfirm, but it was growing increasingly evident to him that this was not the path he\\nwanted to pursue. It was during this time that he stumbled upon the Pathways\\napp. The app offered a platform for people in similar professions to connect and\\nshare knowledge, and he saw it as an opportunity to finally connect with others\\nwho shared his passion for writing. Ethan saw an opportunity to meet others who\\nshared his passion and could offer guidance and mentorship. He quickly signed\\nup and was surprised by the number of writers he found on the platform, from\\nwell establish professionals to beginners just starting out in the business.”\\nH N ARRATIVE QA E VALUATION SCRIPT\\nWe made several modifications to AllenNLP’s evaluation script3 to better fit our evaluation needs:\\n• Added Smoothing: Smoothing was incorporated to handle cases where BLEU score is\\nzero, due to no n-gram matches occurring in the reference text. A BLEU score of zero\\nskews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding\\n3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/\\n20'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 20}, page_content='Published as a conference paper at ICLR 2024\\na smoothing function, we prevent the BLEU scores from dropping to zero, providing a more\\nfair evaluation.\\n• Modified BLEU-4 Weighting: The original script applied a weight of 1 to the highest\\norder n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0,\\n0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order\\nmatches. To provide a more balanced evaluation, we evenly distributed the weight across\\nall n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25,\\n0.25).\\n• Tokenization before Mapping in METEOR Calculation: The original script utilized a\\nsimple split and map method for METEOR calculation. We fixed this by first tokenizing the\\ntext and then mapping the tokens. This amendment improves the accuracy of the METEOR\\ncalculation by taking into account the correct linguistic boundaries of words.\\nQuestion: What is the central theme of the story?\\nRAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\\nto be glad that he had found the glass slipper.\\nDPR Two mice were turned into footmen; four grasshoppers into white horses. Next, the\\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\\nlace. . . . Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\\nQuestion: How does Cinderella find a happy ending?\\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\\nCinderella must return home before the clock strikes eleven or her dress will turn back\\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it\\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.\\nDPR the clock had struck Eleven. . . The Prince was very much surprised when he missed\\nCinderella again, and leaving the ball, went in search of her. . . Fairy touched Cin-\\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\\nto the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the\\nroom before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”\\nTable 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\\nfairytale Cinderella.\\nI A NALYSIS OF DIFFERENT LAYERS ON RAPTOR’ S PERFORMANCE\\nI.1 H OW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?\\nIn this section, we present a detailed breakdown of RAPTOR’s retrieval performance when querying\\ndifferent layers of the hierarchical tree structure for various stories. These tables validate the utility\\nof RAPTOR’s multi-layered structure for diverse query requirements.\\nTable 14: Performance of RAPTOR when querying different layers of the tree for Story 2.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\\n1 layer 58.8 47.1 41.1\\n2 layers - 64.7 52.9\\n3 layers - - 47.1\\n21'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 21}, page_content='Published as a conference paper at ICLR 2024\\nFigure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR\\ntree across three datasets (NarrativeQA, Quality, and Qasper) using three retrievers (SBERT, BM25,\\nand DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval\\ncomes from non-leaf layers, with a notable percentage from the first and second layers, highlighting\\nthe importance of RAPTOR’s hierarchical summarization in the retrieval process.\\nTable 15: Performance of RAPTOR when querying different layers of the tree for Story 3.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\\n1 layer 66.6 61.1 61.1\\n2 layers - 66.6 66.6\\n3 layers - - 83.3\\nTable 16: Performance of RAPTOR when querying different layers of the tree for Story 4.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1\\n1 layer 94.7 84.2\\n2 layers - 89.4\\nTable 17: Performance of RAPTOR when querying different layers of the tree for Story 5.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1\\n1 layer 57.9 47.3\\n2 layers - 68.4\\nI.2 W HICH LAYERS DO RETRIEVED NODES COME FROM ?\\nWe further conduct an ablation study across all three datasets and across three different retrievers\\nwith RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes\\noriginate. We observe that between 18.5% to 57% of the retrieved nodes come from non-leaf nodes.\\nAs illustrated in Figure 7, the retrieval pattern across layers reveals the importance of RAPTOR’s\\nmulti-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR\\nusing the DPR retriever for the NarrativeQA dataset come from the first and second layers of the\\ntree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers,\\nalbeit with varying percentages.\\nTable 18: Percentage of nodes from non-leaf nodes across different datasets and retrievers\\nDataset DPR SBERT BM25\\nNarrativeQA 57.36% 36.78% 34.96%\\nQuality 32.28% 24.41% 32.36%\\nQasper 22.93% 18.49% 22.76%\\n22'),\n",
       " Document(metadata={'source': 'Raptor.pdf', 'page': 22}, page_content='Published as a conference paper at ICLR 2024\\nTable 19: Percentage of nodes from different layers with DPR as the retriever\\nLayer NarrativeQA Quality Qasper\\n0 42.64% 67.71% 77.07%\\n1 45.00% 29.43% 21.88%\\n2 10.57% 2.85% 1.05%\\n3 1.78% - -\\n4 0.003% - -\\nTable 20: Percentage of nodes from different layers with SBERT as the retriever\\nLayer NarrativeQA Quality Qasper\\n0 63.22% 75.59% 81.51%\\n1 31.51% 22.78% 17.84%\\n2 4.85% 1.63% 0.65%\\n3 0.42% - -\\nTable 21: Percentage of nodes from different layers with BM25 as the retriever\\nLayer NarrativeQA Quality Qasper\\n0 65.04% 67.64% 77.24%\\n1 28.79% 28.85% 21.57%\\n2 5.36% 3.51% 1.19%\\n3 0.81% - -\\n23')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Raptor.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs) # List of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0]) # Document type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web based Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")) # One webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                        ) # Multiple web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nNlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Specific Information from web pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                       bs_kwargs = dict(parse_only = bs4.SoupStrainer(\n",
    "                          class_ = (\"post-title\", \"post-content\", \"post-header\")\n",
    "                       ))\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Arxiv (Reserach Papers dowloaded from Arxiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Load the Attention is all you need paper\n",
    "docs = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "loader = ArxivLoader(\n",
    "    query=\"reasoning\",\n",
    "    load_max_docs=2,\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Published': '2024-10-16', 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan', 'Summary': 'Large language models (LLMs) have demonstrated impressive reasoning\\nabilities, but they still struggle with faithful reasoning due to knowledge\\ngaps and hallucinations. To address these issues, knowledge graphs (KGs) have\\nbeen utilized to enhance LLM reasoning through their structured knowledge.\\nHowever, existing KG-enhanced methods, either retrieval-based or agent-based,\\nencounter difficulties in accurately retrieving knowledge and efficiently\\ntraversing KGs at scale. In this work, we introduce graph-constrained reasoning\\n(GCR), a novel framework that bridges structured knowledge in KGs with\\nunstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures\\nfaithful KG-grounded reasoning by integrating KG structure into the LLM\\ndecoding process through KG-Trie, a trie-based index that encodes KG reasoning\\npaths. KG-Trie constrains the decoding process, allowing LLMs to directly\\nreason on graphs and generate faithful reasoning paths grounded in KGs.\\nAdditionally, GCR leverages a lightweight KG-specialized LLM for\\ngraph-constrained reasoning alongside a powerful general LLM for inductive\\nreasoning over multiple reasoning paths, resulting in accurate reasoning with\\nzero reasoning hallucination. Extensive experiments on several KGQA benchmarks\\ndemonstrate that GCR achieves state-of-the-art performance and exhibits strong\\nzero-shot generalizability to unseen KGs without additional training.'}, page_content='GRAPH-CONSTRAINED REASONING: FAITHFUL REA-\\nSONING ON KNOWLEDGE GRAPHS WITH LARGE LAN-\\nGUAGE MODELS\\nLinhao Luo1∗, Zicheng Zhao2∗, Chen Gong2, Gholamreza Haffari1, Shirui Pan3†\\n1Monash University 2Nanjing University of Science and Technology 3Griffith University\\n{Linhao.Luo,Gholamreza.Haffari}@monash.edu\\n{zicheng.zhao,chen.gong}@njust.edu.cn, s.pan@griffith.edu.au\\nABSTRACT\\nLarge language models (LLMs) have demonstrated impressive reasoning abilities,\\nbut they still struggle with faithful reasoning due to knowledge gaps and halluci-\\nnations. To address these issues, knowledge graphs (KGs) have been utilized to\\nenhance LLM reasoning through their structured knowledge. However, existing\\nKG-enhanced methods, either retrieval-based or agent-based, encounter difficul-\\nties in accurately retrieving knowledge and efficiently traversing KGs at scale.\\nIn this work, we introduce graph-constrained reasoning (GCR), a novel frame-\\nwork that bridges structured knowledge in KGs with unstructured reasoning in\\nLLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reason-\\ning by integrating KG structure into the LLM decoding process through KG-Trie,\\na trie-based index that encodes KG reasoning paths. KG-Trie constrains the de-\\ncoding process, allowing LLMs to directly reason on graphs and generate faith-\\nful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight\\nKG-specialized LLM for graph-constrained reasoning alongside a powerful gen-\\neral LLM for inductive reasoning over multiple reasoning paths, resulting in ac-\\ncurate reasoning with zero reasoning hallucination. Extensive experiments on\\nseveral KGQA benchmarks demonstrate that GCR achieves state-of-the-art per-\\nformance and exhibits strong zero-shot generalizability to unseen KGs without\\nadditional training. Code is available at https://github.com/RManLuo/\\ngraph-constrained-reasoning.\\n1\\nINTRODUCTION\\nLarge language models (LLMs) have shown impressive reasoning abilities in handling complex\\ntasks (Qiao et al., 2023; Huang & Chang, 2023), marking a significant leap that bridges the gap\\nbetween human and machine intelligence. However, LLMs still struggle with conducting faithful\\nreasoning due to issues of lack of knowledge and hallucination (Huang et al., 2024; Wang et al.,\\n2023). These issues result in factual errors and flawed reasoning processes (Nguyen et al., 2024),\\nwhich greatly undermine the reliability of LLMs in real-world applications.\\nTo address these issues, many studies utilize knowledge graphs (KGs), which encapsulate extensive\\nfactual information in a structured format, to improve the reasoning abilities of LLMs (Pan et al.,\\n2024; Luo et al., 2024). Nevertheless, because of the unstructured nature of LLMs, directly applying\\nthem to reason on KGs is challenging.\\nExisting KG-enhanced LLM reasoning methods can be roughly categorized into two groups:\\nretrieval-based and agent-based paradigms, as shown in Figure 2 (a) and (b). Retrieval-based meth-\\nods (Li et al., 2023; Yang et al., 2024b; Dehghan et al., 2024) retrieve relevant facts from KGs\\nwith an external retriever and then feed them into the inputs of LLMs for reasoning. Agent-based\\nmethods (Sun et al., 2024; Zhu et al., 2024; Jiang et al., 2024) treat LLMs as agents that iteratively\\ninteract with KGs to find reasoning paths and answers.\\n∗Equal Contribution.\\n†Corresponding author.\\n1\\narXiv:2410.13080v1  [cs.CL]  16 Oct 2024\\n67.0%\\n18.0%\\n15.0%\\nFaithful Reasoning Path\\nInvalid - Format Error\\nInvalid - Relation Error\\nFigure\\n1:\\nAnalysis\\nof reasoning errors in\\nRoG (Luo et al., 2024).\\nDespite their success, retrieval-based methods require additional accurate\\nretrievers, which may not generalize well to unseen questions or account\\nfor the graph structure (Mavromatis & Karypis, 2024). Conversely, agent-\\nbased methods necessitate multiple rounds of interaction between agents\\nand KGs, leading to high computational costs and latency (Dehghan et al.,\\n2024). Furthermore, existing works still suffer from serious hallucination\\nissues (Agrawal et al., 2024). Sui et al. (2024) indicates that RoG (Luo\\net al., 2024), a leading KG-enhanced reasoning method, still experiences\\n33% hallucination errors during reasoning on KGs, as shown in Figure 1.\\nTo this end, we introduce graph-constrained reasoning (GCR), a novel KG-\\nguided reasoning paradigm that connects unstructured reasoning in LLMs\\nwith structured knowledge in KGs, seeking to eliminate hallucinations dur-\\ning reasoning on KGs and ensure faithful reasoning. Inspired by the con-\\ncept that LLMs reason through decoding (Wei et al., 2022), we incorporate\\nthe KG structure into the LLM decoding process. This enables LLMs to directly reason on graphs\\nby generating reliable reasoning paths grounded in KGs that lead to correct answers.\\nIn GCR, we first convert KG into a structured index, KG-Trie, to facilitate efficient reasoning on KG\\nusing LLM. Trie is also known as the prefix tree (Wikipedia contributors, 2024) that compresses a\\nset of strings, which can be used to restrict LLM output tokens to those starting with valid prefixes\\n(De Cao et al., 2022; Xie et al., 2022). KG-Trie encodes the reasoning paths in KGs as formatted\\nstrings to constrain the decoding process of LLMs. Then, we propose graph-constrained decoding\\nthat employs a lightweight KG-specialized LLM to generate multiple KG-grounded reasoning paths\\nand hypothesis answers. With the constraints from KG-Trie, we ensure faithful reasoning while\\nleveraging the strong reasoning capabilities of LLMs to efficiently explore paths on KGs in constant\\ntime. Finally, we input multiple generated reasoning paths and hypothesis answers into a powerful\\ngeneral LLM to utilize its inductive reasoning ability to produce final answers. In this way, GCR\\ncombines the graph reasoning strength of KG-specialized LLMs and the inductive reasoning advan-\\ntage in general LLMs to achieve faithful and accurate reasoning on KGs. The main contributions of\\nthis work are as follows:\\n• We propose a novel framework called graph-constrained reasoning (GCR) that bridges the\\ngap between structured knowledge in KGs and unstructured reasoning in LLMs, allowing\\nfor efficient reasoning on KGs via LLM decoding.\\n• We combine the complementary strengths of a lightweight KG-specialized LLM with a\\npowerful general LLM to enhance reasoning performance by leveraging their respective\\ngraph-based reasoning and inductive reasoning capabilities.\\n• We conduct extensive experiments on several KGQA reasoning benchmarks, demonstrat-\\ning that GCR not only achieves state-of-the-art performance with zero hallucination, but\\nalso shows zero-shot generalizability for reasoning on unseen KGs without additional train-\\ning.\\n2\\nRELATED WORK\\nLLM reasoning. Many studies have been proposed to analyze and improve the reasoning ability\\nof LLMs (Wei et al., 2022; Wang et al., 2024; Yao et al., 2024). To elicit the reasoning ability\\nof LLMs, Chain-of-thought (CoT) reasoning (Wei et al., 2022) prompts the model to generate a\\nchain of reasoning steps in response to a question. Wang et al. (2024) propose a self-consistency\\nmechanism that generates multiple reasoning paths and selects the most consistent answer across\\nthem. The tree-of-thought (Yao et al., 2024) structures reasoning as a branching process, exploring\\nmultiple steps in a tree-like structure to find optimal solutions. Other studies focus on fine-tuning\\nLLMs on various reasoning tasks to improve reasoning abilities (Yu et al., 2022; Hoffman et al.,\\n2024). For instance, OpenAI (2024c) adopts reinforcement learning to train their most advanced\\nLLMs called “OpenAI o1” to perform complex reasoning, which produces a long internal chain of\\nthought before final answers.\\nKG-enhanced LLM reasoning. To mitigate the knowledge gap and hallucination issues in LLM\\nreasoning, research incorporates KGs to enhance LLM reasoning (Pan et al., 2024). KD-CoT (Wang\\n2\\n# Reasoning Path:\\n# Answer:\\nMelania Trump\\nGeneral\\nLLM\\nKG-specialized\\nLLM\\nQ\\nA\\nQuestion\\nAnswer\\nKnowledge\\nGraph\\n(a) Retrieval-based LLM Reasoning\\nLLM\\nReasoning\\n(b) Agent-based LLM Reasoning\\n(c) Ours: Knowledge Graph-constrained LLM Reasoning\\nQ:\\xa0Who is\\nthe spouse\\nof the ex-\\npresident of\\nUSA?\\nt=2\\nA: Based on the paths,\\nthe answers are: Laura\\nBush, Michelle Obama,\\nMelania Trump.\\nKG-specialized\\nLLM\\nKG-Trie\\nConstraint\\xa0\\n①\\xa0Offline KG-Trie\\nConstruction\\n②\\xa0Graph-constrained\\nDecoding\\nt=1\\nReasoning Paths and\\xa0\\n\\xa0\\xa0Hypothesis\\xa0Answers\\nGeneral\\nLLM\\n③\\xa0Inductive\\nReasoning\\n# Reasoning Path:\\n# Answer:\\nLaura Bush\\n# Reasoning Path:\\n# Answer:\\nMichelle Obama\\nQ\\nKnowledge\\nRetriever\\nA\\nRetrieved\\nFacts\\nt=1\\nLLM\\nEx-president\\nFounded_in\\n1776\\nUSA\\nBarack Obama\\nBorn_in\\nHonolulu\\nMichelle\\nObama\\nSasha Obama\\nMorther_of\\nLLM Agent\\nA\\nSpouse_of\\nt=1\\nt=2\\nt=3\\nT Steps\\nUSA\\nDonald Trump\\nEx-president\\nMichelle\\nObama\\nGeorge W.\\nBush\\xa0\\nEx-president\\nBarack Obama\\nEx-president\\nSpouse_of\\n1776\\nFounded_in\\nWashington\\nD.C.\\nCapital\\nLaura\\nBush\\xa0\\nKnowledge Graph\\nMelania\\nTrump\\nMarry_to\\nIvana\\nTrump\\nEx-wife\\nSpouse_of\\nQ\\nFigure 2: Illustration of existing KG-enhanced LLM reasoning paradigms and proposed graph-\\nconstrained reasoning (GCR). 1) First, given a KG, we convert it into the KG-Trie, serving as a\\nstructured index to facilitate efficient reasoning path searches using LLMs. 2) Then, we design a\\ngraph-constrained decoding process that employs a lightweight KG-specialized LLM to generate\\nmultiple KG-grounded reasoning paths and hypothesis answers. This ensures the faithfulness of the\\nreasoning process while leveraging the strong capabilities of LLMs to efficiently explore reasoning\\npaths within KGs. 3) Finally, we input the generated reasoning paths and hypothesis answers into a\\npowerful general LLM to utilize its inductive reasoning ability to produce final answers.\\net al., 2023) retrieve facts from an external knowledge graph to guide the CoT performed by LLMs.\\nRoG (Luo et al., 2024) proposes a planning-retrieval-reasoning framework that retrieves reasoning\\npaths from KGs to guide LLMs conducting faithful reasoning. To capture graph structure, GNN-\\nRAG (Mavromatis & Karypis, 2024) adopts a lightweight graph neural network to effectively re-\\ntrieve from KGs. Instead of retrieving, StructGPT (Jiang et al., 2023) and ToG (Sun et al., 2024)\\ntreat LLMs as agents to interact with KGs to find reasoning paths leading to the correct answers.\\n3\\nPRELIMINARY\\nKnowledge Graphs (KGs) represent a wealth of factual knowledge as a collection of triples: G =\\n{(e, r, e′) ∈E × R × E}, where E and R denote the set of entities and relations, respectively.\\nReasoning Paths are sequences of consecutive triples in KGs: wz = e0\\nr1\\n−→e1\\nr2\\n−→. . .\\nrl\\n−→el,\\nwhere ∀(ei−1, ri, ei) ∈G. The paths reveal the connections between knowledge that potentially\\nfacilitate reasoning. For example, the reasoning path: wz = Alice\\nmarry to\\n−−−−−−→Bob\\nfather of\\n−−−−−−→\\nCharlie indicates that “Alice” is married to “Bob” and “Bob” is the father of “Charlie”. Therefore,\\n“Alice” could be reasoned to be the mother of “Charlie”.\\nKnowledge Graph Question Answering (KGQA) is a representative reasoning task with the as-\\nsistance of KGs. Given a natural language question q and a KG G, the task aims to design a function\\nf to reason answers a ∈A based on knowledge from G, i.e., a = f(q, G).\\n3\\n4\\nAPPROACH\\n4.1\\nFROM CHAIN-OF-THOUGHT REASONING TO GRAPH-CONSTRAINED REASONING\\nChain-of-Thought Reasoning (CoT) (Wei et al., 2022) has been widely adopted to enhance the\\nreasoning ability of LLMs by autoregressively generating a series of reasoning steps leading to the\\nanswer. Specifically, given a question q, CoT models the joint probability of the answer a and\\nreasoning steps z as\\nP(a|q) =\\nX\\nz\\nPθ(a|z, q)Pθ(z|q) =\\nX\\nz\\nPθ(a|q, z)\\n|z|\\nY\\ni=1\\nPθ(zi|q, z1:i−1),\\n(1)\\nwhere q denotes the input question, a denotes the final answer, θ denotes the parameters of LLMs,\\nand zi denotes the i-th step of the reasoning process z. To further enhance the reasoning ability,\\nmany previous works focus on improving the reasoning process Pθ(z|q) by exploring and aggregat-\\ning multiple reasoning processes (Wang et al., 2024; Yao et al., 2024).\\nDespite the effectiveness, a major issue remains the faithfulness of the reasoning process generated\\nby LLMs (Huang et al., 2024). The reasoning is represented as a sequence of tokens decoded\\nstep-by-step, which can accumulate errors and result in hallucinated reasoning paths and answers\\n(Nguyen et al., 2024). To address these issues, we utilize knowledge graphs (KGs) to guide LLMs\\ntoward faithful reasoning.\\nKG-enhanced Reasoning utilizes the structured knowledge in KGs to improve the reasoning of\\nLLMs (Luo et al., 2024; Sun et al., 2024), which can generally be expressed as finding a reasoning\\npath wz on KGs that connects the entities mentioned in the question and the answer. This can be\\nformulated as\\nP(a|q, G) =\\nX\\nwz\\nPϕ(a|q, wz)Pϕ(wz|q, G),\\n(2)\\nwhere Pϕ(wz|q, G) denotes the probability of discovering a reasoning path wz on KGs G given the\\nquestion q by a function parameterized by ϕ. To acquire reasoning paths for reasoning, most prior\\nstudies follow the retrieval-based (Li et al., 2023) or agent-based paradigm (Sun et al., 2024), as\\nshown in Figure 2 (a) and (b), respectively. Nevertheless, retrieval-based methods rely on precise\\nadditional retrievers, while agent-based methods are computationally intensive and lead to high\\nlatency. To address these issues, we propose a novel graph-constrained reasoning paradigm (GCR).\\nGraph-constrained Reasoning (GCR) directly incorporates KGs into the decoding process of\\nLLMs to achieve faithful reasoning. The overall framework of GCR is illustrated in Figure 2 (c),\\nwhich consists of three main components: 1) Knowledge Graph Trie Construction: building a\\nstructural index of KG to guide LLM reasoning, 2) Graph-constrained Decoding: generating KG-\\ngrounded paths and hypothesis answers using LLMs, and 3) Graph Inductive Reasoning: reasoning\\nover multiple paths and hypotheses to derive final answers.\\n4.2\\nKNOWLEDGE GRAPH TRIE CONSTRUCTION\\nKnowledge graphs (KGs) store abundant knowledge in a structured format. However, large language\\nmodels (LLMs) struggle to efficiently access and reason on KGs due to their unstructured nature. To\\naddress this issue, we propose to convert KGs into knowledge graph Tries (KG-Tries), which serve\\nas a structured index of KGs to facilitate efficient reasoning on graphs using LLMs.\\nA Trie (a.k.a. prefix tree) (Wikipedia contributors, 2024; Fredkin, 1960) is a tree-like data structure\\nthat stores a dynamic set of strings, where each node represents a common prefix of its children.\\nTries can be used to restrict LLM output tokens to those starting with valid prefixes (De Cao et al.,\\n2022; Xie et al., 2022; Chen et al., 2022). The tree structure of Trie is an ideal choice for encoding\\nthe reasoning paths in KGs for LLMs to efficiently traverse.\\nWe first adopt the breadth-first search (BFS) algorithm to retrieve reasoning paths Wz within L hops\\nstarting from entities mentioned in the questions. The retrieved paths are formatted as sentences\\nusing the template shown in Figure 7. The formatted sentences are then split into tokens by the\\n4\\ntokenizer of LLM and stored as a KG-Trie CG. The overall process can be formulated as:\\nWz = BFS(G, {eq}, L),\\n(3)\\nTz = Tokenizer(Wz),\\n(4)\\nCG = Trie(Tz),\\n(5)\\nwhere eq denotes the entities mentioned in the question, L denotes the maximum hops of paths, and\\nTz denotes the tokens of reasoning paths. The KG-Trie CG is used as a constraint to guide the LLM\\ndecoding process.\\nBy constructing KG-Trie for each question entity, we can enable efficient traversal of reasoning paths\\nin constant time (O(|Wz|)) without costly graph traversal (Sun et al., 2024). Moreover, KG-Trie can\\nbe pre-constructed offline and loaded during reasoning. This significantly reduces the computational\\ncost and latency of reasoning on KGs, making it feasible for real-time applications.\\n============================= Prompt Input ================================\\nPlease generate some reasoning paths in the KG starting from the topic entities to answer the question.\\n# Question: what is the name of justin bieber brother?\\n============================= LLM Output ================================\\n# Reasoning Path: <PATH> Justin Bieber →people.person.parents →Jeremy Bieber →peo-\\nple.person.children →Jaxon Bieber </PATH>\\n# Answer: Jaxon Bieber\\nFigure 3: An example of the graph-constrained decoding. Detailed prompts can be found in Figure 8.\\n4.3\\nGRAPH-CONSTRAINED DECODING\\nLarge language models (LLMs) have strong reasoning capabilities but still suffer from severe hal-\\nlucination issues, which undermines the trustworthiness of the reasoning process. To tackle this\\nissue, we propose graph-constrained decoding, which unifies the reasoning ability of LLMs with the\\nstructured knowledge in KGs to generate faithful KG-grounded reasoning paths leading to answers.\\nGiven a question q, we design an instruction prompt to harness the reasoning ability of LLMs to\\ngenerate reasoning paths wz and hypothesis answers a. To eliminate the hallucination during rea-\\nsoning on KGs, we adopt the KG-Trie CG as constraints to guide the decoding process of LLMs and\\nonly generate reasoning paths that are valid in KGs, formulated as:\\nPϕ(a, wz|q) = Pϕ(a|q, wz)\\n|\\n{z\\n}\\nRegular decoding\\nGraph-constrained decoding\\nz\\n}|\\n{\\n|wz|\\nY\\ni=1\\nPϕ(wzi|q, wz1:i−1)CG(wzi|wz1:i−1),\\n(6)\\nCG(wzi|wz1:i−1) =\\n\\x1a1, ∃prefix(wz1:i, wz), ∃wz ∈Wz,\\n0, else,\\n(7)\\nwhere wzi denotes the i-th token of the reasoning path wz, Pϕ denotes the token probabilities\\npredicted by the LLM with parameters ϕ, and CG(wzi|wz1:i−1) denotes the constraint function that\\nchecks whether the generated tokens wz1:i is a valid prefix of the reasoning path using KG-Trie.\\nAfter a valid reasoning path is generated, we switch back to the regular decoding process to generate\\na hypothesis answer conditioned on the path.\\nTo further enhance KG reasoning ability, we fine-tune a lightweight KG-specialized LLM with\\nparameters ϕ on the graph-constrained decoding task. Specifically, given a question q, the LLM is\\noptimized to generate relevant reasoning paths wz that are helpful for answering the question, then\\nprovide a hypothesis answer a based on it, which can be formulated as:\\nL = E(q,wz,a)∼DG log Pϕ(a, wz|q) = E\\n\\uf8ee\\n\\uf8f0log\\n|a|\\nY\\ni=1\\nPϕ(ai|q, wz, a1:i−1)\\n|wz|\\nY\\nj=1\\nPϕ(wzj|q, wz1:j−1)\\n\\uf8f9\\n\\uf8fb,\\n(8)\\nwhere ai and wzj denote the i-th token of the answer a and the j-th token of the reasoning path wz,\\nrespectively.\\n5\\nThe training data (q, wz, a) ∈DG consists of question-answer pairs and reasoning paths generated\\nfrom KGs. We use the shortest paths connecting the entities in the question and answer as the\\nreasoning path wz for training, where details can be found in Section 7. An example of graph-\\nconstrained decoding is illustrated in Figure 3, where <PATH> and </PATH> are special tokens\\nto control the start and end of graph-constrained decoding. Experiment results in Section 5.2 show\\nthat even a lightweight KG-specialized LLM (0.5B) can achieve satisfactory performance in KG\\nreasoning.\\nThe graph-constrained decoding method differs from retrieval-based methods by integrating a pre-\\nconstructed KG-Trie into the decoding process of LLMs. This not only reduces input tokens, but\\nalso bridges the gap between unstructured reasoning in LLMs and structured knowledge in KGs,\\nallowing for efficient reasoning on KGs regardless of its scale, which results in faithful reasoning\\nleading to answers. Additionally, experimental results in Section 5.4 demonstrate that KG-Trie can\\nintegrate with new KGs on the fly, showcasing its zero-shot generalizability for reasoning on unseen\\nKGs without further training.\\n4.4\\nGRAPH INDUCTIVE REASONING\\nGraph-constrained decoding harnesses the reasoning ability of a KG-specialized LLM to generate a\\nfaithful reasoning path and a hypothesis answer. However, complex reasoning tasks typically admit\\nmultiple reasoning paths that lead to correct answers (Stanovich et al., 2000). Incorporating diverse\\nreasoning paths would be beneficial for deliberate thinking and reasoning (Evans, 2010; Wang et al.,\\n2024). To this end, we propose to input multiple reasoning paths and hypothesis answers generated\\nby the KG-specialized LLM into a powerful general LLM to leverage its inductive reasoning ability\\nto produce final answers.\\nThe graph-constrained decoding seamlessly integrates into the decoding process of LLMs, allowing\\nit to be paired with various LLM generation strategies like beam-search (Federico et al., 1995) to\\ntake advantage of the GPU parallel computation. Thus, given a question, we adopt graph-constrained\\ndecoding to simultaneously generate K reasoning paths and hypothesis answers with beam search in\\na single LLM call, which are then inputted into a general LLM to derive final answers. The overall\\nprocess can be formulated as:\\nZK = {ak, wk\\nz}K\\nk=1 = arg top-K Pϕ(a, wz|q),\\n(9)\\nPθ(A|q, ZK) ≃\\nK\\nY\\nk=1\\nPθ(A|q, ak, wk\\nz),\\n(10)\\nwhere θ denotes the parameters of the general LLM, ZK denotes the set of top-K reasoning paths\\nand hypothesis answers, and A denotes the final answers.\\nWe follow the FiD framework (Izacard & Grave, 2021; Singh et al., 2021) to incorporate multiple\\nreasoning paths and hypothesis answers to conduct inductive reasoning within one LLM call, i.e.,\\nPθ(A|q, ZK), where detailed prompts can be found in Figure 9. The general LLM can be any\\npowerful LLM, such as ChatGPT (OpenAI, 2022), or Llama-3 (Meta, 2024), which can effectively\\nleverage their internal reasoning ability to reason over multiple reasoning paths to produce final\\nanswers without additional fine-tuning.\\n5\\nEXPERIMENT\\nIn our experiments, we aim to answer the following research questions: RQ1: Can GCR achieve\\nstate-of-the-art reasoning performance with balances between efficiency and effectiveness? RQ2:\\nCan GCR eliminate hallucinations and conduct faithful reasoning? RQ3: Can GCR generalize to\\nunseen KGs on the fly?\\n5.1\\nEXPERIMENT SETUPS\\nDatasets. Following previous research (Luo et al., 2024; Sun et al., 2024), we first evaluate the\\nreasoning ability of GCR on two benchmark KGQA datasets: WebQuestionSP (WebQSP) (Yih et al.,\\n2016) and Complex WebQuestions (CWQ) (Talmor & Berant, 2018). Freebase (Bollacker et al.,\\n6\\nTable 1: Performance comparison with different baselines on the two KGQA datasets.\\nTypes\\nMethods\\nWebQSP\\nCWQ\\nHit\\nF1\\nHit\\nF1\\nLLM Reasoning\\nQwen2-0.5B (Yang et al., 2024a)\\n26.2\\n17.2\\n12.5\\n11.0\\nQwen2-1.5B (Yang et al., 2024a)\\n41.3\\n28.0\\n18.5\\n15.7\\nQwen2-7B (Yang et al., 2024a)\\n50.8\\n35.5\\n25.3\\n21.6\\nLlama-2-7B (Touvron et al., 2023)\\n56.4\\n36.5\\n28.4\\n21.4\\nLlama-3.1-8B (Meta, 2024)\\n55.5\\n34.8\\n28.1\\n22.4\\nGPT-4o-mini (OpenAI, 2024a)\\n63.8\\n40.5\\n63.8\\n40.5\\nChatGPT (OpenAI, 2022)\\n59.3\\n43.5\\n34.7\\n30.2\\nChatGPT+Few-shot (Brown et al., 2020)\\n68.5\\n38.1\\n38.5\\n28.0\\nChatGPT+CoT (Wei et al., 2022)\\n73.5\\n38.5\\n47.5\\n31.0\\nChatGPT+Self-Consistency (Wang et al., 2024)\\n83.5\\n63.4\\n56.0\\n48.1\\nGraph Reasoning\\nGraftNet (Sun et al., 2018)\\n66.7\\n62.4\\n36.8\\n32.7\\nNSM (He et al., 2021)\\n68.7\\n62.8\\n47.6\\n42.4\\nSR+NSM (Zhang et al., 2022)\\n68.9\\n64.1\\n50.2\\n47.1\\nReaRev (Mavromatis & Karypis, 2022)\\n76.4\\n70.9\\n52.9\\n47.8\\nKG+LLM\\nKD-CoT (Wang et al., 2023)\\n68.6\\n52.5\\n55.7\\n-\\nEWEK-QA (Dehghan et al., 2024)\\n71.3\\n-\\n52.5\\n-\\nToG (ChatGPT) (Sun et al., 2024)\\n76.2\\n-\\n57.6\\n-\\nToG (GPT-4) (Sun et al., 2024)\\n82.6\\n-\\n68.5\\n-\\nEffiQA (Dong et al., 2024)\\n82.9\\n-\\n69.5\\nRoG (Llama-2-7B) (Luo et al., 2024)\\n85.7\\n70.8\\n62.6\\n56.2\\nGNN-RAG (Mavromatis & Karypis, 2024)\\n85.7\\n71.3\\n66.8\\n59.4\\nGNN-RAG+RA (Mavromatis & Karypis, 2024)\\n90.7\\n73.5\\n68.7\\n60.4\\nGCR (Llama-3.1-8B + ChatGPT)\\n92.6\\n73.2\\n72.7\\n60.9\\nGCR (Llama-3.1-8B + GPT-4o-mini)\\n92.2\\n74.1\\n75.8\\n61.7\\n2008) is adopted as the knowledge graph for both datasets. To further evaluate the generalizability\\nof GCR, we conduct zero-shot transfer experiments on two new KGQA datasets: CommonsenseQA\\n(CSQA) (Talmor et al., 2019) and MedQA-USMLE (MedQA) (Jin et al., 2021). For CSQA, we use\\nConceptNet (Speer et al., 2017) as the KG, while for MedQA, we use a medical KG constructed\\nfrom the Unified Medical Language System (Yasunaga et al., 2021). The details of the datasets are\\ndescribed in Section 7.\\nBaselines. We compare GCR with the 22 baselines grouped into three categories: 1) LLM reasoning\\nmethods, 2) graph reasoning methods, and 3) KG-enhanced LLM reasoning methods. The detailed\\nbaselines are listed in Section 8.\\nEvaluation Metrics. We adopt Hit and F1 as the evaluation metrics following previous works (Luo\\net al., 2024; Sun et al., 2024) on WebQSP and CWQ. Hit checks whether any correct answer exists in\\nthe generated predictions, while F1 considers the coverage of all answers by balancing the precision\\nand recall of predictions. Because CSQA and MedQA are multiple-choice QA datasets, we adopt\\naccuracy as the evaluation metric.\\nImplementations. For GCR, we use the KG-Trie to index all the reasoning paths within 2 hops\\nstarting from question entities. For the LLMs, we use a fine-tuned Llama-3-8B (Meta, 2024) as\\nthe KG-specialized LLM. We generate top-10 reasoning paths and hypothesis answers from graph-\\nconstrained decoding. We adopt the advanced ChatGPT (OpenAI, 2022) and GPT-4o-mini (OpenAI,\\n2024a) as the general LLMs for inductive reasoning. The detailed hyperparameters and experiment\\nsettings are described in Section 9.\\n5.2\\nRQ1: REASONING PERFORMANCE AND EFFICIENCY\\nMain Results. In this section, we compare GCR with other baselines on KGQA benchmarks to\\nevaluate the reasoning performance. From the results shown in Table 1, GCR achieves the best\\nperformance on both datasets, outperforming the second-best by 2.1% and 9.1% in terms of Hit on\\nWebQSP and CWQ, respectively. The results demonstrate that GCR can effectively leverage KGs to\\nenhance LLMs and achieve state-of-the-art reasoning performance.\\nAmong the LLM reasoning methods, ChatGPT with self-consistency prompts demonstrates the best\\nperformance, which indicates the powerful reasoning ability inherent in LLMs. However, their per-\\nformances are still limited by the model size and complex reasoning required over structured data.\\nGraph reasoning methods, such as ReaRev, achieve competitive performance on WebQSP by ex-\\n7\\nTable 2: Efficiency and performance comparison of different methods on WebQSP.\\nTypes\\nMethods\\nHit\\nAvg. Runtime (s)\\nAvg. # LLM Calls\\nAvg. # LLM Tokens\\nRetrieval-based\\nS-Bert\\n66.9\\n0.87\\n1\\n293\\nBGE\\n72.7\\n1.05\\n1\\n357\\nOpenAI-Emb.\\n79.0\\n1.77\\n1\\n330\\nGNN-RAG\\n85.7\\n1.52\\n1\\n414\\nRoG\\n85.7\\n2.60\\n2\\n521\\nAgent-based\\nToG\\n75.1\\n16.14\\n11.6\\n7,069\\nEffiQA\\n82.9\\n-\\n7.3\\n-\\nOurs\\nGCR\\n92.6\\n3.60\\n2\\n231\\nplicitly modeling the graph structure. But they struggle to generalize across different datasets and\\nunderperform on CWQ. In KG+LLM methods, both agent-based methods (e.g., ToG, EffiQA) and\\nretrieval-based methods (e.g., RoG, GNN-RAG) achieve the second-best performance. Neverthe-\\nless, they still suffer from inefficiency and reasoning hallucinations which limit their performance.\\nIn contrast, GCR effectively eliminates hallucinations and conducts faithful reasoning by leveraging\\nthe structured KG index and graph-constrained decoding.\\nEfficiency Analysis.\\nTo show the efficiency of GCR, we compare the average runtime, number of LLM calls, and num-\\nber of input tokens with retrieval-based and agent-based methods in Table 2. For retrieval-based\\nmethods, we compare with dense retrievers (e.g., S-Bert (Reimers & Gurevych, 2019), BGE (Zhang\\net al., 2023), OpenAI-Emb. (OpenAI, 2024b)) and graph-based retrievers (e.g., GNN-RAG (Mavro-\\nmatis & Karypis, 2024), RoG (Luo et al., 2024)), which retrieve reasoning paths from KGs and feed\\nthem into LLMs for reasoning answers. For agent-based methods, we compare with ToG (Sun et al.,\\n2024) and EffiQA1 (Dong et al., 2024), which heuristically search on KGs for answers. The detailed\\nsettings are described in Section 9.\\nDense retrievers are most efficient in terms of runtime and LLM calls as they convert all paths into\\nsentences and encode them as embeddings in advance. However, they sacrifice their accuracy in\\nretrieving as they are not designed to encode graph structure. Graph-based retrievers and agent-\\nbased methods achieve better performance by considering graph structure; however, they require\\nmore time and LLM calls. Specifically, the retrieved graph is fed as inputs to LLMs, which leads to\\na large number of input tokens. Agent-based methods, like ToG, require more LLM calls and input\\ntokens as the question difficulty increases due to their iterative reasoning process. In contrast, GCR\\nachieves the best performance with a reasonable runtime and number of LLM calls. With the help\\nof KG-Trie, GCR explores multiple reasoning paths at the same time during the graph-constrained\\ndecoding, which does not involve additional LLM calls or input tokens and benefits from the parallel\\nGPU computation with low latency. More efficiency analysis under different beam sizes used for\\ngraph-constrained decoding can be found in parameter analysis.\\nTable 3: Ablation studies of GCR on two KGQA datasets.\\nVariants\\nWebQSP\\nCWQ\\nF1\\nPrecision\\nRecall\\nF1\\nPrecision\\nRecall\\nGCR (Llama-3.1-8B + ChatGPT)\\n73.2\\n80.0\\n76.9\\n60.9\\n61.1\\n66.6\\nGCR w/o KG-specialized LLM\\n52.9\\n66.3\\n50.2\\n37.5\\n40.8\\n37.9\\nGCR w/o General LLM\\n57.0\\n58.0\\n70.1\\n39.4\\n32.8\\n64.3\\nAblation Study. We first conduct an\\nablation study to analyze the effec-\\ntiveness of the KG-specialized LLM\\nand general LLM in GCR. As shown\\nin Table 3, the full GCR achieves the\\nbest performance on both datasets.\\nBy removing the KG-specialized LLM, we feed all 2-hop reasoning paths into the general LLM.\\nThis results in a significant performance drop, indicating its importance in utilizing reasoning abil-\\nity to find relevant paths on KGs for reasoning. On the other hand, removing the general LLM\\nand relying solely on answers predicted by KG-specialized LLM leads to a noticeable decrease in\\nprecision, due to noises in its predictions. This highlighting the necessity of the general LLM for\\nconducting inductive reasoning over multiple paths to derive final answers.\\nDifferent LLMs. We further analyze LLMs used for KG-specialized and general LLMs in Table 4.\\nFor KG-specialized LLMs, we directly plug the KG-Trie into different LLMs to conduct graph-\\nconstrained decoding and use the same general LLM for final reasoning. For general LLMs, we\\nadopt the same reasoning paths generated by KG-specialized LLMs to different LLMs to produce\\nfinal answers. For zero-shot and few-shot learning, we adopt the original LLMs without fine-tuning,\\nwhose prompt templates can be found in Figures 8 and 10.\\n1Since there is no available code for EffiQA, we directly copy the results from the original paper.\\n8\\nTable 4: Comparison of different LLMs used in\\nGCR on WebQSP.\\nComponents\\nLearning Types\\nVariants\\nHit\\nF1\\nKG-specialized\\nLLM\\nZero-shot\\nLlama-3.1-8B\\n28.25\\n10.32\\nLlama-3.1-70B\\n38.53\\n12.53\\nFew-shot\\nLlama-3.1-8B\\n33.24\\n11.19\\nLlama-3.1-70B\\n41.13\\n13.14\\nFine-tuned\\nQwen2-0.5B\\n87.48\\n60.03\\nQwen2-1.5B\\n89.21\\n62.97\\nQwen2-7B\\n92.31\\n72.74\\nLlama-2-7B\\n92.55\\n73.23\\nLlama-3.1-8B\\n92.74\\n73.14\\nGeneral LLM\\nZero-shot\\nQwen-2-7B\\n86.32\\n67.59\\nLlama-3.1-8B\\n90.24\\n71.19\\nLlama-3.1-70B\\n90.24\\n71.19\\nChatGPT\\n92.55\\n73.23\\nGPT-4o-mini\\n92.23\\n74.05\\nResults in Table 4 show that a lightweight LLM\\n(0.5B) can outperform a large one (70B) af-\\nter fine-tuning, indicating the effectiveness of\\nfine-tuning in enhancing the ability of LLMs\\nand make them specialized for KG reason-\\ning.\\nHowever, the larger LLMs (e.g., 7B\\nand 8B) still perform better than smaller ones,\\nhighlighting the importance of model capac-\\nity in searching relevant reasoning paths on\\nKGs.\\nSimilar trends are observed in gen-\\neral LLMs where larger models (e.g., GPT-4o-\\nmini and ChatGPT) outperform smaller ones\\n(e.g., Qwen-2-7B and Llama-3.1-8B), show-\\ncasing their stronger inductive reasoning abili-\\nties. This further emphasizes the need of paring\\npowerful general LLMs with lightweight KG-specialized LLMs to achieve better reasoning driven\\nby both of them.\\n1\\n3\\n5\\n10\\n20\\nGraph-constrained decoding beam size K\\n0\\n2\\n4\\n6\\n8\\nGeneration Time (s)\\n40\\n50\\n60\\n70\\n80\\n90\\nAnswer Coverage (%)\\nGeneration Time (s)\\nHit\\nF1\\nPrecision\\nRecall\\nFigure 4: Parameter analysis of beam\\nsize K.\\nParameter Analysis. We first analyze the impact of dif-\\nferent beam sizes K for graph-constrained decoding on\\nthe performance of GCR. We conduct the experiments on\\nWebQSP with different beam sizes of 1, 3, 5, 10, and 20.\\nThe results are shown in Figure 4. We observe that the hit\\nand recall of GCR increase with the beam size. Because,\\nwith a larger beam size, the LLMs can explore more rea-\\nsoning paths and find the correct answers. However, the\\nF1 score, peaks when the beam size is set to 10. This is\\nbecause the beam size of 10 can provide a balance be-\\ntween the exploration and exploitation of the reasoning\\npaths. When the beam size is set to 20, the performance\\ndrops due to the increased complexity of the search space,\\nwhich may introduce noise and make the reasoning less\\nreliable. This also highlights the importance of using general LLMs to conduct inductive reason-\\ning over multiple paths to disregard the noise and find the correct answers. Although the graph-\\nconstrained decoding benefits from the parallel GPU computation to explore multiple reasoning\\npaths at the same time, the time cost still slightly increases from 1.4s to 7.8s with the increase of\\nthe beam size. Thus, we set the beam size to 10 in the experiments to balance the performance and\\nefficiency. We also investigate the impact of L hops paths used for KG-Trie construction in Sec-\\ntion 10.1. The results show that GCR can achieve a good balance between reasoning performance\\nand efficiency by setting L = 2 and K = 10.\\n5.3\\nRQ2: HALLUCINATION ELIMINATION AND FAITHFUL REASONING\\nIn this section, we investigate the effectiveness of KG constraints in eliminating hallucinations and\\nensuring faithful reasoning. We first compare the difference of answer accuracy (Hit) and faithful\\nreasoning ratio by removing KG constraints in graph-constrained decoding. The faithful reasoning\\nratio is calculated as the percentage of faithful reasoning in correctly predicted answers. We define\\na reasoning as faithful where the generated reasoning path can be found in KGs, and vice versa.\\nGCR GCR w/o constraint\\n0\\n20\\n40\\n60\\nAnswer Hit\\n100.0%\\n62.4%\\nWebQSP\\nFaithful Reasoning\\nError Reasoning\\nGCR GCR w/o constraint\\n0\\n20\\n40\\n60\\nAnswer Hit\\n100.0%\\n48.1%\\nCWQ\\nFigure 5: Analysis of performance and reasoning\\nerrors in GCR.\\nFrom the Figure 5, we can observe that GCR\\nachieves the 100% faithful reasoning ratio on\\nboth datasets, which indicates that GCR can\\neliminate hallucinations and ensure faithful rea-\\nsoning during reasoning on KGs. In contrast,\\nwhen removing KG constraints, both the an-\\nswer accuracy and faithful reasoning decrease\\nsignificantly on WebQSP. This shows that KG\\nconstraints not only improve reasoning by re-\\nducing the searching space, but also play a cru-\\ncial role in preventing hallucinations for accu-\\n9\\nTable 5: Examples of the faithful reasoning conducted by GCR. Red denotes the incorrect reasoning\\npaths and answers, while bold denotes the correct paths and answers.\\nCase 1: Incorrect answers and hallucinated reasoning paths without constraints.\\nQuestion\\nWho is niall ferguson ’s wife?\\nAnswer\\nAyaan Hirsi Ali\\nGCR w/o constraint\\n# Reasoning Path: Niall Ferguson →people.person.children →Mabel Rose Ferguson →\\npeople.person.parents →Alyssa Mastromonaco\\n#Answer: Alyssa Mastromonaco\\nGCR\\n# Reasoning Path: Niall Ferguson →people.person.children →Thomas Ferguson →peo-\\nple.person.parents →Ayaan Hirsi Ali\\n#Answer: Ayaan Hirsi Ali\\nCase 2: Correct answers but hallucinated reasoning paths without constraints.\\nQuestion\\nWhere is jamarcus russell from?\\nAnswer\\nMobile\\nGCR w/o constraint\\n# Reasoning Path: JaMarcus Russell →people.person.place of birth →Tampa\\n#Answer: Mobile, Alabama\\nGCR\\n# Reasoning Path: JaMarcus Russell →people.person.place of birth →Mobile\\n#Answer: Mobile\\nrate reasoning. While the answer hit rate on CWQ remains almost unchanged, the ratio of faithful\\nreasoning still decreases to 48.1%. This implies that even if LLMs can produce correct answers, the\\nreasoning process is still prone to hallucinations and cannot be trusted, which is aligned with the\\nfindings in previous studies (Nguyen et al., 2024).\\nCase Study. We further provide a case study to illustrate the effectiveness of GCR in eliminating\\nhallucinations and ensuring faithful reasoning. As shown in Table 5, the first case demonstrates that,\\nwithout constraints, the model generates an incorrect reasoning path leading to an incorrect answer\\nby hallucinating facts such as “Mabel Rose Ferguson is the child of Naill Ferguson and her parent\\nis Alyssa Mastromonaco”. In contrast, GCR generates a faithful reasoning path grounded in KGs\\nthat “Naill Ferguson has a child named Thomas Ferguson who has a parent named Ayaan Hirsi Ali”.\\nBased on the paths we can reason the correct answer to the question is “Ayaan Hirsi Ali”. In the\\nsecond case, although the LLM answers the question correctly, the generated reasoning path is still\\nhallucinated with incorrect facts. Conversely, GCR conducts faithful reasoning with both correct\\nanswer and reasoning path. These results demonstrate that GCR can effectively eliminate hallucina-\\ntions and ensure faithful reasoning by leveraging KG constraints in graph-constrained decoding.\\n5.4\\nRQ3: ZERO-SHOT GENERALIZABILITY TO UNSEEN KGS\\nTable 6: Zero-shot transferabil-\\nity to other KGQA datasets.\\nModel\\nCSQA\\nMedQA\\nChatGPT\\n79\\n64\\nGCR (ChatGPT)\\n85\\n66\\nGPT-4o-mini\\n91\\n75\\nGCR (GPT-4o-mini)\\n94\\n79\\nIn GCR, the knowledge graph is converted into a constraint which\\nis plugged into the decoding process of LLMs. This allows GCR\\nto generalize to unseen KGs without further training. To evaluate\\nthe generalizability of GCR, we conduct zero-shot transfer ex-\\nperiments on two unseen KGQA datasets: CSQA (Talmor et al.,\\n2019) and MedQA (Jin et al., 2021). Specifically, we use the\\nsame KG-specialized LLM (Llama-3.1-8B) trained on Freebase\\nas well as two general LLMs (ChatGP, GPT-4o-mini). During\\nreasoning, we directly plug the KG-Trie constructed from ConceptNet and medical KGs into the\\nGCR to conduct graph-constrained decoding without additional fine-tuning. The results are shown\\nin Table 6.\\nFrom the results, it is evident that GCR outperforms ChatGPT and GPT-4o-mini in zero-shot per-\\nformance on both datasets. Specifically, GCR shows a 7.6% increase in accuracy on CSQA and a\\n3.1% improvement on MedQA compared to ChatGPT. This highlights the strong zero-shot general-\\nizability of its graph reasoning capabilities to unseen KGs without additional training. However, the\\nimprovement on MedQA is not as significant as that on CSQA. We hypothesize this difference may\\nbe due to LLMs having more common sense knowledge, which aids in reasoning on common sense\\nknowledge graphs effectively. On the other hand, medical KGs are more specialized and require\\ndomain-specific knowledge for reasoning, potentially limiting the generalizability of our method.\\n10\\n6\\nCONCLUSION\\nIn this paper, we introduce a novel LLM reasoning paradigm called graph-constrained reasoning\\n(GCR) to eliminate hallucination and ensure faithful reasoning by incorporating structured KGs. To\\nbridge the unstructured reasoning in LLMs with the structured knowledge in KGs, we propose a\\nKG-Trie to encode paths in KGs using a trie-based index. KG-Trie constrains the decoding process\\nto guide a KG-specialized LLM to generate faithful reasoning paths grounded in KGs. By impos-\\ning constraints, we can not only eliminate hallucination in reasoning but also reduce the reasoning\\ncomplexity, contributing to more efficient and accurate reasoning. Last, a powerful general LLM is\\nutilized as a complement to inductively reason over multiple reasoning paths to generate the final\\nanswer. Extensive experiments demonstrate that GCR excels in faithful reasoning and generalizes\\nwell to reason on new KGs without additional fine-tuning.\\nACKNOWLEDGMENTS\\nWe would want to express our sincere gratitude to Yuan-Fang Li for his valuable feedback and\\nsuggestions during the preparation of this work.\\nREFERENCES\\nGarima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, and Huan Liu. Mindful-rag: A study of\\npoints of failure in retrieval augmented generation. arXiv preprint arXiv:2407.12216, 2024.\\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collab-\\noratively created graph database for structuring human knowledge. In Proceedings of the 2008\\nACM SIGMOD international conference on Management of data, pp. 1247–1250, 2008.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.\\nChen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. Knowledge is flat: A seq2seq generative\\nframework for various knowledge graph completion. In Proceedings of the 29th International\\nConference on Computational Linguistics, pp. 4005–4017, 2022.\\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval.\\nIn International Conference on Learning Representations, 2022.\\nMohammad Dehghan, Mohammad Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi,\\nAbbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye Hao, Qun Liu, Jimmy Lin, Boxing Chen,\\nPrasanna Parthasarathi, Mahdi Biparva, and Mehdi Rezagholizadeh. EWEK-QA : Enhanced web\\nand efficient knowledge graph retrieval for citation-based question answering systems. In Lun-\\nWei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14169–14187,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.764.\\nZixuan Dong, Baoyun Peng, Yufei Wang, Jia Fu, Xiaodong Wang, Yongxue Shan, and Xin Zhou. Ef-\\nfiqa: Efficient question-answering with strategic multi-model collaboration on knowledge graphs.\\narXiv preprint arXiv:2406.01238, 2024.\\nJonathan St BT Evans. Intuition and reasoning: A dual-process perspective. Psychological Inquiry,\\n21(4):313–326, 2010.\\nMarcello Federico, Mauro Cettolo, Fabio Brugnara, and Giuliano Antoniol. Language modelling\\nfor efficient beam-search. Computer Speech and Language, 9(4):353–380, 1995.\\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-\\nhop relational reasoning for knowledge-aware question answering. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1295–1309,\\n2020.\\n11\\nEdward Fredkin. Trie memory. Communications of the ACM, 3(9):490–499, 1960.\\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. Improving multi-hop knowl-\\nedge base question answering by learning intermediate supervision signals. In Proceedings of the\\n14th ACM international conference on web search and data mining, pp. 553–561, 2021.\\nMatthew Douglas Hoffman, Du Phan, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi,\\nPavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A Saurous. Training chain-of-thought\\nvia latent-variable inference. Advances in Neural Information Processing Systems, 36, 2024.\\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.\\nIn Findings of the Association for Computational Linguistics: ACL 2023, pp. 1049–1065, 2023.\\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song,\\nand Denny Zhou.\\nLarge language models cannot self-correct reasoning yet.\\nIn The Twelfth\\nInternational Conference on Learning Representations, 2024.\\nGautier Izacard and ´Edouard Grave. Leveraging passage retrieval with generative models for open\\ndomain question answering. In Proceedings of the 16th Conference of the European Chapter of\\nthe Association for Computational Linguistics: Main Volume, pp. 874–880, 2021.\\nJinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. Unikgqa: Unified retrieval and reasoning\\nfor solving multi-hop question answering over knowledge graph. In The Eleventh International\\nConference on Learning Representations, 2022.\\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Structgpt: A\\ngeneral framework for large language model to reason over structured data. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural Language Processing, pp. 9237–9251, 2023.\\nJinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong\\nWen. Kg-agent: An efficient autonomous agent framework for complex reasoning over knowl-\\nedge graph. arXiv preprint arXiv:2402.11163, 2024.\\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-\\nease does this patient have? a large-scale open domain question answering dataset from medical\\nexams. Applied Sciences, 11(14):6421, 2021.\\nShiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing\\nYin. Graph reasoning for question answering with triplet retrieval. In Findings of the Association\\nfor Computational Linguistics: ACL 2023, pp. 3366–3375, 2023.\\nLinhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and\\ninterpretable large language model reasoning. In International Conference on Learning Repre-\\nsentations, 2024.\\nCostas Mavromatis and George Karypis. Rearev: Adaptive reasoning for question answering over\\nknowledge graphs. In Findings of the Association for Computational Linguistics: EMNLP 2022,\\npp. 2447–2458, 2022.\\nCostas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139, 2024.\\nMeta. Build the future of ai with meta llama 3, 2024. URL https://llama.meta.com/\\nllama3/.\\nThi Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, and Gho-\\nlamreza Haffari. Direct evaluation of chain-of-thought in multi-hop reasoning with knowledge\\ngraphs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association\\nfor Computational Linguistics ACL 2024, pp. 2862–2883, Bangkok, Thailand and virtual meeting,\\nAugust 2024. Association for Computational Linguistics. URL https://aclanthology.\\norg/2024.findings-acl.168.\\nOpenAI. Introducing chatgpt, 2022. URL https://openai.com/index/chatgpt/.\\n12\\nOpenAI. Hello gpt-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\\nOpenAI.\\nNew embedding models and api updates, 2024b.\\nURL https://openai.com/\\nindex/new-embedding-models-and-api-updates/.\\nOpenAI.\\nLearning to reason with llms, 2024c.\\nURL https://openai.com/index/\\nlearning-to-reason-with-llms/.\\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large\\nlanguage models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data\\nEngineering (TKDE), 2024.\\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei\\nHuang, and Huajun Chen. Reasoning with language model prompting: A survey. In Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), pp. 5368–5393, 2023.\\nNils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing. Association for Computational Linguistics, 11 2019.\\nURL https://arxiv.\\norg/abs/1908.10084.\\nDevendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training\\nof multi-document reader and retriever for open-domain question answering. Advances in Neural\\nInformation Processing Systems, 34:25968–25981, 2021.\\nRobyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of\\ngeneral knowledge. In Proceedings of the AAAI conference on artificial intelligence, volume 31,\\n2017.\\nKE Stanovich, RF West, and R Hertwig. Individual differences in reasoning: Implications for the ra-\\ntionality debate?-open peer commentary-the questionable utility of cognitive ability in explaining\\ncognitive illusions. 2000.\\nYuan Sui, Yufei He, Nian Liu, Xiaoxin He, Kun Wang, and Bryan Hooi.\\nFidelis: Faithful\\nreasoning in large language model for knowledge graph question answering.\\narXiv preprint\\narXiv:2405.13873, 2024.\\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and\\nWilliam Cohen. Open domain question answering using early fusion of knowledge bases and\\ntext. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-\\ncessing, pp. 4231–4242, 2018.\\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni,\\nHeung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large\\nlanguage model on knowledge graph. In The Twelfth International Conference on Learning Rep-\\nresentations, 2024.\\nAlon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 641–\\n651, 2018.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\\nguage Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n13\\nKeheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong,\\nand Zhang Xiong. Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-\\nintensive question answering. arXiv preprint arXiv:2308.13259, 2023.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\\nmodels. In The Eleventh International Conference on Learning Representations, 2024.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\\nNeural Information Processing Systems, 35:24824–24837, 2022.\\nWikipedia contributors. Trie. https://en.wikipedia.org/wiki/Trie, 2024. Accessed:\\n2024-09-11.\\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\\ncomprehensive survey on graph neural networks. IEEE transactions on neural networks and\\nlearning systems, 32(1):4–24, 2020.\\nXin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui Chen, Feiyu Xiong, Mosha Chen, and\\nHuajun Chen. From discrimination to generation: Knowledge graph completion with generative\\ntransformer. In Companion Proceedings of the Web Conference 2022, pp. 162–165, 2022.\\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai,\\nJinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng\\nXue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai\\nBai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan\\nZhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang\\nZhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2\\ntechnical report. arXiv preprint arXiv:2407.10671, 2024a.\\nRui Yang, Haoran Liu, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James\\nCaverlee, Yutaka Matsuo, and Irene Li. Kg-rank: Enhancing large language models for medical\\nqa with knowledge graphs and ranking techniques. arXiv preprint arXiv:2403.05881, 2024b.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad-\\nvances in Neural Information Processing Systems, 36, 2024.\\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn:\\nReasoning with language models and knowledge graphs for question answering. In Proceedings\\nof the 2021 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pp. 535–546, 2021.\\nWen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. The value\\nof semantic parse labeling for knowledge base question answering. In Proceedings of the 54th\\nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp.\\n201–206, 2016.\\nPing Yu, Tianlu Wang, Olga Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi\\nGhosh, Mona Diab, and Asli Celikyilmaz. Alert: Adapting language models to reasoning tasks.\\narXiv preprint arXiv:2212.08286, 2022.\\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. Subgraph\\nretrieval enhanced model for multi-hop knowledge base question answering. In Proceedings of the\\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npp. 5773–5784, 2022.\\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to\\naugment large language models. arXiv preprint arXiv:2310.07554, 2023.\\n14\\nYuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang,\\nJinjie Gu, and Huajun Chen. Knowagent: Knowledge-augmented planning for llm-based agents.\\narXiv preprint arXiv:2403.03101, 2024.\\nAppendix\\nTable of Contents\\n7\\nDatasets\\n15\\n8\\nBaselines\\n16\\n9\\nImplementation Details and Experiment Settings\\n17\\n10 Additional Experiment Results\\n19\\n10.1 Performance on Different Hops . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n11 Templates and Prompts\\n19\\n7\\nDATASETS\\nKGQA Datasets. To compare the reasoning performance with existing methods, we use two bench-\\nmark KGQA datasets in this study: WebQuestionSP (WebQSP) (Yih et al., 2016) and Complex We-\\nbQuestions (CWQ) (Talmor & Berant, 2018). To ensure fairness, we adopt the same train and test\\nsplits as previous works (Jiang et al., 2022; Luo et al., 2024). Details of the datasets can be found in\\nTable 7.\\nBoth WebQSP and CWQ can be reasoned using Freebase KGs2 (Bollacker et al., 2008). To reduce\\nthe size of the KGs, we use a subgraph of Freebase by extracting all triples that start from question\\nentities within the maximum reasoning hops provided by previous works3 (Luo et al., 2024). The\\nstatistics of the knowledge graphs are shown in Table 9.\\nFine-tuning Datasets. To enhance the KG reasoning ability of LLMs, we construct fine-tuning\\ndatasets by generating reasoning paths from the KGs. Specifically, we adopt the training split of\\nWebQSP and CWQ, which contain 2,826 and 27,639 question-answer pairs, respectively. For each\\nquestion, we find all the shortest reasoning paths on KGs that connect the question entity to the\\nanswer entity. We then convert the reasoning paths into formatted strings and pair them with the\\nquestion-answer pairs with the template shown in Figure 8 to form the fine-tuning datasets. Since\\nthere could be multiple reasoning paths for a question, we generate multiple training instances paired\\nwith different reasoning paths for each question-answer pair. The fine-tuning datasets contain 28,307\\nand 181,602 question-reasoning path-answer triples for WebQSP and CWQ, respectively. The statis-\\ntics of the fine-tuning datasets are shown in Table 8.\\nZero-shot Generalization Datasets.\\nTo evaluate the transferability of GCR, we further select\\ntwo new KGQA datasets4: CommonsenseQA (CSQA) (Talmor et al., 2019) and MedQA-USMLE\\n(MedQA) (Jin et al., 2021). CSQA is a 5-way multiple choice QA dataset that involves reasoning\\nwith commonsense knowledge. MedQA is a 4-way multiple choice QA task that requires biomed-\\nical and clinical knowledge. For CSQA, we use the ConceptNet (Speer et al., 2017), which is a\\ngeneral-purpose KG that contains commonsense knowledge. For MedQA, we use a medical KG\\n2https://github.com/microsoft/FastRDFStore\\n3WebQSP: https://huggingface.co/datasets/rmanluo/RoG-webqsp, CWQ: https://\\nhuggingface.co/datasets/rmanluo/RoG-cwq\\n4https://github.com/michiyasunaga/qagnn\\n15\\nconstructed from the Unified Medical Language System (Yasunaga et al., 2021). The statistics of\\nthe knowledge graphs are shown in Table 9. We respectively select 100 questions from each dataset.\\nFor each question, following previous studies (Feng et al., 2020; Yasunaga et al., 2021), a 2-hop\\nsubgraph is extracted from the KGs to form the zero-shot generalization datasets.\\nTable 7: Statistics of datasets.\\nDataset\\nDataset Statistics\\nStatistics of Answer Numbers\\n#Train\\n#Test\\n#Ans = 1\\n2 ≥#Ans ≤4\\n5 ≥#Ans ≤9\\n#Ans ≥10\\nWebQSP\\n2,826\\n1,628\\n51.2%\\n27.4%\\n8.3%\\n12.1%\\nCWQ\\n27,639\\n3,531\\n70.6%\\n19.4%\\n6%\\n4%\\nTable 8: Statistics of fine-tuning datasets for graph-constrained decoding.\\nTotal\\nWebQSP\\nCWQ\\n209,909\\n28,307\\n181,602\\nTable 9: Statistics of constructed knowledge graphs.\\nKG\\n#Entities\\n#Relations\\n#Triples\\nFreebase\\n2,566,291\\n7,058\\n8,309,195\\nConceptNet\\n799,273\\n17\\n2,151,303\\nMedKG\\n9,958\\n15\\n49,974\\n8\\nBASELINES\\nWe compare GCR with the 22 baselines grouped into three categories: 1) LLM reasoning methods,\\n2) graph reasoning methods, and 3) KG-enhanced LLM reasoning methods. The details of each\\nbaseline are described as follows.\\nLLM reasoning methods only rely on LLMs for reasoning without utilizing external KGs. We\\ninclude both the vanilla LLMs with different sizes and the LLMs with advanced reasoning mecha-\\nnisms. Specifically, we consider the following baselines:\\n• Qwen2-0.5B/1.5B.7B (Yang et al., 2024a) provides a series of pre-trained LLMs with dif-\\nferent sizes, including 0.5B, 1.5B, and 7B parameters.\\n• Llama-2-7B (Touvron et al., 2023) is a large-scale LLM pre-trained on a diverse range of\\ntasks.\\n• Llama-3.1-8B (Meta, 2024) is the updated version of Llama-2 with more powerful reason-\\ning capabilities.\\n• ChatGPT (OpenAI, 2022) is a powerful closed-source LLM that could follow instructions\\nto conduct complex tasks.\\n• GPT-4o-mini (OpenAI, 2024a) is the new flagship model of OpenAI that could reason\\nacross different modalities and tasks.\\n• Few-shot prompt (Brown et al., 2020) is a few-shot learning method that provides LLMs\\nwith a few examples in the prompts to conduct reasoning.\\n• CoT (Wei et al., 2022) is a chain-of-thought reasoning method that prompts LLMs to gen-\\nerate a chain of reasoning steps.\\n• Self-consistency (Wang et al., 2024) generates multiple reasoning paths and selects the\\nmost consistent answer.\\n16\\nGraph reasoning methods focus on reasoning on KGs using graph neural networks (GNNs) (Wu\\net al., 2020) or graph-based reasoning mechanisms. We include the following baselines:\\n• GraftNet (Sun et al., 2018) is a graph-based reasoning method that retrieves relevant sub-\\ngraphs from KGs with entity linking.\\n• NSM (He et al., 2021) utilizes the sequential model to mimic the multi-hop reasoning\\nprocess on KGs.\\n• SR+NSM (Zhang et al., 2022) proposes a relation-path retrieval to retrieve subgraphs for\\nmulti-hop reasoning.\\n• ReaRev (Mavromatis & Karypis, 2022) is a GNN-based method that reasons on KGs by\\nconsidering complex graph information.\\nKG-enhanced LLM reasoning methods incorporate KGs to enhance the reasoning abilities of\\nLLMs which can be further divided into retrieval-based and agent-based paradigms. We include the\\nfollowing baselines:\\nRetrieval-based methods retrieve relevant facts from KGs with an external retriever and then feed\\nthem into the inputs of LLMs for reasoning:\\n• KD-CoT (Wang et al., 2023) retrieves relevant knowledge from KGs to generate faithful\\nreasoning plans for LLMs.\\n• EWEK-QA (Dehghan et al., 2024) enriches the retrieved knowledge by searching from\\nboth KGs and web.\\n• RoG (Luo et al., 2024) proposes a planning-retrieval-reasoning framework that retrieves\\nreasoning paths from KGs to guide LLMs conducting faithful reasoning.\\n• GNN-RAG (Mavromatis & Karypis, 2024) adopts a lightweight graph neural network to\\neffectively retrieve from KGs.\\n• GNN-RAG+RA (Mavromatis & Karypis, 2024) combines the retrieval results of both RoG\\nand GNN-RAG to enhance the reasoning performance.\\nAgent-based methods treat LLMs as agents that iteratively interact with KGs to find reasoning paths\\nand answers:\\n• ToG (Sun et al., 2024) conducts the reasoning on KGs by exploring multiple paths and\\nconcludes the final answer by aggregating the evidence from them.\\n• EffiQA (Jiang et al., 2024) proposes an efficient agent-based method to reason on KGs.\\n9\\nIMPLEMENTATION DETAILS AND EXPERIMENT SETTINGS\\nIn this section, we will detail the implementation of GCR as well as the experiment settings.\\nFine-tuning KG-specialized LLMs. We fine-tune several lightweight LLMs ranging from 0.5B to\\n8B (Yang et al., 2024a; Touvron et al., 2023; Meta, 2024) on the fine-tuning datasets for 3 epochs.\\nThe batch size is set to 4 and the learning rate is set to 2e-5. We use the cosine learning rate scheduler\\npolicy with the warmup ratio set to 0.03. The training is conducted on 2 A100-80G GPUs for each\\nmodel. The training time and memory usage are shown in Table 10.\\nKGQA Experiment Settings. The KGQA experiment shown in Table 1 aims to compare the rea-\\nsoning performance of GCR with existing methods. For our method, we use the fine-tuned Llama-\\n3.1-8B as KG-specialized LLMs, the general LLM is selected as ChatGPT and GPT-4o-mini. The\\nKG-Trie is constructed from the subgraph of Freebase KGs. The maximum reasoning hops are set\\nto 2 for both WebQSP and CWQ. The beam size is set to 10 for graph-constrained decoding. For\\nvanilla LLMs baselines, we use the zero-shot prompting to ask the models to answer the questions.\\nFor other baselines, we strictly check whether the original papers follow the same settings and copy\\nthe results for fair comparison.\\nEfficiency Analysis Settings. The efficiency analysis shown in Table 2 aims to compare the effi-\\nciency and performance of different methods on WebQSP. For GCR, we use the same settings as the\\n17\\nTable 10: Training time and memory usage for different KG-specialized LLMs.\\nModel\\nTime\\nMem. Usage per GPU\\nQwen2-0.5B\\n3.47h\\n10G\\nQwen2-1.5B\\n4.11h\\n25G\\nQwen2-7B\\n14.37h\\n81G\\nLlama-2-7B\\n13.93h\\n80G\\nLlama-3.1-8B\\n14.52h\\n85G\\nKGQA experiment. For dense retriever methods (e.g., S-Bert (Reimers & Gurevych, 2019), BGE\\n(Zhang et al., 2023), OpenAI-Emb. (OpenAI, 2024b)), we first search all paths within 2-hops on the\\nKGs which are formatted as sentences with the template in Figure 7. Then, we adopt the embedding\\nmodel to encode the path sentences as embeddings which are stored in a vector database. During in-\\nference, we retrieve 10 paths from the vector database with the question as query and feed them into\\nthe LLMs for reasoning. For GNN-RAG (Mavromatis & Karypis, 2024) and RoG (Luo et al., 2024),\\nwe strictly follow the original papers to retrieve reasoning paths and conduct the experiments. For\\nagent-based methods (e.g., ToG (Sun et al., 2024)), we use the same settings detailed in the original\\npapers. For EffiQA (Jiang et al., 2024), since there is no available code, we directly copy the results\\nfrom the original paper.\\nThe average runtime is measured by the time taken to answer the questions. The average number\\nof LLM calls is the number of times the LLMs are called to answer the questions. The average\\nnumber of LLM tokens is the number of tokens inputted into LLMs to answer the questions, such\\nas questions and retrieved reasoning paths. The experiments are conducted on a single A100-80G\\nGPU for each method.\\nAblation Study. In ablation study, we first try to analyze the effectiveness of different components in\\nGCR. We conduct the experiments on WebQSP and CWQ datasets. By removing the KG-specialized\\nLLM (w/o KG-specialized LLM), we search all the 2-hop paths starting from question entities and\\nfeed them into the general LLMs for reasoning. By removing the general LLM (w/o general LLM),\\nwe directly use the hypothesis answers generated by the KG-specialized LLMs as the final answers.\\nDifferent LLMs. We also analyze the different LLMs used for KG-specialized LLMs and general\\nLLMs on WebQSP. For KG-specialized LLMs, we first use the vanilla LLMs with different learning\\ntypes (i.e., zero-shot and few-shot prompting). For zero-shot prompting, we directly ask the models\\nto generate the reasoning paths with the constraints. For few-shot prompting, we provide the models\\nwith a few examples in the prompts to conduct path generation. Detailed prompts can be found in\\nFigures 8 and 10. Then, we fine-tune the lightweight LLMs with different sizes (0.5B to 8B) on the\\ngraph-constrained decoding task. For general LLMs, we use the vanilla LLMs to directly conduct\\nreasoning over multiple reasoning paths. The detailed reasoning prompts can be found in Figure 9.\\nParameter Analysis. We first analyze the performance of GCR with different beam sizes for graph-\\nconstrained decoding. We conduct the experiments on the WebQSP datasets with beam sizes of 1, 3,\\n5, 10, and 20. Then, we analyze the performance of GCR with different hops of paths encoded in the\\nKG-Trie. We conduct the experiments on the WebQSP datasets with maximum paths hops ranging\\nfrom 1 to 4.\\nFaithful Reasoning Analysis. We investigate the effect of the KG constraints on ensuring faithful\\nreasoning. We adopt the fine-tuned Llama-3.1-8B as KG-specialized LLMs. Then, we compare\\nthe faithful reasoning rate and answer hit of GCR with and without the KG constraints in graph-\\nconstrained decoding. The faithful reasoning rate is the percentage of the faithful reasoning in the\\ncorrectly predicted answers. A reasoning path is considered faithful if it can be found in the KGs,\\nand vice versa. The answer hit is the percentage of the correct answers in the predictions.\\nZero-shot Generalization Analysis. We evaluate the transferability of GCR on two zero-shot gen-\\neralization datasets: CSQA and MedQA. We use the fine-tuned Llama-3.1-8B as KG-specialized\\nLLMs and ChatGPT as well as GPT-4o-mini as the general LLMs. The KG-Trie is constructed\\nfrom the subgraph of ConceptNet and MedKG. The maximum reasoning hops are set to 2 for both\\ndatasets. The beam size is set to 10 for graph-constrained decoding. For vanilla LLMs baselines\\n18\\n(i.e., ChatGPT and GPT-4o-mini), we use the zero-shot prompting to ask the models to answer the\\nquestions.\\n10\\nADDITIONAL EXPERIMENT RESULTS\\n10.1\\nPERFORMANCE ON DIFFERENT HOPS\\nIn this section, we analyze the impact of different hops of reasoning paths on the performance of\\nGCR. We conduct the experiments on WebQSP with different maximum hops of reasoning paths\\nencoded in the KG-Trie. The results are shown in Figure 6. We observe that the performance\\nof GCR increases with the number of hops of reasoning paths. The performance peaks when the\\nmaximum hops of reasoning paths are set to 2. This is because the 2-hop paths can provide sufficient\\ninformation for the LLMs to conduct reasoning. When the hops are set to 3 or 4, the performance\\ndrops due to the increased complexity of the reasoning paths, which may introduce noise and make\\nthe reasoning less reliable. Additionally, the size of the KG-Trie slightly increases from 0.5 MB to\\n7.5 MB with the increase of the hops from 1 to 4. This indicates that the KG-Trie can be efficiently\\nconstructed with a small size and guide the LLMs to reason on graphs effectively.\\n1\\n2\\n3\\n4\\nKG-Trie Path Length L\\n0\\n2\\n4\\n6\\nAvg. KG-Trie size (MB)\\n60\\n70\\n80\\n90\\nAnswer Coverage (%)\\nAvg. KG-Trie size (MB)\\nHit\\nF1\\nPrecision\\nRecall\\nFigure 6: Parameter analysis of path hop L for KG-Trie construction on WebQSP.\\n11\\nTEMPLATES AND PROMPTS\\nIn this section, we illustrate all the templates and prompts used in the experiments.\\nPath Sentence Template. The template for converting reasoning paths into natural language sen-\\ntences is shown in Figure 7, where the e∗and r∗denotes the entities and relations in a reasoning\\npath wz = e0\\nr1\\n−→e1\\nr2\\n−→. . .\\nrl\\n−→el,\\nPath Sentence Template\\n<PATH> e1 →r1 →e2 →. . . →rl →el </PATH>\\nFigure 7: The template for converting reasoning paths into formatted sentences.\\nGraph-constrained Decoding Prompt. The prompt for graph-constrained decoding is shown in\\nFigure 8, where the question and mentioned entities are provided to the LLMs to generate rea-\\nsoning paths and hypothesis answers. In the fine-tuning datasets, the supervised LLM outputs are\\nconstructed from the ground-truth answers and reasoning paths extracted from the KGs.\\n19\\nGraph-constrained Decoding Prompt\\n============================= Prompt Input ================================\\nReasoning path is a sequence of triples in the KG that connects the topic entities in the question to\\nanswer entities. Given a question, please generate some reasoning paths in the KG starting from the\\ntopic entities to answer the question.\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n============================= LLM Output ================================\\n# Reasoning Path:\\n<PATH> <Reasoning Path> </PATH>\\n# Answer:\\n<Hypothesis Answer>\\nFigure 8: The prompt template for graph-constrained decoding.\\nThe few-shot prompt template for graph-constrained decoding is shown in Figure 10. We provide a\\nfew examples in the prompts to guide the LLMs to generate reasoning paths. Since the LLMs with\\nfew-shot prompt learning are not fine-tuned on the graph-constrained decoding task, we only apply\\nthe constraint to generate reasoning paths.\\nGraph Inductive Reasoning Prompt. The prompt for graph inductive reasoning is shown in Fig-\\nure 9. We adopt the graph-constrained decoding to generate K reasoning paths and hypothesis\\nanswers for each question. The reasoning paths and hypothesis answers are provided to the general\\nLLMs to answer the questions without fine-tuning.\\nGraph Inductive Reasoning Prompt\\n============================= Prompt Input ================================\\n# Reasoning Paths:\\n<Reasoning Path 1><Hypothesis Answer 1>\\n. . .\\n<Reasoning Path K><Hypothesis Answer K>\\n# Question:\\n<Question>\\nBased on the reasoning paths, please answer the given question. Please keep the answer as simple as\\npossible and only return answers. Please return each answer in a new line.\\n============================= LLM Output ================================\\n<Answer 1>\\n<Answer 2>\\n. . .\\nFigure 9: The prompt template for graph inductive reasoning.\\n20\\nFew-shot Graph-constrained Decoding Prompt\\n============================= Prompt Input ================================\\nReasoning path is a sequence of triples in the KG that connects the topic entities in the question to\\nanswer entities. Given a question, please generate some reasoning paths in the KG starting from the\\ntopic entities to answer the question.\\nExample 1\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n# Reasoning Path:\\n<Reasoning Path>\\nExample 2\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n# Reasoning Path:\\n<Reasoning Path>\\nExample 3\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n# Reasoning Path:\\n<Reasoning Path>\\nInput\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n============================= LLM Output ================================\\n# Reasoning Path:\\n<Reasoning Path>\\nFigure 10: The few-shot prompt template for graph-constrained decoding.\\n21\\n')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Published': '2024-10-16', 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan', 'Summary': 'Large language models (LLMs) have demonstrated impressive reasoning\\nabilities, but they still struggle with faithful reasoning due to knowledge\\ngaps and hallucinations. To address these issues, knowledge graphs (KGs) have\\nbeen utilized to enhance LLM reasoning through their structured knowledge.\\nHowever, existing KG-enhanced methods, either retrieval-based or agent-based,\\nencounter difficulties in accurately retrieving knowledge and efficiently\\ntraversing KGs at scale. In this work, we introduce graph-constrained reasoning\\n(GCR), a novel framework that bridges structured knowledge in KGs with\\nunstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures\\nfaithful KG-grounded reasoning by integrating KG structure into the LLM\\ndecoding process through KG-Trie, a trie-based index that encodes KG reasoning\\npaths. KG-Trie constrains the decoding process, allowing LLMs to directly\\nreason on graphs and generate faithful reasoning paths grounded in KGs.\\nAdditionally, GCR leverages a lightweight KG-specialized LLM for\\ngraph-constrained reasoning alongside a powerful general LLM for inductive\\nreasoning over multiple reasoning paths, resulting in accurate reasoning with\\nzero reasoning hallucination. Extensive experiments on several KGQA benchmarks\\ndemonstrate that GCR achieves state-of-the-art performance and exhibits strong\\nzero-shot generalizability to unseen KGs without additional training.'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Hunter x hunter\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Hunter × Hunter', 'summary': 'Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.', 'source': 'https://en.wikipedia.org/wiki/Hunter_%C3%97_Hunter'}, page_content='Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.\\n\\n\\n== Synopsis ==\\n\\n\\n=== Setting ===\\nHunters (ハンター, Hantā) are licensed, elite members of humanity who are capable of tracking down secret treasures, rare beasts, or even other individuals, and can also access locations that regulars cannot. To obtain a license one must pass the rigorous annual Hunter Examination run by the Hunter Association, which has a success rate of less than one in a hundred-thousand. A Hunter may be awarded up to three stars: a single star for making \"remarkable achievements in a particular field\"; they may then be upgraded to two stars for \"holding an official position\" and mentoring another Hunter up to single star level; and finally upgraded to three stars for \"remarkable achievements in multiple fields\".\\nNen (念) is the ability to control one\\'s own life energy or aura, which is constantly emitted from them, knowingly or not. There are four basic Nen techniques: Ten (纏) maintains the aura in the body, strengthening it for defense; Zetsu (絕) shuts the aura flow off, useful for concealing one\\'s presence and relieving fatigue; Ren (練) enables a user to produce more Nen; and Hatsu (發) is a person\\'s specific use of Nen. Nen users are classified into six types based on their Hatsu abilities; Enhancers (強化系, Kyōkakei) strengthen and reinforce their natural physical abilities; Emitters (放出系, Hōshutsukei) project aura out of their bodies; Manipulators (操作系, Sōsakei) control objects or living things; Transmuters (変化系, Henkakei) change the type or properties of their aura; Conjurers (具現化系, Gugenkakei) create objects out of their aura; and Specialists (特質系, Tokushitsukei) have unique abilities that do not fall into the previous categories. A Nen user can enter into a Contract (誓約, Seiyaku) where, by pledging to follow certain Limitations (制約, Seiyaku), their abilities are strengthened in relation to how strict they are. An example of this is Kurapika who, in order to have an unbreakable chain that will fully restrain members of the Ph'),\n",
       " Document(metadata={'title': 'List of Hunter × Hunter characters', 'summary': \"The Hunter × Hunter manga series, created by Yoshihiro Togashi, features an extensive cast of characters. Such as Shihad Gandhi. It takes place in a fictional universe where licensed specialists known as Hunters travel the world taking on special jobs ranging from treasure hunting to assassination. The story initially focuses on Gon Freecss and his quest to become a Hunter in order to find his father, Ging, who is himself a famous Hunter. On the way, Gon meets and becomes close friends with Killua Zoldyck, Kurapika and Leorio Paradinight.\\nAlthough most characters are human, most possess superhuman strength and/or supernatural abilities due to Nen, the ability to control one's own life energy or aura. The world of the series also includes fantastical beasts such as the Chimera Ants or the Five great calamities.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/List_of_Hunter_%C3%97_Hunter_characters'}, page_content=\"The Hunter × Hunter manga series, created by Yoshihiro Togashi, features an extensive cast of characters. Such as Shihad Gandhi. It takes place in a fictional universe where licensed specialists known as Hunters travel the world taking on special jobs ranging from treasure hunting to assassination. The story initially focuses on Gon Freecss and his quest to become a Hunter in order to find his father, Ging, who is himself a famous Hunter. On the way, Gon meets and becomes close friends with Killua Zoldyck, Kurapika and Leorio Paradinight.\\nAlthough most characters are human, most possess superhuman strength and/or supernatural abilities due to Nen, the ability to control one's own life energy or aura. The world of the series also includes fantastical beasts such as the Chimera Ants or the Five great calamities.\\n\\n\\n== Protagonists ==\\n\\n\\n=== Gon Freecss ===\\n\\nVoiced by: Rica Matsumoto (1998 film), Junko Takeuchi (1999 series), Megumi Han (2011 series) (Japanese); Elinor Holt (1999 series), Erica Mendez (2011 series) (English)\\nGon Freecss (Japanese: ゴン=フリークス, Hepburn: Gon Furīkusu) is an athletic, naïve, and friendly boy. Having spent a lot of time in the woods as a child, he gets along very well with animals and has superhuman senses such as heightened sense of smell and sight, as well as very keen taste. Raised by Mito, Gon wants to become a Hunter in order to find his father, Ging, who is a Hunter as well. During the Hunter Exam Gon befriends Killua Zoldyck, Kurapika and Leorio Paradinight. After successfully becoming a licensed Hunter, Gon and Killua learn about Nen from Wing and later train further under Biscuit Krueger.[ch. 47, 48, 137] After becoming one of the first people to beat Greed Island, and helping to stop the Chimera Ants, Gon meets his father.[ch. 335]. Following the meeting with his father, Gon decides to return to Whale Island and reunites with Mito [ch. 345]. Gon is a popular character with fans, coming in third place in the series' first two popularity polls.\\n\\n\\n=== Killua Zoldyck ===\\n\\nVoiced by: Kanako Mitsuhashi (1999 series), Mariya Ise (2011 series) (Japanese); Annika Odegard (1999 series), Cristina Vee (2011 series) (English)\\nKillua Zoldyck (キルア=ゾルディック, Kirua Zorudikku) is initially introduced as a cheeky, cheerful and mischievous boy who befriends Gon during the Hunter Exam. His ruthlessness and aptitude in killing show the other side of him — deadly, violent, and bloodthirsty. A member of the famous Zoldyck Family of assassins, Killua has been trained to be an assassin since birth and conditioned to possess extreme tolerance for poison, electricity and overall pain. Although Killua fails during his first Hunter Exam by killing an opponent due to his elder brother Illumi's influence, he attends the exam again the following year, earning his license by eliminating all other applicants in the very first trial.[ch. 36, 37, 148] Killua and Gon learn about Nen from Wing and later train further under Biscuit Krueger.[ch. 47, 48, 137] He becomes one of the first people to beat Greed Island, helps stop the Chimera Ants, and uses his sibling Alluka's special abilities to heal the dying Gon. As Gon is about to meet Ging, Killua decides to part ways and travel the world with Alluka [ch. 338].\\nConflicting with Killua's predisposition to kill is his unyielding loyalty to his new friends, as Killua puts them before himself without a single complaint. His greatest flaw is initially his fear of those seemingly more powerful than him,[ch. 210] a result of Illumi's instructions to be extremely cautious and only engage in combat if victory is absolutely certain. Showing great promise from birth, he possesses extraordinary agility and strength as a one-man killing machine. Killua has mastered many killing techniques at a tender age and is set to be one of the best assassins his family has ever produced. His Nen type is Transmutation, which he utilizes by altering his aura into electricity. [ch. 60, 122] His Hatsu involves vari\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
