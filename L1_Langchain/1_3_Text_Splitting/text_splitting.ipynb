{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation:- Text Splitting Using Langchain\n",
    "- In RAG, our first step is DATA Ingestion.\n",
    "- DATA Ingestion: Ingest data from different data sources using Dataloaders.\n",
    "- Loading pdf files, web pages, images, tabular data etc.\n",
    "- In Step 2, we have to transform the data.\n",
    "- Data Transformation: Transform the data into a format that can be used by the model.\n",
    "- This includes text splitting, tokenization, stop words removal, stemming or lemmatization, etc\n",
    "- In this step we convert Data (for example pdf documents) into chunks (eg. text chunks).\n",
    "- Because the model can only process a certain amount of text at a time, we need to split the text.\n",
    "- Every LLM model have their own limitation of context size, in order to take care of that we do text splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Published as a conference paper at ICLR 2024\\nRAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING\\nFOR TREE -ORGANIZED RETRIEVAL\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning\\nStanford University\\npsarthi@cs.stanford.edu\\nABSTRACT\\nRetrieval-augmented language models can better adapt to changes in world state\\nand incorporate long-tail knowledge. However, most existing methods retrieve\\nonly short contiguous chunks from a retrieval corpus, limiting holistic under-\\nstanding of the overall document context. We introduce the novel approach of\\nrecursively embedding, clustering, and summarizing chunks of text, constructing\\na tree with differing levels of summarization from the bottom up. At inference\\ntime, our RAPTOR model retrieves from this tree, integrating information across\\nlengthy documents at different levels of abstraction. Controlled experiments show\\nthat retrieval with recursive summaries offers significant improvements over tra-\\nditional retrieval-augmented LMs on several tasks. On question-answering tasks\\nthat involve complex, multi-step reasoning, we show state-of-the-art results; for\\nexample, by coupling RAPTOR retrieval with the use of GPT-4, we can improve\\nthe best performance on the QuALITY benchmark by 20% in absolute accuracy.\\n1 I NTRODUCTION\\nLarge Language Models (LLMs) have emerged as transformative tools showing impressive perfor-\\nmance on many tasks. With the growing size of LLMs, they can serve standalone as very effective\\nknowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al., 2020;\\nTalmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al.,\\n2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream\\ntasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain-\\nspecific knowledge for particular tasks and the world continues to change, invalidating facts in the\\nLLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,\\nparticularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-\\nnative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\\nquestion as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,\\n2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to\\nsome domain and enabling easy interpretability and provenance tracking, whereas the parametric\\nknowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).\\nNevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that\\nmost existing methods retrieve only a few short, contiguous text chunks, which limits their ability\\nto represent and leverage large-scale discourse structure. This is particularly relevant for thematic\\nquestions that require integrating knowledge from multiple parts of a text, such as understanding\\nan entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tale of\\nCinderella, and the question “How did Cinderella reach her happy ending?”. The top- k retrieved\\nshort contiguous texts will not contain enough context to answer the question.\\nTo address this, we design an indexing and retrieval system that uses a tree structure to capture both\\nhigh-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters\\nchunks of text, generates text summaries of those clusters, and then repeats, generating a tree from\\nthe bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing\\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\\n1\\narXiv:2401.18059v1  [cs.CL]  31 Jan 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Published as a conference paper at ICLR 2024\\n2\\n 3\\n 4\\n 5\\n1\\n1\\n 2\\n3\\n 3\\n4\\n 5\\n5\\n6\\n 8\\n7\\n Index #8\\nText:  summary of \\nnodes 2 and 3\\nChild Nodes: 2, 3\\nText Embedding\\nText chunks\\n3\\n.1\\n4\\n.1\\n5\\n2. Summarization \\nby LLM\\n1. Clustering\\n10\\n7\\n1\\n 2\\n8\\n4\\n3\\n 5\\n6\\n9\\nFormation of one tree layer\\nRoot layer\\nLeaf layer\\nContents of a nodeRAPTOR Tree \\nFigure 1: Tree construction process: RAPTOR recursively clusters chunks of text based on their\\nvector embeddings and generates text summaries of those clusters, constructing a tree from the\\nbottom up. Nodes clustered together are siblings; a parent node contains the text summary of that\\ncluster.\\nOur main contribution is the idea of using text summarization to allow retrieval augmentation of\\ncontext at different scales, and to show its effectiveness in experiments on collections of long doc-\\numents. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),\\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current\\nretrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni-\\nfiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books\\nand movies (NarrativeQA, Koˇcisk`y et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),\\nand multiple-choice questions based on medium-length passages (QuALITY , Pang et al. 2022).1\\n2 R ELATED WORK\\nWhy Retrieval? Recent advances in hardware and algorithms have indeed expanded the con-\\ntext lengths that models can handle, leading to questions about the need for retrieval systems (Dai\\net al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)\\nhave noted, models tend to underutilize long-range context and see diminishing performance as con-\\ntext length increases, especially when pertinent information is embedded within a lengthy context.\\nMoreover, practically, use of long contexts is expensive and slow. This suggests that selecting the\\nmost relevant information for knowledge-intensive tasks is still crucial.\\nRetrieval Methods Retrieval-augmented language models (RALMs) have seen improvements in\\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\\nhave transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and\\nBM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin\\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\\n2022; Sun et al., 2022). Research on the reader component includes Fusion-in-Decoder (FiD)\\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\\nindependently in the encoder andRETRO (Borgeaud et al., 2022; Wang et al., 2023), which utilizes\\ncross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.\\nEnd-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-\\ndecoder model in conjunction with the retriever;REALM (Guu et al., 2020), a bidirectional, masked\\nLM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-\\ntion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\\nretriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-\\ndecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi-\\nerarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements\\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).\\n1We will release the code of RAPTOR publicly here.\\n2'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Published as a conference paper at ICLR 2024\\nDespite a diversity in methods, the retrieving components of models predominantly rely on stan-\\ndard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this\\napproach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous seg-\\nmentation might not capture the complete semantic depth of the text. Reading extracted snippets\\nfrom technical or scientific documents may lack important context making them difficult to read or\\neven misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).\\nRecursive summarization as Context Summarization techniques provide a condensed view of\\ndocuments, enabling more focused engagement with the content (Angelidis & Lapata, 2018). The\\nsummarization/snippet model by Gao et al. (2023) uses summarizations and snippets of passages,\\nwhich improves correctness on most datasets but can sometimes be a lossy means of compression.\\nThe recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition\\nto summarize smaller text chunks, which are later integrated to form summaries of larger sections.\\nWhile this method is effective for capturing broader themes, it can miss granular details. LlamaIndex\\n(Liu, 2022) mitigates this issue by similarly summarizing adjacent text chunks but also retaining\\nintermediate nodes thus storing varying levels of detail, keeping granular details. However, both\\nmethods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still\\noverlook distant interdependencies within the text, which we can find and group with RAPTOR.\\n3 M ETHODS\\nOverview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi-\\ncal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic\\ndepth and connection in reading by building a recursive tree structure that balances broader thematic\\ncomprehension with granular details and which allows nodes to be grouped based on semantic sim-\\nilarity not just order in the text.\\nConstruction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous\\ntexts of length 100, similar to traditional retrieval augmentation techniques. If a sentence exceeds the\\n100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence.\\nThis preserves the contextual and semantic coherence of the text within each chunk. These texts\\nare then embedded using SBERT, a BERT-based encoder ( multi-qa-mpnet-base-cos-v1)\\n(Reimers & Gurevych, 2019). The chunks and their corresponding SBERT embeddings form the\\nleaf nodes of our tree structure.\\nTo group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\\nresulting in a structured, multi-layered tree representation of the original documents. An important\\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\\ncomprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A.\\nFor querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree.\\nThe tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant\\nnodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find\\nthe most relevant ones.\\nClustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text\\nsegments into cohesive groups. This step groups related content together, which helps the subse-\\nquent retrieval process.\\nOne of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\\nbelong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen-\\ntial because individual text segments often contain information relevant to various topics, thereby\\nwarranting their inclusion in multiple summaries.\\nOur clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers\\nboth flexibility and a probabilistic framework. GMMs assume that data points are generated from a\\nmixture of several Gaussian distributions.\\n3'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='Published as a conference paper at ICLR 2024\\nGiven a set of N text segments, each represented as a d-dimensional dense vector embedding, the\\nlikelihood of a text vector, x, given its membership in the kth Gaussian distribution, is denoted by\\nP(x|k) = N(x; µk, Σk). The overall probability distribution is a weighted combination P(x) =PK\\nk=1 πkN(x; µk, Σk), where πk signifies the mixture weight for the kth Gaussian distribution.\\nThe high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-\\ntance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-\\ngarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection\\n(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The\\nnumber of nearest neighbors parameter, n neighbors, in UMAP determines the balance between\\nthe preservation of local and global structures. Our algorithm varies n neighbors to create a hierar-\\nchical clustering structure: it first identifies global clusters and then performs local clustering within\\nthese global clusters. This two-step clustering process captures a broad spectrum of relationships\\namong the text data, from broad themes to specific details.\\nShould a local cluster’s combined context ever exceed the summarization model’s token threshold,\\nour algorithm recursively applies clustering within the cluster, ensuring that the context remains\\nwithin the token threshold.\\nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\\nfor model selection. BIC not only penalizes model complexity but also rewards goodness of fit\\n(Schwarz, 1978). The BIC for a given GMM is BIC = ln(N)k − 2 ln(ˆL), where N is the number\\nof text segments (or data points), k is the number of model parameters, and ˆL is the maximized\\nvalue of the likelihood function of the model. In the context of GMM, the number of parameters k\\nis a function of the dimensionality of the input vectors and the number of clusters.\\nWith the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm\\nis then used to estimate the GMM parameters, namely the means, covariances, and mixture weights.\\nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\\noften exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\\neffective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\\ncontiguous chunks and provide details in Appendix B.\\nModel-Based Summarization After clustering the nodes using Gaussian Mixture Models, the\\nnodes in each cluster are sent to a language model for summarization. This step allows the model\\nto transform large chunks of text into concise, coherent summaries of the selected nodes. For our\\nexperiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-\\ndenses the potentially large volume of retrieved information into a manageable size. We provide\\nstatistics on the compression due to the summarization in Appendix C and the prompt used for\\nsummarization in Appendix D.\\nWhile the summarization model generally produces reliable summaries, a focused annotation study\\nrevealed that about 4% of the summaries contained minor hallucinations. These did not propagate\\nto parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis\\nof hallucinations, refer to the appendix E.\\nQuerying In this section, we elaborate on the two querying mechanisms employed by RAPTOR:\\ntree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered\\nRAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We\\nprovide the pseudocode of both methods in Appendix F. Note that we embed all nodes using SBERT.\\nThe tree traversal method first selects the top-k most relevant root nodes based on their cosine\\nsimilarity to the query embedding. The children of these selected nodes are considered at the next\\nlayer and the top-k nodes are selected from this pool again based on their cosine similarity to the\\nquery vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected\\nnodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:\\n1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the\\nquery embedding and the embeddings of all nodes present at this initial layer.\\n2. Choose the top- k nodes based on the highest cosine similarity scores, forming the set S1.\\n4'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='Published as a conference paper at ICLR 2024\\nFigure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms.Tree traver-\\nsal starts at the root level of the tree and retrieves the top- k (here, top-1) node(s) based on cosine\\nsimilarity to the query vector. At each level, it retrieves the top- k node(s) from the child nodes of\\nthe previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The\\nnodes on which cosine similarity search is performed are highlighted in both illustrations.\\n3. Proceed to the child nodes of the elements in setS1. Compute the cosine similarity between\\nthe query vector and the vector embeddings of these child nodes.\\n4. Select the top k child nodes with the highest cosine similarity scores to the query, forming\\nthe set S2.\\n5. Continue this process recursively for d layers, producing sets S1, S2, . . . , Sd.\\n6. Concatenate sets S1 through Sd to assemble the relevant context to the query.\\nBy adjusting the depth d and the number of nodes k selected at each layer, the tree traversal method\\noffers control over the specificity and breadth of the information retrieved. The algorithm starts with\\na broad outlook by considering the top layers of the tree and progressively focuses on finer details\\nas it descends through the lower layers.\\nThe collapsed tree approach offers a simpler way to search for relevant information by considering\\nall nodes in the tree simultaneously, as depicted in Figure 2. Instead of going layer-by-layer, this\\nmethod flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\\nsame level for comparison. The steps for this method are outlined below:\\n1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted\\nas C, contains nodes from every layer of the original tree.\\n2. Next, calculate the cosine similarity between the query embedding and the embeddings of\\nall nodes present in the collapsed set C.\\n3. Finally, pick the top- k nodes that have the highest cosine similarity scores with the query.\\nKeep adding nodes to the result set until you reach a predefined maximum number of\\ntokens, ensuring you don’t exceed the model’s input limitations.\\nWe tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance\\nof tree traversal with different top- sizes and collapsed tree with different maximum token numbers.\\nThe collapsed tree approach consistently performs better. We believe collapsed tree retrieval is\\nbetter due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes\\nsimultaneously, it retrieves information that is at the correct level of granularity for a given question.\\nIn comparison, while using tree traversal with the same values of d and k, the ratio of nodes from\\neach level of the tree will be constant. So, the ratio of higher-order thematic information to granular\\ndetails will remain the same regardless of the question.\\n5'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Published as a conference paper at ICLR 2024\\nOne drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\\nbe performed on all nodes in the tree. However, this can be made more efficient with fast k-nearest\\nneighbor libraries such as FAISS (Johnson et al., 2019).\\nFigure 3: Comparison of querying methods.\\nResults on 20 stories from the QASPER dataset\\nusing tree traversal with different top-k values,\\nand collapsed tree with different context lengths.\\nCollapsed tree with 2000 tokens produces the best\\nresults, so we use this querying strategy for our\\nmain results.\\nOverall, given the collapsed tree approach’s\\ngreater flexibility and its superior performance\\non the subset of the QASPER dataset, this is\\nthe querying approach with which we proceed.\\nSpecifically, we use the collapsed tree with\\n2000 maximum tokens, which approximately\\nequates to retrieving the top-20 nodes. Using a\\ntoken-based approach ensures the context does\\nnot exceed model context constraints as token\\ncounts can vary across nodes. For experiments\\nwith the UnifiedQA model, we provide 400 to-\\nkens of context, as UnifiedQA has a max con-\\ntext length of 512 tokens. We provide the same\\namount of tokens of context to RAPTOR and to\\nthe baselines.\\nQualitative Study We conduct a qualitative\\nanalysis to understand the benefits of RAP-\\nTOR’s retrieval process compared to Dense\\nPassage Retrieval (DPR) methods. Our study\\nfocuses on thematic, multi-hop questions using\\na 1500-word Cinderella fairytale. As illustrated\\nin Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers,\\nmatching the question’s detail level. This approach often yields more relevant and comprehensive\\ninformation for downstream tasks than DPR. For a detailed discussion and examples, including the\\ntext retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\\n4 E XPERIMENTS\\nDatasets We measure RAPTOR’s performance across three question-answering datasets: Narra-\\ntiveQA, QASPER, and QuALITY .\\nNarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\\nand movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al., 2018; Wu et al., 2021). The\\nNarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\\nto accurately answer its questions, thus testing the model’s ability to comprehend longer texts in\\nthe literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\\nROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the Narra-\\ntiveQA evaluation script used in our experiments.\\nThe QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing\\nfor information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER\\nare categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive. Accuracy is\\nmeasured using standard F1.\\nLastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context\\npassages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for\\nreasoning over the entire document for QA tasks, enabling us to measure the performance of our re-\\ntrieval system on medium-length documents. The dataset includes a challenging subset, QuALITY-\\nHARD, which contains questions that a majority of human annotators answered incorrectly in a\\nspeed-setting. We report accuracies for both the entire test set and the HARD subset.\\nControlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA\\n3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree\\nstructure, on three datasets: QASPER, NarrativeQA, and QuALITY . As shown in Tables 1 and 2,\\n6'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Published as a conference paper at ICLR 2024\\nFigure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions\\nabout the Cinderella story: “What is the central theme of the story?” and “How did Cinderella find\\na happy ending?”. Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s\\nleaf nodes. Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either\\ndirectly or within higher-layer summaries.\\nour results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms\\nthe respective retriever across all datasets. 2\\nSince RAPTOR with SBERT has the best performance, we use it in all subsequent experiments.\\nWe now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and\\nUnifiedQA. As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all\\nthree Language Models on the QASPER dataset. RAPTOR’s F-1 Match scores are 53.1%, 55.7%,\\nand 36.6% when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by\\nmargins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the respective\\nLLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that\\nRAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract\\nthe top-k most similar raw chunks of text, which may not contain the correct response in isolation.\\nTable 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of\\nvarious retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\\ndataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-\\nspective retrieval method.\\nModel ROUGE BLEU-1 BLEU-4 METEOR\\nSBERT with RAPTOR 30.87% 23.50% 6.42% 19.20%\\nSBERT without RAPTOR 29.26% 22.56% 5.95% 18.15%\\nBM25 with RAPTOR 27.93% 21.17% 5.70% 17.03%\\nBM25 without RAPTOR 23.52% 17.73% 4.65% 13.98%\\nDPR with RAPTOR 30.94% 23.51% 6.45% 19.05%\\nDPR without RAPTOR 29.56% 22.84% 6.12% 18.44%\\nLikewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62.4%,\\nwhich is a 2% and 5.1% improvement over DPR and BM25. Similar trends are observed when Uni-\\nfiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2.7% and 6.7%, respectively.\\nFinally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met-\\nrics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other\\nmetrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins\\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\\nthe performance observed in Karpukhin et al. (2020), wheredpr-multiset-base showed superior results.\\n7'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Published as a conference paper at ICLR 2024\\nTable 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\\nparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\\nforms baselines of each respective retrieval method for both datasets.\\nModel Accuracy (QuALITY) Answer F1 (QASPER)\\nSBERT with RAPTOR 56.6% 36.70%\\nSBERT without RAPTOR 54.9% 36.23%\\nBM25 with RAPTOR 52.1% 27.00%\\nBM25 without RAPTOR 49.9% 26.47%\\nDPR with RAPTOR 54.7% 32.23%\\nDPR without RAPTOR 53.1% 31.70%\\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan-\\nguage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title +\\nAbstract” reflects performance when only the title and abstract of the papers are used for context.\\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\\nSpecifically, RAPTOR’s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\\nhigher than BM25.\\nRetriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\\nTitle + Abstract 25.2 22.2 17.5\\nBM25 46.6 50.2 26.4\\nDPR 51.3 53.0 32.1\\nRAPTOR 53.1 55.7 36.6\\nTable 4: Comparison of accuracies on the QuAL-\\nITY dev dataset for two different language mod-\\nels (GPT-3, UnifiedQA 3B) using various retrieval\\nmethods. RAPTOR outperforms the baselines of\\nBM25 and DPR by at least 2.0% in accuracy.\\nModel GPT-3 Acc. UnifiedQA Acc.\\nBM25 57.3 49.9\\nDPR 60.4 53.9\\nRAPTOR 62.4 56.6\\nTable 5: Results on F-1 Match scores of various\\nmodels on the QASPER dataset.\\nModel F-1 Match\\nLongT5 XL (Guo et al., 2022) 53.1\\nCoLT5 XL (Ainslie et al., 2023) 53.9\\nRAPTOR + GPT-4 55.7\\nComparison to State-of-the-art Systems\\nBuilding upon our controlled comparisons,\\nwe examine RAPTOR’s performance relative\\nto other state-of-the-art models. As shown\\nin Table 5, RAPTOR with GPT-4 sets a new\\nbenchmark on QASPER, with a 55.7% F-1\\nscore, surpassing the CoLT5 XL’s score of\\n53.9%.\\nIn the QuALITY dataset, as shown in Table 7,\\nRAPTOR paired with GPT-4 sets a new state-\\nof-the-art with an accuracy of 82.6%, surpass-\\ning the previous best result of 62.3%. In par-\\nticular, it outperforms CoLISA by 21.5% on\\nQuALITY-HARD, which represents questions\\nthat humans took unusually long to correctly\\nanswer, requiring rereading parts of the text,\\ndifficult reasoning, or both.\\nFor the NarrativeQA dataset, as represented in\\nTable 6, RAPTOR paired with UnifiedQA sets\\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by\\nWu et al. (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While\\nWu et al. (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR\\nbenefits from its intermediate layers and clustering approaches, which allows it to capture a range of\\ninformation, from general themes to specific details, contributing to its overall strong performance.\\n4.1 C ONTRIBUTION OF THE TREE STRUCTURE\\nWe examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy-\\npothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring\\na broader understanding of the text.\\n8'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Published as a conference paper at ICLR 2024\\nTable 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing\\non four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with Uni-\\nfiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of-\\nthe-art in the METEOR metric.\\nModel ROUGE-L BLEU-1 BLEU-4 METEOR\\nBiDAF (Koˇcisk`y et al., 2018) 6.2 5 .7 0 .3 3 .7\\nBM25 + BERT (Mou et al., 2020) 15.5 14 .5 1 .4 5 .0\\nRecursively Summarizing Books (Wu et al., 2021) 21.6 22 .3 4 .2 10 .6\\nRetriever + Reader (Izacard & Grave, 2022) 32.0 35.3 7.5 11.1\\nRAPTOR + UnifiedQA 30.8 23.5 6.4 19.1\\nTable 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging\\nhard subset. GPT-4 with RAPTOR sets a new state-of-the-art.\\nModel Accuracy\\nTest Set Hard Subset\\nLongformer-base (Beltagy et al., 2020) 39.5 35 .3\\nDPR and DeBERTaV3-large (Pang et al., 2022) 55.4 46 .1\\nCoLISA (DeBERTaV3-large) (Dong et al., 2023a) 62.3 54 .7\\nRAPTOR + GPT-4 82.6 76.2\\nTable 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL-\\nITY dataset. Columns represent different starting points (highest layer) and rows represent different\\nnumbers of layers queried.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\\n1 layer 57.9 57.8 57.9\\n2 layers - 52.6 63.15\\n3 layers - - 73.68\\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in\\nappendix G. To quantitatively understand the contribution of the upper-level nodes, we used stories\\nfrom the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in\\nSection 3. However, during retrieval, we limit the search to different subsets of layers. For example,\\nwe exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous\\nsubsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree\\nsearch, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.\\nThese findings highlight the importance of the full tree structure in RAPTOR. By providing both\\nthe original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider\\nrange of questions, from higher-order thematic queries to detail-oriented questions. Detailed results\\nfor additional stories and an ablation study on layer contributions can be found in Appendix I.\\n5 C ONCLUSION\\nIn this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\\nparametric knowledge of large language models with contextual information at various levels of\\nabstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\\nhierarchical tree structure that is capable of synthesizing information across various sections of the\\nretrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\\nretrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\\nretrieval methods but also sets new performance benchmarks on several question-answering tasks.\\n9'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Published as a conference paper at ICLR 2024\\n6 R EPRODUCIBILITY STATEMENT\\nLanguage Models for QA and Summarization Four language models are used in our RAPTOR\\nexperiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3,\\ngpt-4, and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,\\nwhich is used for QA tasks, is publicly available at Hugging Face.\\nEvaluation Datasets The three evaluation datasets used in our experiments—QuALITY,\\nQASPER, and NarrativeQA—are all publicly accessible. These datasets ensure that the retrieval\\nand QA tests conducted in this study can be replicated.\\nSource Code The source code for RAPTOR will be publicly available here.\\nREFERENCES\\nCharu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Dis-\\ntance Metrics in High Dimensional Space. In Database Theory—ICDT 2001: 8th International\\nConference London, UK, January 4–6, 2001 Proceedings 8, pp. 420–434. Springer, 2001. URL\\nhttps://link.springer.com/chapter/10.1007/3-540-44503-x_27 .\\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyan-\\nskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range\\ntransformers with conditional computation. arXiv preprint arXiv:2303.09752, 2023. URL\\nhttps://arxiv.org/abs/2303.09752.\\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\\nKelvin Guu. Towards tracing knowledge in language models back to the training data. In\\nFindings of the Association for Computational Linguistics: EMNLP 2022, pp. 2429–2446,\\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.\\nfindings-emnlp.180.\\nStefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment\\nprediction and they are both weakly supervised. arXiv preprint arXiv:1808.08858, 2018. URL\\nhttps://arxiv.org/abs/1808.08858.\\nManoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng\\nHuang. Hybrid hierarchical retrieval for open-domain question answering. In Anna Rogers,\\nJordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational\\nLinguistics: ACL 2023, pp. 10680–10689, Toronto, Canada, July 2023. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.\\norg/2023.findings-acl.679.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer,\\n2020. URL https://arxiv.org/abs/2004.05150. arXiv preprint arXiv:2004.05150.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\\ncan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\\nImproving language models by retrieving from trillions of tokens. In International conference on\\nmachine learning, pp. 2206–2240. PMLR, 2022. URL https://arxiv.org/abs/2112.\\n04426.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\\nZiegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\\nford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In\\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\\nral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc.,\\n10'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Published as a conference paper at ICLR 2024\\n2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/\\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\nS´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General\\nIntelligence: Early Experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. URL\\nhttps://arxiv.org/abs/2303.12712.\\nShuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long\\ndocument summarization. In Proceedings of the 60th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pp. 786–807, Dublin, Ireland, May 2022.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https:\\n//aclanthology.org/2022.acl-long.58.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pp. 1870–1879, Vancouver, Canada, July\\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:\\n//aclanthology.org/P17-1171.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\\nScaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022. URL\\nhttps://arxiv.org/abs/2204.02311.\\nArman Cohan and Nazli Goharian. Contextualizing citations for scientific summarization using\\nword embeddings and domain knowledge. In Proceedings of the 40th International ACM SIGIR\\nConference on Research and Development in Information Retrieval, pp. 1133–1136, 2017. URL\\nhttps://dl.acm.org/doi/abs/10.1145/3077136.3080740.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\\nTransformer-XL: Attentive language models beyond a fixed-length context. InProceedings of the\\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence,\\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\\nhttps://aclanthology.org/P19-1285.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-Awareness.Advances in Neural Information Processing\\nSystems, 35:16344–16359, 2022. URL https://arxiv.org/abs/2205.14135.\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset\\nof Information-Seeking Questions and Answers Anchored in Research Papers. In Proceed-\\nings of the 2021 Conference of the North American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies, pp. 4599–4610, Online, June 2021. Asso-\\nciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https:\\n//aclanthology.org/2021.naacl-main.365.\\nMengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive\\nLearning for Multi-choice Reading Comprehension. In Advances in Information Retrieval: 45th\\nEuropean Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023,\\nProceedings, Part I, pp. 264–278. Springer, 2023a. URL https://link.springer.com/\\nchapter/10.1007/978-3-031-28244-7_17 .\\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with\\ntransformers. arXiv preprint arXiv:2302.14502, 2023b. URL https://arxiv.org/abs/\\n2302.14502.\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/\\nabs/2305.14627.\\n11'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Published as a conference paper at ICLR 2024\\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\\nYinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the\\nAssociation for Computational Linguistics: NAACL 2022, pp. 724–736, Seattle, United States,\\nJuly 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.\\nURL https://aclanthology.org/2022.findings-naacl.55.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented\\nLanguage Model Pre-Training. In International conference on machine learning, pp. 3929–3938.\\nPMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL\\nhttps://arxiv.org/abs/2203.15556.\\nGautier Izacard and Edouard Grave. Distilling Knowledge from Reader to Retriever for Ques-\\ntion Answering, 2022. URL https://arxiv.org/abs/2012.04584. arXiv preprint\\narXiv:2012.04584.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. URL https:\\n//arxiv.org/abs/2208.03299.\\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\\nmodels know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.\\nURL https://arxiv.org/abs/1911.12543.\\nJeff Johnson, Matthijs Douze, and Herv´e J´egou. Billion-Scale Similarity Search with GPUs. IEEE\\nTransactions on Big Data, 7(3):535–547, 2019. URL https://arxiv.org/abs/1702.\\n08734.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\\nModels struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-\\ning, pp. 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/\\nkandpal23a/kandpal23a.pdf.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In\\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP), pp. 6769–6781, Online, November 2020. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\\nemnlp-main.550.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\\nHannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.\\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907,\\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\\nfindings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.\\n171.\\nOmar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via con-\\ntextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR\\nconference on research and development in Information Retrieval, pp. 39–48, 2020. URL\\nhttps://arxiv.org/abs/2004.12832.\\nTom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´abor Melis,\\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions\\nof the Association for Computational Linguistics, 6:317–328, 2018. URL https://arxiv.\\norg/abs/1712.07040.\\n12'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Published as a conference paper at ICLR 2024\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-Augmented Gener-\\nation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems,\\n33:9459–9474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401.\\nJerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index.\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\\narXiv:2307.03172, 2023. URL https://arxiv.org/abs/2307.03172.\\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense\\nhierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing\\nHuang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2021, pp. 188–200, Punta Cana, Dominican Republic, Novem-\\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19.\\nURL https://aclanthology.org/2021.findings-emnlp.19.\\nLeland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation\\nand Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.\\n03426. arXiv preprint arXiv:1802.03426.\\nSewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint\\npassage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang,\\nLucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing, pp. 6997–7008, Online and Punta Cana, Dominican\\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\\nemnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560.\\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\\nZettlemoyer. Nonparametric masked language modeling. In Findings of the Association for\\nComputational Linguistics: ACL 2023, pp. 2097–2118, Toronto, Canada, July 2023. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:\\n//aclanthology.org/2023.findings-acl.132.\\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.\\nMemory-based model editing at scale. In International Conference on Machine Learning,\\npp. 15817–15831. PMLR, 2022. URL https://proceedings.mlr.press/v162/\\nmitchell22a/mitchell22a.pdf.\\nXiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui\\nSu. Frustratingly hard evidence retrieval for QA over books. In Proceedings of the First Joint\\nWorkshop on Narrative Understanding, Storylines, and Events, pp. 108–113, Online, July 2020.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:\\n//aclanthology.org/2020.nuse-1.13.\\nInderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr-\\nishna Karanam, and Sumit Shekhar. A neural CRF-based hierarchical approach for lin-\\near text segmentation. In Findings of the Association for Computational Linguistics: EACL\\n2023, pp. 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.\\nfindings-eacl.65.\\nBenjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa-\\nbased framework for decontextualization.arXiv preprint arXiv:2305.14772, 2023. URL https:\\n//arxiv.org/pdf/2305.14772.pdf.\\nOpenAI. GPT-4 Technical Report. ArXiv, abs/2303.08774, 2023. URL https://arxiv.org/\\nabs/2303.08774.\\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\\nVishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\\n13'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Published as a conference paper at ICLR 2024\\nQuestion Answering with Long Input Texts, Yes! In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pp. 5336–5358, Seattle, United States, July 2022. Association for Computational\\nLinguistics. URL https://aclanthology.org/2022.naacl-main.391.\\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,\\n2019. URL https://arxiv.org/abs/1909.01066.\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\\nMethods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446, 2021.\\nURL https://arxiv.org/abs/2112.11446.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\\nBrown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint\\narXiv:2302.00083, 2023. URL https://arxiv.org/abs/2302.00083.\\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP), pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\\nD19-1410.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into\\nthe Parameters of a Language Model? In Proceedings of the 2020 Conference on Empir-\\nical Methods in Natural Language Processing (EMNLP), pp. 5418–5426, Online, November\\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\\nhttps://aclanthology.org/2020.emnlp-main.437.\\nStephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and\\nBeyond. Foundations and Trends in Information Retrieval, 3(4):333–389, 2009. URL https:\\n//doi.org/10.1561/1500000019.\\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\\net al. Okapi at TREC-3. Nist Special Publication Sp, 109:109, 1995. URL https://www.\\nmicrosoft.com/en-us/research/publication/okapi-at-trec-3/ .\\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\\nZaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-\\nsociation for Computational Linguistics, 11:600–616, 2023. doi: 10.1162/tacl a 00564. URL\\nhttps://aclanthology.org/2023.tacl-1.35.\\nGideon Schwarz. Estimating the Dimension of a Model. The annals of statistics, pp. 461–464,\\n1978. URL https://projecteuclid.org/journals/annals-of-statistics/\\nvolume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\\naos/1176344136.full.\\nKaren Sp ¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\\ntrieval. Journal of documentation, 28(1):11–21, 1972. URL https://doi.org/10.1108/\\neb026526.\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\\nmodels actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia\\nSpecia, and Scott Wen-tau Yih (eds.),Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, pp. 807–822, Online and Punta Cana, Dominican Republic,\\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\\n62. URL https://aclanthology.org/2021.emnlp-main.62.\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\\nmodels. arXiv preprint arXiv:2210.01296, 2022. URL https://arxiv.org/abs/2210.\\n01296.\\n14'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Published as a conference paper at ICLR 2024\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics– on what language\\nmodel pre-training captures. Transactions of the Association for Computational Linguistics, 8:\\n743–758, 2020. URL https://arxiv.org/abs/1912.13283.\\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\\nwith retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:\\n//arxiv.org/abs/2304.06762.\\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\\nChristiano. Recursively Summarizing Books with Human Feedback, 2021. URL https:\\n//arxiv.org/abs/2109.10862.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,\\nand Quoc V . Le. QANet: Combining Local Convolution with Global Self-Attention for Read-\\ning Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint\\narXiv:1804.09541.\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang\\nZhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are\\nstrong context generators, 2022. URL https://arxiv.org/abs/2209.10063.\\nShiyue Zhang, David Wan, and Mohit Bansal. Extractive is not faithful: An investigation of\\nbroad unfaithfulness problems in extractive summarization. In Anna Rogers, Jordan Boyd-\\nGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), pp. 2153–2174, Toronto, Canada, July\\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL\\nhttps://aclanthology.org/2023.acl-long.120.\\nA S CALABILITY AND COMPUTATIONAL EFFICIENCY OF THE\\nTREE -BUILDING PROCESS\\nTo assess the computational efficiency and cost-effectiveness of RAPTOR’s tree-building process,\\nwe conducted experiments on a consumer-grade laptop, specifically an Apple M1 Mac with 16GB\\nof RAM. These experiments aimed to demonstrate the scalability and feasibility of RAPTOR on\\ntypical hardware. We varied the context length from 12,500 to 78,000 tokens and measured both the\\ntoken expenditure and the time required to complete the tree-building process, from initial splitting\\nand embedding to the construction of the final root node.\\nFigure 5: Token cost as a function of document length for QASPER, NarrativeQA, and QuALITY .\\nRAPTOR tree construction costs scale linearly with document length for each of the datasets.\\nToken Expenditure We empirically investigated the relationship between the initial document\\nlength and the total number of tokens expended during the tree-building process, which includes\\nboth the prompt and completion tokens. The document lengths varied significantly across the three\\n15'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='Published as a conference paper at ICLR 2024\\ndatasets examined: QuALITY , QASPER, and NarrativeQA. Figure 5 illustrates a clear linear corre-\\nlation between the initial document length and the total token expenditure, emphasizing that RAP-\\nTOR maintains a linear token scaling regardless of document complexity or length.\\nFigure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAP-\\nTOR tree construction time scales linearly with document length for each of the datasets.\\nBuild Time We also empirically observed a consistent linear trend between the document length\\nand the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of\\ntime, making it a viable solution for efficiently processing large corpora of varying lengths.\\nConclusion Overall, our empirical results indicate that RAPTOR scales both in terms of tokens\\nexpended and build time. Even as the complexity and volume of the input text grow, the cost of\\nconstructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa-\\ntionally efficient and well-suited for processing large and diverse corpora.\\nB A BLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR\\nTo assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted\\nan ablation study on the QuALITY dataset. This study compares RAPTOR’s performance with a\\nbalanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard\\nclustering method.\\nB.1 M ETHODOLOGY\\nBoth configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain\\nconsistency in retrieval. For RAPTOR, we employed our typical clustering and summarization\\nprocess. In contrast, the alternative setup involved creating a balanced tree by recursively encoding\\nand summarizing contiguous text chunks. We determined the window size for this setup based on\\nthe average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose\\na window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models.\\nB.2 R ESULTS & DISCUSSION\\nThe results of the ablation study are presented in table 9. The results from this ablation study clearly\\nindicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the\\nrecency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in\\nRAPTOR is more effective in capturing homogeneous content for summarization, thereby enhancing\\nthe overall retrieval performance.\\n16'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Published as a conference paper at ICLR 2024\\nTable 9: Ablation study results comparing RAPTOR with a recency-based tree approach\\nConfiguration Accuracy\\nRAPTOR + SBERT embeddings + UnifiedQA 56.6%\\nRecency-based tree + SBERT embeddings + UnifiedQA 55.8%\\nC D ATASET STATISTICS AND COMPRESSION RATIOS\\nThe average ratio of the summary length to the sum of child node lengths across all datasets is 0.28,\\nindicating a 72% compression rate. On average, the summary length is 131 tokens, and the average\\nchild node length is 86 tokens. Below are the detailed statistics for all three datasets:\\nTable 10: Statistics of Average Summary Length and Child Node Length Across Datasets\\nDataset Avg.\\nSummary\\nLength\\n(tokens)\\nAvg. Child\\nNode Text\\nLength\\n(tokens)\\nAvg. # of\\nChild Nodes\\nPer Parent\\nAvg.\\nCompression\\nRatio (%)\\nAll Datasets 131 85.6 6.7 .28\\nQuALITY 124.4 87.9 5.7 .28\\nNarrativeQA 129.7 85.5 6.8 .27\\nQASPER 145.9 86.2 5.7 .35\\nD S UMMARIZATION PROMPT\\nTable 11 shows the prompt used for summarization.\\nTable 11: Prompt for Summarization\\nRole Content\\nsystem You are a Summarizing Text Portal\\nuser Write a summary of the following, including as many key details as\\npossible: {context}:\\nE H ALLUCINATION ANALYSIS\\nTo assess the quality and accuracy of the summarizations within our RAPTOR model, we conducted\\nan analysis focusing on hallucinations in the generated summaries. The summaries were generated\\nby gpt-3.5-turbo and subsequently annotated to quantify the rates of hallucinations, to examine\\nwhether such inaccuracies propagate to parent nodes, and to evaluate their impact on question-\\nanswering (QA) tasks.\\nE.1 M ETHODOLOGY\\nWe randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This\\nsampling strategy provides a broad view of the model’s performance across different contexts. Each\\nnode was annotated by hand, and determined if it contained a hallucination.\\nE.2 F INDINGS\\nOut of the 150 nodes sampled, 4% (6 nodes) contained some form of hallucination. Most commonly,\\nthese hallucinations originated from the model adding minor information possibly from its training\\ndata that was not present in the text being summarized, or from incorrectly extrapolating some\\ninformation when creating the summary.\\n17'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Published as a conference paper at ICLR 2024\\nExample:\\nText of the child nodes:\\n”And you will come with me to my people? We may live here among them, and\\nyou will be a great warrior–oh, when Jor dies you may even be chief, for there is\\nnone so mighty as my warrior...”But your father will not permit it–Jor, my father,\\nHigh Chief of the Galus, will not permit it, for like me you are cos-ata-lo. Oh, Co-\\nTan, if we but could!... Bradley noticed that she spoke in English–broken English\\nlike Co-Tan’s but equally appealing.\\nSummary found in the parent of that node:\\nThe protagonist, Bradley, is being asked by Co-Tan to stay with her people and\\nbecome a great warrior, but he refuses and must return to his own country. Tom\\nBillings of Santa Monica arrives and tells them he came to search for a man named\\nBowen J. Tyler, Jr. Ajor, Co-Tan’s sister, is excited about the possibility of going\\nto Tom’s country to see strange and wonderful things...\\nThe hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not\\nexplicitly mention or imply this.\\nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers.\\nGenerally, the hallucinations were minor and did not alter the thematic interpretation of the text.\\nE.3 I MPACT ON QA TASKS\\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-\\ngests that hallucination is not a major concerns for the summarization component in our RAPTOR\\narchitecture.\\nF P SEUDOCODE FOR RETRIEVAL METHODS\\nAlgorithm 1 Tree Traversal Algorithm\\nfunction TRAVERSE TREE (tree, query, k)\\nScurrent ← tree.layer[0]\\nfor layer in range(tree.num layers) do\\ntopk ← []\\nfor node in Scurrent do\\nscore ← dot product(query, node)\\ntop k.append((node, score))\\nend for\\nSlayer ← sorted(top k)[:k].nodes\\nScurrent ← Slayer\\nend for\\nreturn S0 ∪ S1 ∪ S2 ∪ . . .∪ Sk\\nend function\\nG Q UALITATIVE ANALYSIS\\nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions\\nabout a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP-\\nTOR with the context retrieved by Dense Passage Retrieval (DPR). Figure 4 in the main paper details\\nthe retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR\\nselects for each question are highlighted, while the leaf nodes that DPR selects for the same question\\nare indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure.\\nRAPTOR selects nodes from different layers depending on the level of granularity required by the\\n18'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='Published as a conference paper at ICLR 2024\\nAlgorithm 2 Collapsed Tree Algorithm\\nfunction COLLAPSED TREE (tree, query, k,max tokens)\\ntree ← flatten(tree) ▷ Flatten tree into 1D\\ntop nodes ← []\\nfor node in tree do\\ntop nodes.append((node, dot product(query, node))\\nend for\\ntop nodes ← sorted(top nodes)\\nresult ← []\\ntotal tokens ← 0\\nfor node in top nodes do\\nif total tokens + node.token size < max tokens then\\nresult.append(node)\\nend if\\ntotal tokens ← total tokens + node.token size\\nend for\\nreturn result\\nend function\\nQuestion: What is the central theme of the story?\\nRAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\\nto be glad that he had found the glass slipper.\\nDPR Two mice were turned into footmen; four grasshoppers into white horses. Next, the\\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\\nlace. . . . Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\\nQuestion: How does Cinderella find a happy ending?\\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\\nCinderella must return home before the clock strikes eleven or her dress will turn back\\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it\\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.\\nDPR the clock had struck Eleven. . . The Prince was very much surprised when he missed\\nCinderella again, and leaving the ball, went in search of her. . . Fairy touched Cin-\\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\\nto the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the\\nroom before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”\\nTable 12: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\\nfairytale Cinderella.\\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not\\nincluded in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a\\nsummary from a higher layer.\\n”The first question we examine is “How does Cinderella find a happy ending?”, a multi-hop question\\nbest answered by synthesizing information from various text segments. To control for the language\\nmodel’s potential familiarity with the Cinderella story, we instructed it to rely solely on the retrieved\\ninformation for its answers. Table 13 shows the text retrieved by both RAPTOR and DPR for this\\nquestion. RAPTOR’s context succinctly describes Cinderella’s journey to happiness, while DPR’s\\nleaf nodes primarily focus on her initial transformation. The difference in retrieved information\\n19'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='Published as a conference paper at ICLR 2024\\nsignificantly impacts downstream tasks. When GPT-4 is provided with RAPTOR’s context, it gen-\\nerates a detailed answer: “Cinderella finds a happy ending when the Prince searches for the owner\\nof the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transform-\\ning Cinderella’s life for the better.” In contrast, using DPR’s context, GPT-4 states: “Based on the\\ngiven context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks\\ninformation about the story’s conclusion.”\\nThe second question we examine is “What is the central theme of the story?”, a thematic question\\nthat requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for\\nthis question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of\\nall the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of\\na narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance\\nof GPT-4 when answering the question. Given DPR’s context, it outputs “The central theme of\\nthe story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is\\nmagically transformed into a beautiful princess, capturing the attention and admiration of the Prince\\nand others at the ball.” This answer only takes into account the first portion of the story, up until\\nCinderella first meets the prince. In contrast, given RAPTOR’s context, GPT-4 outputs “The central\\ntheme of the story is transformation and overcoming adversity, as Cinderella, with the help of her\\nFairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident\\nyoung woman who ultimately finds happiness and love with the Prince.” This is a more complete\\nanswer, demonstrating a comprehensive understanding of the story.\\nThis qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because\\nthe information that it retrieves is more relevant and exhaustive, allowing for better performance on\\ndownstream tasks.\\nWe also created a 2600-word story along with questions about its narrative and theme. An excerpt\\nfrom the story is present below and the full PDF of this story is linked here. For questions like “What\\nis the central theme of the story?”, an upper-level node is retrieved which includes the sentence:\\n“This story is about the power of human connection... inspiring and uplifting each other as they\\npursued their passions.” This summary, not explicitly present in the original text, almost directly\\nanswers the question.\\nExcerpt from ”The Eager Writer”:\\n”Ethan’s passion for writing had always been a part of him. As a child, he would\\noften scribble stories and poems in his notebook, and as he grew older, his love\\nfor writing only intensified. His evenings were often spent in the dim light of his\\nroom, typing away at his laptop. He had recently taken a job as a content writer\\nfor an online marketing firm to pay the bills, but his heart still longed for the\\nworld of storytelling. However, like many aspiring writers, he struggled to find a\\nfoothold in the industry. He took a job as a content writer for an online marketing\\nfirm, but it was growing increasingly evident to him that this was not the path he\\nwanted to pursue. It was during this time that he stumbled upon the Pathways\\napp. The app offered a platform for people in similar professions to connect and\\nshare knowledge, and he saw it as an opportunity to finally connect with others\\nwho shared his passion for writing. Ethan saw an opportunity to meet others who\\nshared his passion and could offer guidance and mentorship. He quickly signed\\nup and was surprised by the number of writers he found on the platform, from\\nwell establish professionals to beginners just starting out in the business.”\\nH N ARRATIVE QA E VALUATION SCRIPT\\nWe made several modifications to AllenNLP’s evaluation script3 to better fit our evaluation needs:\\n• Added Smoothing: Smoothing was incorporated to handle cases where BLEU score is\\nzero, due to no n-gram matches occurring in the reference text. A BLEU score of zero\\nskews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding\\n3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/\\n20'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Published as a conference paper at ICLR 2024\\na smoothing function, we prevent the BLEU scores from dropping to zero, providing a more\\nfair evaluation.\\n• Modified BLEU-4 Weighting: The original script applied a weight of 1 to the highest\\norder n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0,\\n0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order\\nmatches. To provide a more balanced evaluation, we evenly distributed the weight across\\nall n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25,\\n0.25).\\n• Tokenization before Mapping in METEOR Calculation: The original script utilized a\\nsimple split and map method for METEOR calculation. We fixed this by first tokenizing the\\ntext and then mapping the tokens. This amendment improves the accuracy of the METEOR\\ncalculation by taking into account the correct linguistic boundaries of words.\\nQuestion: What is the central theme of the story?\\nRAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\\nto be glad that he had found the glass slipper.\\nDPR Two mice were turned into footmen; four grasshoppers into white horses. Next, the\\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\\nlace. . . . Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\\nrags, and she had to run home alone. . . They told her a beautiful Princess had been at\\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\\nQuestion: How does Cinderella find a happy ending?\\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\\nCinderella must return home before the clock strikes eleven or her dress will turn back\\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it\\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.\\nDPR the clock had struck Eleven. . . The Prince was very much surprised when he missed\\nCinderella again, and leaving the ball, went in search of her. . . Fairy touched Cin-\\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\\nto the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the\\nroom before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”\\nTable 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\\nfairytale Cinderella.\\nI A NALYSIS OF DIFFERENT LAYERS ON RAPTOR’ S PERFORMANCE\\nI.1 H OW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?\\nIn this section, we present a detailed breakdown of RAPTOR’s retrieval performance when querying\\ndifferent layers of the hierarchical tree structure for various stories. These tables validate the utility\\nof RAPTOR’s multi-layered structure for diverse query requirements.\\nTable 14: Performance of RAPTOR when querying different layers of the tree for Story 2.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\\n1 layer 58.8 47.1 41.1\\n2 layers - 64.7 52.9\\n3 layers - - 47.1\\n21'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 21}, page_content='Published as a conference paper at ICLR 2024\\nFigure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR\\ntree across three datasets (NarrativeQA, Quality, and Qasper) using three retrievers (SBERT, BM25,\\nand DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval\\ncomes from non-leaf layers, with a notable percentage from the first and second layers, highlighting\\nthe importance of RAPTOR’s hierarchical summarization in the retrieval process.\\nTable 15: Performance of RAPTOR when querying different layers of the tree for Story 3.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\\n1 layer 66.6 61.1 61.1\\n2 layers - 66.6 66.6\\n3 layers - - 83.3\\nTable 16: Performance of RAPTOR when querying different layers of the tree for Story 4.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1\\n1 layer 94.7 84.2\\n2 layers - 89.4\\nTable 17: Performance of RAPTOR when querying different layers of the tree for Story 5.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1\\n1 layer 57.9 47.3\\n2 layers - 68.4\\nI.2 W HICH LAYERS DO RETRIEVED NODES COME FROM ?\\nWe further conduct an ablation study across all three datasets and across three different retrievers\\nwith RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes\\noriginate. We observe that between 18.5% to 57% of the retrieved nodes come from non-leaf nodes.\\nAs illustrated in Figure 7, the retrieval pattern across layers reveals the importance of RAPTOR’s\\nmulti-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR\\nusing the DPR retriever for the NarrativeQA dataset come from the first and second layers of the\\ntree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers,\\nalbeit with varying percentages.\\nTable 18: Percentage of nodes from non-leaf nodes across different datasets and retrievers\\nDataset DPR SBERT BM25\\nNarrativeQA 57.36% 36.78% 34.96%\\nQuality 32.28% 24.41% 32.36%\\nQasper 22.93% 18.49% 22.76%\\n22'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 22}, page_content='Published as a conference paper at ICLR 2024\\nTable 19: Percentage of nodes from different layers with DPR as the retriever\\nLayer NarrativeQA Quality Qasper\\n0 42.64% 67.71% 77.07%\\n1 45.00% 29.43% 21.88%\\n2 10.57% 2.85% 1.05%\\n3 1.78% - -\\n4 0.003% - -\\nTable 20: Percentage of nodes from different layers with SBERT as the retriever\\nLayer NarrativeQA Quality Qasper\\n0 63.22% 75.59% 81.51%\\n1 31.51% 22.78% 17.84%\\n2 4.85% 1.63% 0.65%\\n3 0.42% - -\\nTable 21: Percentage of nodes from different layers with BM25 as the retriever\\nLayer NarrativeQA Quality Qasper\\n0 65.04% 67.64% 77.24%\\n1 28.79% 28.85% 21.57%\\n2 5.36% 3.51% 1.19%\\n3 0.81% - -\\n23')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to recursively split text by characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      3\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m      4\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      5\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m final_documents \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m final_documents\n",
      "File \u001b[0;32m~/Desktop/Langchain/langchain/lib/python3.10/site-packages/langchain_text_splitters/base.py:79\u001b[0m, in \u001b[0;36mTextSplitter.create_documents\u001b[0;34m(self, texts, metadatas)\u001b[0m\n\u001b[1;32m     77\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     78\u001b[0m previous_chunk_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     80\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(_metadatas[i])\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_start_index:\n",
      "File \u001b[0;32m~/Desktop/Langchain/langchain/lib/python3.10/site-packages/langchain_text_splitters/character.py:118\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Langchain/langchain/lib/python3.10/site-packages/langchain_text_splitters/character.py:88\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter._split_text\u001b[0;34m(self, text, separators)\u001b[0m\n\u001b[1;32m     86\u001b[0m     separator \u001b[38;5;241m=\u001b[39m _s\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     89\u001b[0m     separator \u001b[38;5;241m=\u001b[39m _s\n\u001b[1;32m     90\u001b[0m     new_separators \u001b[38;5;241m=\u001b[39m separators[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n",
      "File \u001b[0;32m~/Desktop/Langchain/langchain/lib/python3.10/re.py:200\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "final_documents = text_splitter.create_documents(docs) # Helpful for Text Files\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Published as a conference paper at ICLR 2024\\nRAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING\\nFOR TREE -ORGANIZED RETRIEVAL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Stanford University\\npsarthi@cs.stanford.edu\\nABSTRACT'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='ABSTRACT\\nRetrieval-augmented language models can better adapt to changes in world state'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='and incorporate long-tail knowledge. However, most existing methods retrieve'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='only short contiguous chunks from a retrieval corpus, limiting holistic under-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='standing of the overall document context. We introduce the novel approach of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='recursively embedding, clustering, and summarizing chunks of text, constructing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='a tree with differing levels of summarization from the bottom up. At inference'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='time, our RAPTOR model retrieves from this tree, integrating information across'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='lengthy documents at different levels of abstraction. Controlled experiments show'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='that retrieval with recursive summaries offers significant improvements over tra-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='ditional retrieval-augmented LMs on several tasks. On question-answering tasks'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='that involve complex, multi-step reasoning, we show state-of-the-art results; for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='the best performance on the QuALITY benchmark by 20% in absolute accuracy.\\n1 I NTRODUCTION'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Large Language Models (LLMs) have emerged as transformative tools showing impressive perfor-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='mance on many tasks. With the growing size of LLMs, they can serve standalone as very effective'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='knowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al.,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='parameters (Petroni et al., 2019; Jiang et al., 2020;'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Talmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='et al., 2022; Chowdhery et al., 2022; Bubeck et al.,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='specific knowledge for particular tasks and the world continues to change, invalidating facts in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='world continues to change, invalidating facts in the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='LLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='particularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='(Lewis et al., 2020; Mitchell et al., 2022). An alter-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='native approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='after splitting it into chunks (paragraphs), in a separate'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='information retrieval system. Retrieved information is then presented to the LLM along with the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='question as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Lewis et al., 2020; Izacard et al., 2022; Min et al.,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='some domain and enabling easy interpretability and provenance tracking, whereas the parametric'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='knowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Nevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='most existing methods retrieve only a few short, contiguous text chunks, which limits their ability'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='to represent and leverage large-scale discourse structure. This is particularly relevant for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='structure. This is particularly relevant for thematic'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='questions that require integrating knowledge from multiple parts of a text, such as understanding'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tale of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='Cinderella, and the question “How did Cinderella reach her happy ending?”. The top- k retrieved'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='short contiguous texts will not contain enough context to answer the question.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='To address this, we design an indexing and retrieval system that uses a tree structure to capture'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='system that uses a tree structure to capture both'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='chunks of text, generates text summaries of those clusters, and then repeats, generating a tree'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='clusters, and then repeats, generating a tree from'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='the bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='the text at different levels so that it can effectively and efficiently answer questions at'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='effectively and efficiently answer questions at different levels.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}, page_content='1\\narXiv:2401.18059v1  [cs.CL]  31 Jan 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Published as a conference paper at ICLR 2024\\n2\\n 3\\n 4\\n 5\\n1\\n1\\n 2\\n3\\n 3\\n4\\n 5\\n5\\n6\\n 8\\n7\\n Index #8'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='2\\n 3\\n 4\\n 5\\n1\\n1\\n 2\\n3\\n 3\\n4\\n 5\\n5\\n6\\n 8\\n7\\n Index #8\\nText:  summary of \\nnodes 2 and 3\\nChild Nodes: 2, 3'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='nodes 2 and 3\\nChild Nodes: 2, 3\\nText Embedding\\nText chunks\\n3\\n.1\\n4\\n.1\\n5\\n2. Summarization \\nby LLM'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Text chunks\\n3\\n.1\\n4\\n.1\\n5\\n2. Summarization \\nby LLM\\n1. Clustering\\n10\\n7\\n1\\n 2\\n8\\n4\\n3\\n 5\\n6\\n9'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='by LLM\\n1. Clustering\\n10\\n7\\n1\\n 2\\n8\\n4\\n3\\n 5\\n6\\n9\\nFormation of one tree layer\\nRoot layer\\nLeaf layer'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Formation of one tree layer\\nRoot layer\\nLeaf layer\\nContents of a nodeRAPTOR Tree'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Figure 1: Tree construction process: RAPTOR recursively clusters chunks of text based on their'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='vector embeddings and generates text summaries of those clusters, constructing a tree from the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='cluster.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Our main contribution is the idea of using text summarization to allow retrieval augmentation of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='context at different scales, and to show its effectiveness in experiments on collections of long'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='in experiments on collections of long doc-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='uments. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='retrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='fiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='and movies (NarrativeQA, Koˇcisk`y et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='and multiple-choice questions based on medium-length passages (QuALITY , Pang et al. 2022).1'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='2 R ELATED WORK'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Why Retrieval? Recent advances in hardware and algorithms have indeed expanded the con-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='text lengths that models can handle, leading to questions about the need for retrieval systems (Dai'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='et al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='However, as Liu et al. (2023) and Sun et al. (2021)'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='have noted, models tend to underutilize long-range context and see diminishing performance as con-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='text length increases, especially when pertinent information is embedded within a lengthy context.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Moreover, practically, use of long contexts is expensive and slow. This suggests that selecting the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='most relevant information for knowledge-intensive tasks is still crucial.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='Retrieval Methods Retrieval-augmented language models (RALMs) have seen improvements in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='various components: the retriever, the reader, and end-to-end system training. Retrieval methods'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='large language models as retrievers due to their ability to memorize extensive knowledge (Yu et'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='ability to memorize extensive knowledge (Yu et al.,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='2022; Sun et al., 2022). Research on the reader component includes Fusion-in-Decoder (FiD)'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='independently in the encoder andRETRO (Borgeaud et al., 2022; Wang et al., 2023), which utilizes'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='cross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='End-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='decoder model in conjunction with the retriever;REALM (Guu et al., 2020), a bidirectional, masked'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='tion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='retriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='decoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='erarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='in retrieval accuracy by combining document and passage level retrievals and integrating sparse and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='dense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 1}, page_content='1We will release the code of RAPTOR publicly here.\\n2'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Despite a diversity in methods, the retrieving components of models predominantly rely on stan-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='approach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous seg-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='mentation might not capture the complete semantic depth of the text. Reading extracted snippets'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='from technical or scientific documents may lack important context making them difficult to read or'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='even misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Recursive summarization as Context Summarization techniques provide a condensed view of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='documents, enabling more focused engagement with the content (Angelidis & Lapata, 2018). The'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='summarization/snippet model by Gao et al. (2023) uses summarizations and snippets of passages,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='which improves correctness on most datasets but can sometimes be a lossy means of compression.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='The recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='to summarize smaller text chunks, which are later integrated to form summaries of larger sections.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='While this method is effective for capturing broader themes, it can miss granular details.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='broader themes, it can miss granular details. LlamaIndex'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='(Liu, 2022) mitigates this issue by similarly summarizing adjacent text chunks but also retaining'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='intermediate nodes thus storing varying levels of detail, keeping granular details. However, both'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='overlook distant interdependencies within the text, which we can find and group with RAPTOR.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='3 M ETHODS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Overview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='cal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='depth and connection in reading by building a recursive tree structure that balances broader'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='a recursive tree structure that balances broader thematic'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='comprehension with granular details and which allows nodes to be grouped based on semantic sim-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='ilarity not just order in the text.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Construction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='texts of length 100, similar to traditional retrieval augmentation techniques. If a sentence'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='retrieval augmentation techniques. If a sentence exceeds the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='100-token limit, we move the entire sentence to the next chunk, rather than cutting it'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='to the next chunk, rather than cutting it mid-sentence.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='This preserves the contextual and semantic coherence of the text within each chunk. These texts'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='are then embedded using SBERT, a BERT-based encoder ( multi-qa-mpnet-base-cos-v1)'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='(Reimers & Gurevych, 2019). The chunks and their corresponding SBERT embeddings form the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='leaf nodes of our tree structure.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='of embedding, clustering, and summarization continues until further clustering becomes infeasible,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='resulting in a structured, multi-layered tree representation of the original documents. An'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='tree representation of the original documents. An important'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='time and token expenditure, making it suitable for processing large and complex corpora. For a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='comprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='For querying within this tree, we introduce two distinct strategies: tree traversal and collapsed'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='distinct strategies: tree traversal and collapsed tree.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='The tree traversal method traverses the tree layer-by-layer, pruning and selecting the most'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='layer-by-layer, pruning and selecting the most relevant'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='nodes at each level. The collapsed tree method evaluates nodes collectively across all layers to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='evaluates nodes collectively across all layers to find'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='the most relevant ones.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='segments into cohesive groups. This step groups related content together, which helps the subse-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='quent retrieval process.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='belong to multiple clusters without requiring a fixed number of clusters. This flexibility is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='a fixed number of clusters. This flexibility is essen-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='tial because individual text segments often contain information relevant to various topics, thereby'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='warranting their inclusion in multiple summaries.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='both flexibility and a probabilistic framework. GMMs assume that data points are generated from a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 2}, page_content='mixture of several Gaussian distributions.\\n3'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='Given a set of N text segments, each represented as a d-dimensional dense vector embedding, the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='likelihood of a text vector, x, given its membership in the kth Gaussian distribution, is denoted'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='in the kth Gaussian distribution, is denoted by'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='P(x|k) = N(x; µk, Σk). The overall probability distribution is a weighted combination P(x) =PK'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='k=1 πkN(x; µk, Σk), where πk signifies the mixture weight for the kth Gaussian distribution.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='tance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='garwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='number of nearest neighbors parameter, n neighbors, in UMAP determines the balance between'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='the preservation of local and global structures. Our algorithm varies n neighbors to create a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='Our algorithm varies n neighbors to create a hierar-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='chical clustering structure: it first identifies global clusters and then performs local clustering'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='clusters and then performs local clustering within'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='these global clusters. This two-step clustering process captures a broad spectrum of relationships'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='among the text data, from broad themes to specific details.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='Should a local cluster’s combined context ever exceed the summarization model’s token threshold,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='our algorithm recursively applies clustering within the cluster, ensuring that the context remains'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='within the token threshold.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='for model selection. BIC not only penalizes model complexity but also rewards goodness of fit'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='(Schwarz, 1978). The BIC for a given GMM is BIC = ln(N)k − 2 ln(ˆL), where N is the number'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='of text segments (or data points), k is the number of model parameters, and ˆL is the maximized'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='value of the likelihood function of the model. In the context of GMM, the number of parameters k'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='is a function of the dimensionality of the input vectors and the number of clusters.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='is then used to estimate the GMM parameters, namely the means, covariances, and mixture weights.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='our empirical observations suggest that it offers an'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='effective model for our purpose. We run an ablation comparing GMM Clustering with summarizing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='contiguous chunks and provide details in Appendix B.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='Model-Based Summarization After clustering the nodes using Gaussian Mixture Models, the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='nodes in each cluster are sent to a language model for summarization. This step allows the model'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='to transform large chunks of text into concise, coherent summaries of the selected nodes. For our'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='experiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='denses the potentially large volume of retrieved information into a manageable size. We provide'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='statistics on the compression due to the summarization in Appendix C and the prompt used for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='summarization in Appendix D.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='While the summarization model generally produces reliable summaries, a focused annotation study'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='revealed that about 4% of the summaries contained minor hallucinations. These did not propagate'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='to parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='of hallucinations, refer to the appendix E.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='provide the pseudocode of both methods in Appendix F. Note that we embed all nodes using SBERT.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='The tree traversal method first selects the top-k most relevant root nodes based on their cosine'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='similarity to the query embedding. The children of these selected nodes are considered at the next'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='layer and the top-k nodes are selected from this pool again based on their cosine similarity to the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='reach the leaf nodes. Finally, the text from all selected'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='query embedding and the embeddings of all nodes present at this initial layer.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 3}, page_content='2. Choose the top- k nodes based on the highest cosine similarity scores, forming the set S1.\\n4'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms.Tree traver-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='sal starts at the root level of the tree and retrieves the top- k (here, top-1) node(s) based on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='the top- k (here, top-1) node(s) based on cosine'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='similarity to the query vector. At each level, it retrieves the top- k node(s) from the child nodes'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='retrieves the top- k node(s) from the child nodes of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='the previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='the tree into a single layer and retrieves nodes'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='until a threshold number of tokens is reached, based on cosine similarity to the query vector. The'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='nodes on which cosine similarity search is performed are highlighted in both illustrations.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='3. Proceed to the child nodes of the elements in setS1. Compute the cosine similarity between'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='the query vector and the vector embeddings of these child nodes.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='4. Select the top k child nodes with the highest cosine similarity scores to the query, forming'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='the set S2.\\n5. Continue this process recursively for d layers, producing sets S1, S2, . . . , Sd.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='6. Concatenate sets S1 through Sd to assemble the relevant context to the query.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='By adjusting the depth d and the number of nodes k selected at each layer, the tree traversal'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='k selected at each layer, the tree traversal method'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='offers control over the specificity and breadth of the information retrieved. The algorithm starts'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='the information retrieved. The algorithm starts with'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='a broad outlook by considering the top layers of the tree and progressively focuses on finer'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='of the tree and progressively focuses on finer details'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='as it descends through the lower layers.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='The collapsed tree approach offers a simpler way to search for relevant information by considering'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='all nodes in the tree simultaneously, as depicted in Figure 2. Instead of going layer-by-layer,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='in Figure 2. Instead of going layer-by-layer, this'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='method flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='layer, essentially bringing all the nodes onto the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='same level for comparison. The steps for this method are outlined below:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='as C, contains nodes from every layer of the original tree.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='2. Next, calculate the cosine similarity between the query embedding and the embeddings of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='all nodes present in the collapsed set C.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='3. Finally, pick the top- k nodes that have the highest cosine similarity scores with the query.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='Keep adding nodes to the result set until you reach a predefined maximum number of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='tokens, ensuring you don’t exceed the model’s input limitations.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='We tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='of tree traversal with different top- sizes and collapsed tree with different maximum token'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='and collapsed tree with different maximum token numbers.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='The collapsed tree approach consistently performs better. We believe collapsed tree retrieval is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='better due to offering greater flexibility than tree traversal; i.e., by searching through all the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='traversal; i.e., by searching through all the nodes'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='simultaneously, it retrieves information that is at the correct level of granularity for a given'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='at the correct level of granularity for a given question.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='In comparison, while using tree traversal with the same values of d and k, the ratio of nodes from'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='each level of the tree will be constant. So, the ratio of higher-order thematic information to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='the ratio of higher-order thematic information to granular'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 4}, page_content='details will remain the same regardless of the question.\\n5'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='One drawback, however, of the collapsed tree approach is that it requires cosine similarity search'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='is that it requires cosine similarity search to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='be performed on all nodes in the tree. However, this can be made more efficient with fast k-nearest'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='neighbor libraries such as FAISS (Johnson et al., 2019).\\nFigure 3: Comparison of querying methods.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Figure 3: Comparison of querying methods.\\nResults on 20 stories from the QASPER dataset'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Results on 20 stories from the QASPER dataset\\nusing tree traversal with different top-k values,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='and collapsed tree with different context lengths.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Collapsed tree with 2000 tokens produces the best\\nresults, so we use this querying strategy for our'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='results, so we use this querying strategy for our\\nmain results.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='main results.\\nOverall, given the collapsed tree approach’s'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Overall, given the collapsed tree approach’s\\ngreater flexibility and its superior performance'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='greater flexibility and its superior performance\\non the subset of the QASPER dataset, this is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='on the subset of the QASPER dataset, this is\\nthe querying approach with which we proceed.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='the querying approach with which we proceed.\\nSpecifically, we use the collapsed tree with'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Specifically, we use the collapsed tree with\\n2000 maximum tokens, which approximately'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='2000 maximum tokens, which approximately\\nequates to retrieving the top-20 nodes. Using a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='equates to retrieving the top-20 nodes. Using a\\ntoken-based approach ensures the context does'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='token-based approach ensures the context does\\nnot exceed model context constraints as token'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='not exceed model context constraints as token\\ncounts can vary across nodes. For experiments'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='counts can vary across nodes. For experiments\\nwith the UnifiedQA model, we provide 400 to-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='with the UnifiedQA model, we provide 400 to-\\nkens of context, as UnifiedQA has a max con-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='kens of context, as UnifiedQA has a max con-\\ntext length of 512 tokens. We provide the same'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='text length of 512 tokens. We provide the same\\namount of tokens of context to RAPTOR and to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='amount of tokens of context to RAPTOR and to\\nthe baselines.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='the baselines.\\nQualitative Study We conduct a qualitative'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Qualitative Study We conduct a qualitative\\nanalysis to understand the benefits of RAP-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='analysis to understand the benefits of RAP-\\nTOR’s retrieval process compared to Dense'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='TOR’s retrieval process compared to Dense\\nPassage Retrieval (DPR) methods. Our study'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Passage Retrieval (DPR) methods. Our study\\nfocuses on thematic, multi-hop questions using'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='focuses on thematic, multi-hop questions using\\na 1500-word Cinderella fairytale. As illustrated'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='in Figure 4, RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='matching the question’s detail level. This approach often yields more relevant and comprehensive'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='information for downstream tasks than DPR. For a detailed discussion and examples, including the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='text retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='4 E XPERIMENTS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Datasets We measure RAPTOR’s performance across three question-answering datasets: Narra-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='tiveQA, QASPER, and QuALITY .'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='NarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='and movie transcripts, totaling 1,572 documents (Ko ˇcisk`y et al., 2018; Wu et al., 2021). The'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='NarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='to accurately answer its questions, thus testing the model’s ability to comprehend longer texts in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='the literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='ROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the Narra-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='tiveQA evaluation script used in our experiments.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='The QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='for information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='are categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive. Accuracy is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='measured using standard F1.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Lastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='passages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='reasoning over the entire document for QA tasks, enabling us to measure the performance of our re-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='trieval system on medium-length documents. The dataset includes a challenging subset, QuALITY-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='HARD, which contains questions that a majority of human annotators answered incorrectly in a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='speed-setting. We report accuracies for both the entire test set and the HARD subset.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='and DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 5}, page_content='structure, on three datasets: QASPER, NarrativeQA, and QuALITY . As shown in Tables 1 and 2,\\n6'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Figure 4: Querying Process: Illustration of how RAPTOR retrieves information for two questions'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='about the Cinderella story: “What is the central theme of the story?” and “How did Cinderella find'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='a happy ending?”. Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='leaf nodes. Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='directly or within higher-layer summaries.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='our results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='the respective retriever across all datasets. 2'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Since RAPTOR with SBERT has the best performance, we use it in all subsequent experiments.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='We now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='UnifiedQA. As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='three Language Models on the QASPER dataset. RAPTOR’s F-1 Match scores are 53.1%, 55.7%,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='and 36.6% when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='margins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='BM25 by 6.5, 5.5, and 10.2 points across the respective'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='LLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='RAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='the top-k most similar raw chunks of text, which may not contain the correct response in isolation.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Table 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='dataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='spective retrieval method.\\nModel ROUGE BLEU-1 BLEU-4 METEOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Model ROUGE BLEU-1 BLEU-4 METEOR\\nSBERT with RAPTOR 30.87% 23.50% 6.42% 19.20%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='SBERT with RAPTOR 30.87% 23.50% 6.42% 19.20%\\nSBERT without RAPTOR 29.26% 22.56% 5.95% 18.15%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='SBERT without RAPTOR 29.26% 22.56% 5.95% 18.15%\\nBM25 with RAPTOR 27.93% 21.17% 5.70% 17.03%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='BM25 with RAPTOR 27.93% 21.17% 5.70% 17.03%\\nBM25 without RAPTOR 23.52% 17.73% 4.65% 13.98%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='BM25 without RAPTOR 23.52% 17.73% 4.65% 13.98%\\nDPR with RAPTOR 30.94% 23.51% 6.45% 19.05%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='DPR with RAPTOR 30.94% 23.51% 6.45% 19.05%\\nDPR without RAPTOR 29.56% 22.84% 6.12% 18.44%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Likewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62.4%,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='which is a 2% and 5.1% improvement over DPR and BM25. Similar trends are observed when Uni-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='fiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2.7% and 6.7%, respectively.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='Finally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple met-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='rics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='metrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='ranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='dpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='experiments done earlier. This decision was based on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='the performance observed in Karpukhin et al. (2020), wheredpr-multiset-base showed superior'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='(2020), wheredpr-multiset-base showed superior results.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 6}, page_content='7'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Table 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='parison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='DPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='forms baselines of each respective retrieval method for both datasets.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Model Accuracy (QuALITY) Answer F1 (QASPER)\\nSBERT with RAPTOR 56.6% 36.70%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='SBERT with RAPTOR 56.6% 36.70%\\nSBERT without RAPTOR 54.9% 36.23%\\nBM25 with RAPTOR 52.1% 27.00%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='BM25 with RAPTOR 52.1% 27.00%\\nBM25 without RAPTOR 49.9% 26.47%\\nDPR with RAPTOR 54.7% 32.23%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='DPR with RAPTOR 54.7% 32.23%\\nDPR without RAPTOR 53.1% 31.70%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Table 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title +'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Abstract” reflects performance when only the title and abstract of the papers are used for context.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='RAPTOR outperforms the established baselines BM25 and DPR across all tested language models.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Specifically, RAPTOR’s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='higher than BM25.\\nRetriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Title + Abstract 25.2 22.2 17.5\\nBM25 46.6 50.2 26.4\\nDPR 51.3 53.0 32.1\\nRAPTOR 53.1 55.7 36.6'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='DPR 51.3 53.0 32.1\\nRAPTOR 53.1 55.7 36.6\\nTable 4: Comparison of accuracies on the QuAL-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Table 4: Comparison of accuracies on the QuAL-\\nITY dev dataset for two different language mod-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='ITY dev dataset for two different language mod-\\nels (GPT-3, UnifiedQA 3B) using various retrieval'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='els (GPT-3, UnifiedQA 3B) using various retrieval\\nmethods. RAPTOR outperforms the baselines of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='methods. RAPTOR outperforms the baselines of\\nBM25 and DPR by at least 2.0% in accuracy.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='BM25 and DPR by at least 2.0% in accuracy.\\nModel GPT-3 Acc. UnifiedQA Acc.\\nBM25 57.3 49.9'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Model GPT-3 Acc. UnifiedQA Acc.\\nBM25 57.3 49.9\\nDPR 60.4 53.9\\nRAPTOR 62.4 56.6'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='BM25 57.3 49.9\\nDPR 60.4 53.9\\nRAPTOR 62.4 56.6\\nTable 5: Results on F-1 Match scores of various'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Table 5: Results on F-1 Match scores of various\\nmodels on the QASPER dataset.\\nModel F-1 Match'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='models on the QASPER dataset.\\nModel F-1 Match\\nLongT5 XL (Guo et al., 2022) 53.1'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Model F-1 Match\\nLongT5 XL (Guo et al., 2022) 53.1\\nCoLT5 XL (Ainslie et al., 2023) 53.9'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='CoLT5 XL (Ainslie et al., 2023) 53.9\\nRAPTOR + GPT-4 55.7\\nComparison to State-of-the-art Systems'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Comparison to State-of-the-art Systems\\nBuilding upon our controlled comparisons,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Building upon our controlled comparisons,\\nwe examine RAPTOR’s performance relative'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='we examine RAPTOR’s performance relative\\nto other state-of-the-art models. As shown'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='to other state-of-the-art models. As shown\\nin Table 5, RAPTOR with GPT-4 sets a new'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='in Table 5, RAPTOR with GPT-4 sets a new\\nbenchmark on QASPER, with a 55.7% F-1'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='benchmark on QASPER, with a 55.7% F-1\\nscore, surpassing the CoLT5 XL’s score of\\n53.9%.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='score, surpassing the CoLT5 XL’s score of\\n53.9%.\\nIn the QuALITY dataset, as shown in Table 7,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='In the QuALITY dataset, as shown in Table 7,\\nRAPTOR paired with GPT-4 sets a new state-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='RAPTOR paired with GPT-4 sets a new state-\\nof-the-art with an accuracy of 82.6%, surpass-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='of-the-art with an accuracy of 82.6%, surpass-\\ning the previous best result of 62.3%. In par-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='ing the previous best result of 62.3%. In par-\\nticular, it outperforms CoLISA by 21.5% on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='ticular, it outperforms CoLISA by 21.5% on\\nQuALITY-HARD, which represents questions'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='QuALITY-HARD, which represents questions\\nthat humans took unusually long to correctly'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='that humans took unusually long to correctly\\nanswer, requiring rereading parts of the text,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='answer, requiring rereading parts of the text,\\ndifficult reasoning, or both.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='difficult reasoning, or both.\\nFor the NarrativeQA dataset, as represented in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='For the NarrativeQA dataset, as represented in\\nTable 6, RAPTOR paired with UnifiedQA sets'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='a new state-of-the-art METEOR score. When compared to the recursively summarizing model by'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Wu et al. (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='Wu et al. (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='benefits from its intermediate layers and clustering approaches, which allows it to capture a range'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='approaches, which allows it to capture a range of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='information, from general themes to specific details, contributing to its overall strong'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='details, contributing to its overall strong performance.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='4.1 C ONTRIBUTION OF THE TREE STRUCTURE'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='We examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 7}, page_content='a broader understanding of the text.\\n8'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Table 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='on four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with Uni-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='fiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='the-art in the METEOR metric.\\nModel ROUGE-L BLEU-1 BLEU-4 METEOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Model ROUGE-L BLEU-1 BLEU-4 METEOR\\nBiDAF (Koˇcisk`y et al., 2018) 6.2 5 .7 0 .3 3 .7'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='BM25 + BERT (Mou et al., 2020) 15.5 14 .5 1 .4 5 .0'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Recursively Summarizing Books (Wu et al., 2021) 21.6 22 .3 4 .2 10 .6'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Retriever + Reader (Izacard & Grave, 2022) 32.0 35.3 7.5 11.1\\nRAPTOR + UnifiedQA 30.8 23.5 6.4 19.1'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Table 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='hard subset. GPT-4 with RAPTOR sets a new state-of-the-art.\\nModel Accuracy\\nTest Set Hard Subset'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Model Accuracy\\nTest Set Hard Subset\\nLongformer-base (Beltagy et al., 2020) 39.5 35 .3'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='DPR and DeBERTaV3-large (Pang et al., 2022) 55.4 46 .1'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='CoLISA (DeBERTaV3-large) (Dong et al., 2023a) 62.3 54 .7\\nRAPTOR + GPT-4 82.6 76.2'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Table 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='ITY dataset. Columns represent different starting points (highest layer) and rows represent'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='points (highest layer) and rows represent different'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='numbers of layers queried.\\nLayers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='1 layer 57.9 57.8 57.9\\n2 layers - 52.6 63.15\\n3 layers - - 73.68'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='We validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='qualitatively. We present qualitative analysis in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='appendix G. To quantitatively understand the contribution of the upper-level nodes, we used stories'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='from the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='Section 3. However, during retrieval, we limit the search to different subsets of layers. For'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='the search to different subsets of layers. For example,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='we exclusively retrieve from the leaf nodes and each upper layer, as well as from different'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='and each upper layer, as well as from different contiguous'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='subsets of the layers. We show findings specific to one story in Table 8, revealing that a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='to one story in Table 8, revealing that a full-tree'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='search, utilizing all layers, outperformed retrieval strategies that focused only on specific'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='strategies that focused only on specific layers.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='These findings highlight the importance of the full tree structure in RAPTOR. By providing both'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='the original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='range of questions, from higher-order thematic queries to detail-oriented questions. Detailed'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='queries to detail-oriented questions. Detailed results'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='for additional stories and an ablation study on layer contributions can be found in Appendix I.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='5 C ONCLUSION'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='parametric knowledge of large language models with contextual information at various levels of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='abstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='hierarchical tree structure that is capable of synthesizing information across various sections of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='information across various sections of the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='retrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='retrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 8}, page_content='retrieval methods but also sets new performance benchmarks on several question-answering tasks.\\n9'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Published as a conference paper at ICLR 2024\\n6 R EPRODUCIBILITY STATEMENT'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Language Models for QA and Summarization Four language models are used in our RAPTOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='experiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='gpt-4, and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='which is used for QA tasks, is publicly available at Hugging Face.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Evaluation Datasets The three evaluation datasets used in our experiments—QuALITY,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='QASPER, and NarrativeQA—are all publicly accessible. These datasets ensure that the retrieval'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='and QA tests conducted in this study can be replicated.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Source Code The source code for RAPTOR will be publicly available here.\\nREFERENCES'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Dis-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='tance Metrics in High Dimensional Space. In Database Theory—ICDT 2001: 8th International'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Conference London, UK, January 4–6, 2001 Proceedings 8, pp. 420–434. Springer, 2001. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='https://link.springer.com/chapter/10.1007/3-540-44503-x_27 .'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyan-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='skiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='transformers with conditional computation. arXiv preprint arXiv:2303.09752, 2023. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='https://arxiv.org/abs/2303.09752.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Kelvin Guu. Towards tracing knowledge in language models back to the training data. In'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2429–2446,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='doi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.\\nfindings-emnlp.180.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Stefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='prediction and they are both weakly supervised. arXiv preprint arXiv:1808.08858, 2018. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='https://arxiv.org/abs/1808.08858.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Huang. Hybrid hierarchical retrieval for open-domain question answering. In Anna Rogers,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Linguistics: ACL 2023, pp. 10680–10689, Toronto, Canada, July 2023. Association for Computa-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='tional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='org/2023.findings-acl.679.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='2020. URL https://arxiv.org/abs/2004.05150. arXiv preprint arXiv:2004.05150.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Improving language models by retrieving from trillions of tokens. In International conference on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='machine learning, pp. 2206–2240. PMLR, 2022. URL https://arxiv.org/abs/2112.\\n04426.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='04426.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='ford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 9}, page_content='ral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc.,\\n10'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Intelligence: Early Experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='https://arxiv.org/abs/2303.12712.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Shuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='document summarization. In Proceedings of the 60th Annual Meeting of the Association for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Computational Linguistics (Volume 1: Long Papers), pp. 786–807, Dublin, Ireland, May 2022.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='//aclanthology.org/2022.acl-long.58.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Computational Linguistics (Volume 1: Long Papers), pp. 1870–1879, Vancouver, Canada, July'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='//aclanthology.org/P17-1171.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='https://arxiv.org/abs/2204.02311.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Arman Cohan and Nazli Goharian. Contextualizing citations for scientific summarization using'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='word embeddings and domain knowledge. In Proceedings of the 40th International ACM SIGIR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Conference on Research and Development in Information Retrieval, pp. 1133–1136, 2017. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='https://dl.acm.org/doi/abs/10.1145/3077136.3080740.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Transformer-XL: Attentive language models beyond a fixed-length context. InProceedings of the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='https://aclanthology.org/P19-1285.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='memory-efficient exact attention with IO-Awareness.Advances in Neural Information Processing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Systems, 35:16344–16359, 2022. URL https://arxiv.org/abs/2205.14135.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='of Information-Seeking Questions and Answers Anchored in Research Papers. In Proceed-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='ings of the 2021 Conference of the North American Chapter of the Association for Computa-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='tional Linguistics: Human Language Technologies, pp. 4599–4610, Online, June 2021. Asso-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='ciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='//aclanthology.org/2021.naacl-main.365.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Mengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Learning for Multi-choice Reading Comprehension. In Advances in Information Retrieval: 45th'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Proceedings, Part I, pp. 264–278. Springer, 2023a. URL https://link.springer.com/'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='chapter/10.1007/978-3-031-28244-7_17 .'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='transformers. arXiv preprint arXiv:2302.14502, 2023b. URL https://arxiv.org/abs/\\n2302.14502.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='text with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/\\nabs/2305.14627.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 10}, page_content='abs/2305.14627.\\n11'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Association for Computational Linguistics: NAACL 2022, pp. 724–736, Seattle, United States,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='URL https://aclanthology.org/2022.findings-naacl.55.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Language Model Pre-Training. In International conference on machine learning, pp. 3929–3938.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='PMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='https://arxiv.org/abs/2203.15556.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Gautier Izacard and Edouard Grave. Distilling Knowledge from Reader to Retriever for Ques-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='tion Answering, 2022. URL https://arxiv.org/abs/2012.04584. arXiv preprint\\narXiv:2012.04584.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='trieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='//arxiv.org/abs/2208.03299.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='URL https://arxiv.org/abs/1911.12543.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Jeff Johnson, Matthijs Douze, and Herv´e J´egou. Billion-Scale Similarity Search with GPUs. IEEE'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Transactions on Big Data, 7(3):535–547, 2019. URL https://arxiv.org/abs/1702.\\n08734.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='08734.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Models struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='ing, pp. 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='kandpal23a/kandpal23a.pdf.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='(EMNLP), pp. 6769–6781, Online, November 2020. Association for Computational Linguis-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='tics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\\nemnlp-main.550.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='findings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.\\n171.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='171.\\nOmar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via con-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='textualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='conference on research and development in Information Retrieval, pp. 39–48, 2020. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='https://arxiv.org/abs/2004.12832.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='Tom´aˇs Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´abor Melis,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='and Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='of the Association for Computational Linguistics, 6:317–328, 2018. URL https://arxiv.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 11}, page_content='org/abs/1712.07040.\\n12'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Heinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-Augmented Gener-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='ation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='33:9459–9474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Jerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='arXiv:2307.03172, 2023. URL https://arxiv.org/abs/2307.03172.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='hierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Compu-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='tational Linguistics: EMNLP 2021, pp. 188–200, Punta Cana, Dominican Republic, Novem-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='ber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='URL https://aclanthology.org/2021.findings-emnlp.19.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='and Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='03426. arXiv preprint arXiv:1802.03426.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='passage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Methods in Natural Language Processing, pp. 6997–7008, Online and Punta Cana, Dominican'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='emnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Zettlemoyer. Nonparametric masked language modeling. In Findings of the Association for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Computational Linguistics: ACL 2023, pp. 2097–2118, Toronto, Canada, July 2023. Associ-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='ation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='//aclanthology.org/2023.findings-acl.132.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Memory-based model editing at scale. In International Conference on Machine Learning,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='pp. 15817–15831. PMLR, 2022. URL https://proceedings.mlr.press/v162/\\nmitchell22a/mitchell22a.pdf.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Su. Frustratingly hard evidence retrieval for QA over books. In Proceedings of the First Joint'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Workshop on Narrative Understanding, Storylines, and Events, pp. 108–113, Online, July 2020.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Association for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='//aclanthology.org/2020.nuse-1.13.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='ishna Karanam, and Sumit Shekhar. A neural CRF-based hierarchical approach for lin-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='ear text segmentation. In Findings of the Association for Computational Linguistics: EACL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='2023, pp. 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='tics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.\\nfindings-eacl.65.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='based framework for decontextualization.arXiv preprint arXiv:2305.14772, 2023. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='//arxiv.org/pdf/2305.14772.pdf.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='OpenAI. GPT-4 Technical Report. ArXiv, abs/2303.08774, 2023. URL https://arxiv.org/\\nabs/2303.08774.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 12}, page_content='Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\\n13'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Question Answering with Long Input Texts, Yes! In Proceedings of the 2022 Conference of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='the North American Chapter of the Association for Computational Linguistics: Human Language'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Technologies, pp. 5336–5358, Seattle, United States, July 2022. Association for Computational'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Linguistics. URL https://aclanthology.org/2022.naacl-main.391.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Fabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='2019. URL https://arxiv.org/abs/1909.01066.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Methods, Analysis & Insights from Training Gopher. arXiv preprint arXiv:2112.11446, 2021.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='URL https://arxiv.org/abs/2112.11446.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='arXiv:2302.00083, 2023. URL https://arxiv.org/abs/2302.00083.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='guage Processing and the 9th International Joint Conference on Natural Language Processing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='(EMNLP-IJCNLP), pp. 3982–3992, Hong Kong, China, November 2019. Association for Com-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='putational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/\\nD19-1410.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='D19-1410.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='the Parameters of a Language Model? In Proceedings of the 2020 Conference on Empir-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='ical Methods in Natural Language Processing (EMNLP), pp. 5418–5426, Online, November'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='https://aclanthology.org/2020.emnlp-main.437.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Stephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Beyond. Foundations and Trends in Information Retrieval, 3(4):333–389, 2009. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='//doi.org/10.1561/1500000019.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='et al. Okapi at TREC-3. Nist Special Publication Sp, 109:109, 1995. URL https://www.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='microsoft.com/en-us/research/publication/okapi-at-trec-3/ .'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Zaheer. Questions are all you need to train a dense passage retriever. Transactions of the As-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='sociation for Computational Linguistics, 11:600–616, 2023. doi: 10.1162/tacl a 00564. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='https://aclanthology.org/2023.tacl-1.35.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Gideon Schwarz. Estimating the Dimension of a Model. The annals of statistics, pp. 461–464,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='1978. URL https://projecteuclid.org/journals/annals-of-statistics/'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\\naos/1176344136.full.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Karen Sp ¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='trieval. Journal of documentation, 28(1):11–21, 1972. URL https://doi.org/10.1108/\\neb026526.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='models actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Specia, and Scott Wen-tau Yih (eds.),Proceedings of the 2021 Conference on Empirical Methods'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='in Natural Language Processing, pp. 807–822, Online and Punta Cana, Dominican Republic,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='62. URL https://aclanthology.org/2021.emnlp-main.62.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 13}, page_content='models. arXiv preprint arXiv:2210.01296, 2022. URL https://arxiv.org/abs/2210.\\n01296.\\n14'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics– on what language'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='model pre-training captures. Transactions of the Association for Computational Linguistics, 8:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='743–758, 2020. URL https://arxiv.org/abs/1912.13283.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='//arxiv.org/abs/2304.06762.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Christiano. Recursively Summarizing Books with Human Feedback, 2021. URL https:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='//arxiv.org/abs/2109.10862.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='and Quoc V . Le. QANet: Combining Local Convolution with Global Self-Attention for Read-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='ing Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint\\narXiv:1804.09541.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='strong context generators, 2022. URL https://arxiv.org/abs/2209.10063.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Shiyue Zhang, David Wan, and Mohit Bansal. Extractive is not faithful: An investigation of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='broad unfaithfulness problems in extractive summarization. In Anna Rogers, Jordan Boyd-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='for Computational Linguistics (Volume 1: Long Papers), pp. 2153–2174, Toronto, Canada, July'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='https://aclanthology.org/2023.acl-long.120.\\nA S CALABILITY AND COMPUTATIONAL EFFICIENCY OF THE'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='TREE -BUILDING PROCESS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='To assess the computational efficiency and cost-effectiveness of RAPTOR’s tree-building process,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='we conducted experiments on a consumer-grade laptop, specifically an Apple M1 Mac with 16GB'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='of RAM. These experiments aimed to demonstrate the scalability and feasibility of RAPTOR on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='typical hardware. We varied the context length from 12,500 to 78,000 tokens and measured both the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='token expenditure and the time required to complete the tree-building process, from initial'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='complete the tree-building process, from initial splitting'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='and embedding to the construction of the final root node.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Figure 5: Token cost as a function of document length for QASPER, NarrativeQA, and QuALITY .'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='RAPTOR tree construction costs scale linearly with document length for each of the datasets.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='Token Expenditure We empirically investigated the relationship between the initial document'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='length and the total number of tokens expended during the tree-building process, which includes'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='both the prompt and completion tokens. The document lengths varied significantly across the three'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 14}, page_content='15'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='datasets examined: QuALITY , QASPER, and NarrativeQA. Figure 5 illustrates a clear linear corre-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='lation between the initial document length and the total token expenditure, emphasizing that RAP-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='TOR maintains a linear token scaling regardless of document complexity or length.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='Figure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAP-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='TOR tree construction time scales linearly with document length for each of the datasets.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='Build Time We also empirically observed a consistent linear trend between the document length'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='and the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='time, making it a viable solution for efficiently processing large corpora of varying lengths.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='Conclusion Overall, our empirical results indicate that RAPTOR scales both in terms of tokens'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='expended and build time. Even as the complexity and volume of the input text grow, the cost of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='constructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='tionally efficient and well-suited for processing large and diverse corpora.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='B A BLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='To assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='an ablation study on the QuALITY dataset. This study compares RAPTOR’s performance with a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='balanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='clustering method.\\nB.1 M ETHODOLOGY'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='Both configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='consistency in retrieval. For RAPTOR, we employed our typical clustering and summarization'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='process. In contrast, the alternative setup involved creating a balanced tree by recursively'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='involved creating a balanced tree by recursively encoding'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='and summarizing contiguous text chunks. We determined the window size for this setup based on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='the average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='a window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='B.2 R ESULTS & DISCUSSION'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='The results of the ablation study are presented in table 9. The results from this ablation study'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='in table 9. The results from this ablation study clearly'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='our hypothesis that the clustering strategy in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='RAPTOR is more effective in capturing homogeneous content for summarization, thereby enhancing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 15}, page_content='the overall retrieval performance.\\n16'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Table 9: Ablation study results comparing RAPTOR with a recency-based tree approach'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Configuration Accuracy\\nRAPTOR + SBERT embeddings + UnifiedQA 56.6%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='RAPTOR + SBERT embeddings + UnifiedQA 56.6%\\nRecency-based tree + SBERT embeddings + UnifiedQA 55.8%'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='C D ATASET STATISTICS AND COMPRESSION RATIOS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='The average ratio of the summary length to the sum of child node lengths across all datasets is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='sum of child node lengths across all datasets is 0.28,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='indicating a 72% compression rate. On average, the summary length is 131 tokens, and the average'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='child node length is 86 tokens. Below are the detailed statistics for all three datasets:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Table 10: Statistics of Average Summary Length and Child Node Length Across Datasets\\nDataset Avg.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Dataset Avg.\\nSummary\\nLength\\n(tokens)\\nAvg. Child\\nNode Text\\nLength\\n(tokens)\\nAvg. # of\\nChild Nodes'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Node Text\\nLength\\n(tokens)\\nAvg. # of\\nChild Nodes\\nPer Parent\\nAvg.\\nCompression\\nRatio (%)'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Child Nodes\\nPer Parent\\nAvg.\\nCompression\\nRatio (%)\\nAll Datasets 131 85.6 6.7 .28'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Ratio (%)\\nAll Datasets 131 85.6 6.7 .28\\nQuALITY 124.4 87.9 5.7 .28\\nNarrativeQA 129.7 85.5 6.8 .27'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='NarrativeQA 129.7 85.5 6.8 .27\\nQASPER 145.9 86.2 5.7 .35\\nD S UMMARIZATION PROMPT'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='QASPER 145.9 86.2 5.7 .35\\nD S UMMARIZATION PROMPT\\nTable 11 shows the prompt used for summarization.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Table 11 shows the prompt used for summarization.\\nTable 11: Prompt for Summarization\\nRole Content'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Table 11: Prompt for Summarization\\nRole Content\\nsystem You are a Summarizing Text Portal'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='user Write a summary of the following, including as many key details as\\npossible: {context}:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='possible: {context}:\\nE H ALLUCINATION ANALYSIS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='To assess the quality and accuracy of the summarizations within our RAPTOR model, we conducted'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='an analysis focusing on hallucinations in the generated summaries. The summaries were generated'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='by gpt-3.5-turbo and subsequently annotated to quantify the rates of hallucinations, to examine'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='whether such inaccuracies propagate to parent nodes, and to evaluate their impact on question-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='answering (QA) tasks.\\nE.1 M ETHODOLOGY'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='We randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='sampling strategy provides a broad view of the model’s performance across different contexts. Each'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='node was annotated by hand, and determined if it contained a hallucination.\\nE.2 F INDINGS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='Out of the 150 nodes sampled, 4% (6 nodes) contained some form of hallucination. Most commonly,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='these hallucinations originated from the model adding minor information possibly from its training'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='data that was not present in the text being summarized, or from incorrectly extrapolating some'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 16}, page_content='information when creating the summary.\\n17'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Published as a conference paper at ICLR 2024\\nExample:\\nText of the child nodes:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Text of the child nodes:\\n”And you will come with me to my people? We may live here among them, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='you will be a great warrior–oh, when Jor dies you may even be chief, for there is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='none so mighty as my warrior...”But your father will not permit it–Jor, my father,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='High Chief of the Galus, will not permit it, for like me you are cos-ata-lo. Oh, Co-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Tan, if we but could!... Bradley noticed that she spoke in English–broken English'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='like Co-Tan’s but equally appealing.\\nSummary found in the parent of that node:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='The protagonist, Bradley, is being asked by Co-Tan to stay with her people and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='become a great warrior, but he refuses and must return to his own country. Tom'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Billings of Santa Monica arrives and tells them he came to search for a man named'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Bowen J. Tyler, Jr. Ajor, Co-Tan’s sister, is excited about the possibility of going'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='to Tom’s country to see strange and wonderful things...'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='The hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='that Jr. Ajor and Co-Tan are sisters, but does not'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='explicitly mention or imply this.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Upon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Generally, the hallucinations were minor and did not alter the thematic interpretation of the text.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='E.3 I MPACT ON QA TASKS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='In our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='gests that hallucination is not a major concerns for the summarization component in our RAPTOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='architecture.\\nF P SEUDOCODE FOR RETRIEVAL METHODS\\nAlgorithm 1 Tree Traversal Algorithm'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Algorithm 1 Tree Traversal Algorithm\\nfunction TRAVERSE TREE (tree, query, k)'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='function TRAVERSE TREE (tree, query, k)\\nScurrent ← tree.layer[0]'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Scurrent ← tree.layer[0]\\nfor layer in range(tree.num layers) do\\ntopk ← []\\nfor node in Scurrent do'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='topk ← []\\nfor node in Scurrent do\\nscore ← dot product(query, node)\\ntop k.append((node, score))'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='top k.append((node, score))\\nend for\\nSlayer ← sorted(top k)[:k].nodes\\nScurrent ← Slayer\\nend for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='Scurrent ← Slayer\\nend for\\nreturn S0 ∪ S1 ∪ S2 ∪ . . .∪ Sk\\nend function\\nG Q UALITATIVE ANALYSIS'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='To qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='TOR with the context retrieved by Dense Passage Retrieval (DPR). Figure 4 in the main paper details'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='the retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='selects for each question are highlighted, while the leaf nodes that DPR selects for the same'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='the leaf nodes that DPR selects for the same question'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='are indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 17}, page_content='RAPTOR selects nodes from different layers depending on the level of granularity required by the\\n18'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='Published as a conference paper at ICLR 2024\\nAlgorithm 2 Collapsed Tree Algorithm'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='Algorithm 2 Collapsed Tree Algorithm\\nfunction COLLAPSED TREE (tree, query, k,max tokens)'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='tree ← flatten(tree) ▷ Flatten tree into 1D\\ntop nodes ← []\\nfor node in tree do'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='top nodes ← []\\nfor node in tree do\\ntop nodes.append((node, dot product(query, node))\\nend for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='end for\\ntop nodes ← sorted(top nodes)\\nresult ← []\\ntotal tokens ← 0\\nfor node in top nodes do'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='total tokens ← 0\\nfor node in top nodes do\\nif total tokens + node.token size < max tokens then'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='result.append(node)\\nend if\\ntotal tokens ← total tokens + node.token size\\nend for\\nreturn result'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='end for\\nreturn result\\nend function\\nQuestion: What is the central theme of the story?'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='RAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='impresses the Prince at the ball. . . she loses track of time and has to run home alone in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='the darkness. The Prince is unable to find Cinderella and goes in search of her . . . She'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='forgave her sisters, and treated them always very kindly, and the Prince had great cause'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='to be glad that he had found the glass slipper.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='DPR Two mice were turned into footmen; four grasshoppers into white horses. Next, the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='Fairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='lace. . . . Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='the rat and mice ran quickly away when they saw her; while all her fine dress turned to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='rags, and she had to run home alone. . . They told her a beautiful Princess had been at'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='the ball, with whom the Prince was delighted. They did not know it was Cinderella.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='Question: How does Cinderella find a happy ending?'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='RAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='into a grand coach with her wand and allows Cinderella to attend the ball. However,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='Cinderella must return home before the clock strikes eleven or her dress will turn back'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='into rags. . . Cinderella impresses the Prince at the ball but leaves before he can find'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='out who she is. . . The Prince searched for the owner of a lost glass slipper and found it'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='belonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='her.\\nDPR the clock had struck Eleven. . . The Prince was very much surprised when he missed'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='Cinderella again, and leaving the ball, went in search of her. . . Fairy touched Cin-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='derella’s rags, and they became rich satin robes, trimmed with point lace... Her old'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='shoes became a charming pair of glass slippers, which shone like diamonds. “Now go'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='to the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='room before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='rags.”\\nTable 12: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='fairytale Cinderella.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='question at hand. Further, the information that would be retrieved by DPR is more often than not'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='included in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='directly as a leaf node or indirectly as part of a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='summary from a higher layer.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='”The first question we examine is “How does Cinderella find a happy ending?”, a multi-hop question'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='best answered by synthesizing information from various text segments. To control for the language'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='model’s potential familiarity with the Cinderella story, we instructed it to rely solely on the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='story, we instructed it to rely solely on the retrieved'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='information for its answers. Table 13 shows the text retrieved by both RAPTOR and DPR for this'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='question. RAPTOR’s context succinctly describes Cinderella’s journey to happiness, while DPR’s'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='leaf nodes primarily focus on her initial transformation. The difference in retrieved information'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 18}, page_content='19'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='significantly impacts downstream tasks. When GPT-4 is provided with RAPTOR’s context, it gen-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='erates a detailed answer: “Cinderella finds a happy ending when the Prince searches for the owner'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='of the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transform-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='ing Cinderella’s life for the better.” In contrast, using DPR’s context, GPT-4 states: “Based on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='using DPR’s context, GPT-4 states: “Based on the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='given context, it is not possible to determine how Cinderella finds a happy ending, as the text'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='how Cinderella finds a happy ending, as the text lacks'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='information about the story’s conclusion.”'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='The second question we examine is “What is the central theme of the story?”, a thematic question'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='that requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='this question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='all the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='retrieved by DPR contains detailed descriptions of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='a narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='of GPT-4 when answering the question. Given DPR’s context, it outputs “The central theme of'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='the story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='beauty, as Cinderella, a kind and humble girl, is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='magically transformed into a beautiful princess, capturing the attention and admiration of the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='capturing the attention and admiration of the Prince'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='and others at the ball.” This answer only takes into account the first portion of the story, up'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='into account the first portion of the story, up until'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='Cinderella first meets the prince. In contrast, given RAPTOR’s context, GPT-4 outputs “The central'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='theme of the story is transformation and overcoming adversity, as Cinderella, with the help of her'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='Fairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='young woman who ultimately finds happiness and love with the Prince.” This is a more complete'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='answer, demonstrating a comprehensive understanding of the story.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='This qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='the information that it retrieves is more relevant and exhaustive, allowing for better performance'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='and exhaustive, allowing for better performance on'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='downstream tasks.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='We also created a 2600-word story along with questions about its narrative and theme. An excerpt'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='from the story is present below and the full PDF of this story is linked here. For questions like'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='of this story is linked here. For questions like “What'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='is the central theme of the story?”, an upper-level node is retrieved which includes the sentence:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='“This story is about the power of human connection... inspiring and uplifting each other as they'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='pursued their passions.” This summary, not explicitly present in the original text, almost directly'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='answers the question.\\nExcerpt from ”The Eager Writer”:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='”Ethan’s passion for writing had always been a part of him. As a child, he would'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='often scribble stories and poems in his notebook, and as he grew older, his love'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='for writing only intensified. His evenings were often spent in the dim light of his'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='room, typing away at his laptop. He had recently taken a job as a content writer'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='for an online marketing firm to pay the bills, but his heart still longed for the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='world of storytelling. However, like many aspiring writers, he struggled to find a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='foothold in the industry. He took a job as a content writer for an online marketing'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='firm, but it was growing increasingly evident to him that this was not the path he'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='wanted to pursue. It was during this time that he stumbled upon the Pathways'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='app. The app offered a platform for people in similar professions to connect and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='share knowledge, and he saw it as an opportunity to finally connect with others'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='who shared his passion for writing. Ethan saw an opportunity to meet others who'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='shared his passion and could offer guidance and mentorship. He quickly signed'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='up and was surprised by the number of writers he found on the platform, from'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='well establish professionals to beginners just starting out in the business.”'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='H N ARRATIVE QA E VALUATION SCRIPT'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='We made several modifications to AllenNLP’s evaluation script3 to better fit our evaluation needs:'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='• Added Smoothing: Smoothing was incorporated to handle cases where BLEU score is'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='zero, due to no n-gram matches occurring in the reference text. A BLEU score of zero'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='skews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 19}, page_content='3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/\\n20'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='a smoothing function, we prevent the BLEU scores from dropping to zero, providing a more'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='fair evaluation.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='• Modified BLEU-4 Weighting: The original script applied a weight of 1 to the highest'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='order n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='matches. To provide a more balanced evaluation, we evenly distributed the weight across'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='all n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25,\\n0.25).'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='0.25).\\n• Tokenization before Mapping in METEOR Calculation: The original script utilized a'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='simple split and map method for METEOR calculation. We fixed this by first tokenizing the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='text and then mapping the tokens. This amendment improves the accuracy of the METEOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='calculation by taking into account the correct linguistic boundaries of words.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Question: What is the central theme of the story?'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='RAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='impresses the Prince at the ball. . . she loses track of time and has to run home alone in'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='the darkness. The Prince is unable to find Cinderella and goes in search of her . . . She'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='forgave her sisters, and treated them always very kindly, and the Prince had great cause'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='to be glad that he had found the glass slipper.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='DPR Two mice were turned into footmen; four grasshoppers into white horses. Next, the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Fairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='lace. . . . Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='the rat and mice ran quickly away when they saw her; while all her fine dress turned to'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='rags, and she had to run home alone. . . They told her a beautiful Princess had been at'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='the ball, with whom the Prince was delighted. They did not know it was Cinderella.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Question: How does Cinderella find a happy ending?'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='RAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='into a grand coach with her wand and allows Cinderella to attend the ball. However,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Cinderella must return home before the clock strikes eleven or her dress will turn back'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='into rags. . . Cinderella impresses the Prince at the ball but leaves before he can find'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='out who she is. . . The Prince searched for the owner of a lost glass slipper and found it'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='belonged to Cinderella. She forgave her sisters and the Prince was glad to have found\\nher.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='her.\\nDPR the clock had struck Eleven. . . The Prince was very much surprised when he missed'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Cinderella again, and leaving the ball, went in search of her. . . Fairy touched Cin-'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='derella’s rags, and they became rich satin robes, trimmed with point lace... Her old'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='shoes became a charming pair of glass slippers, which shone like diamonds. “Now go'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='to the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='room before the clock strikes eleven. If you do not your dress will return to its original\\nrags.”'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='rags.”\\nTable 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='fairytale Cinderella.\\nI A NALYSIS OF DIFFERENT LAYERS ON RAPTOR’ S PERFORMANCE'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='I.1 H OW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='In this section, we present a detailed breakdown of RAPTOR’s retrieval performance when querying'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='different layers of the hierarchical tree structure for various stories. These tables validate the'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='for various stories. These tables validate the utility'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='of RAPTOR’s multi-layered structure for diverse query requirements.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Table 14: Performance of RAPTOR when querying different layers of the tree for Story 2.'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1 Layer 2\\n1 layer 58.8 47.1 41.1'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 20}, page_content='1 layer 58.8 47.1 41.1\\n2 layers - 64.7 52.9\\n3 layers - - 47.1\\n21'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 21}, page_content='Published as a conference paper at ICLR 2024'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 21}, page_content='Figure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 21}, page_content='tree across three datasets (NarrativeQA, Quality, and Qasper) using three retrievers (SBERT, BM25,'),\n",
       " Document(metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 21}, page_content='and DPR). The data indicate that a substantial portion of the nodes contributing to the final'),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "final_documents = text_splitter.split_documents(docs)\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap of 50 characters from page 1 into page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Published as a conference paper at ICLR 2024\n",
      "RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING' metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}\n",
      "page_content='RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING\n",
      "FOR TREE -ORGANIZED RETRIEVAL' metadata={'source': '/Users/surajbhardwaj/Desktop/Langchain/L1_Langchain/1_2_Data_Ingestion/Raptor.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[0])\n",
    "print(final_documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='A text file (sometimes spelled textfile; \\nan old alternative name is flat file) is a kind of computer file \\nthat is structured as a sequence of lines of electronic text. \\nA text file exists stored as data within a computer file system.\\n\\nIn operating systems such as CP/M, where the operating system does \\nnot keep track of the file size in bytes, the end of a text file is \\ndenoted by placing one or more special characters, \\nknown as an end-of-file (EOF) marker, as padding after the last \\nline in a text file. In modern operating systems such as DOS, \\nMicrosoft Windows and Unix-like systems, \\ntext files do not contain any special EOF character, \\nbecause file systems on those operating systems \\nkeep track of the file size in bytes.\\n\\nSome operating systems, such as Multics, Unix-like systems, \\nCP/M, DOS, the classic Mac OS, and Windows, \\nstore text files as a sequence of bytes, \\nwith an end-of-line delimiter at the end of each line. \\nOther operating systems, such as OpenVMS and OS/360 \\nand its successors, have record-oriented filesystems, \\nin which text files are stored as a sequence either of \\nfixed-length records or of variable-length records with a \\nrecord-length value in the record header.\\n\\n\"Text file\" refers to a type of container, \\nwhile plain text refers to a type of content.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A text file (sometimes spelled textfile; \\nan old alternative name is flat file) is a kind of computer file \\nthat is structured as a sequence of lines of electronic text. \\nA text file exists stored as data within a computer file system.\\n\\nIn operating systems such as CP/M, where the operating system does \\nnot keep track of the file size in bytes, the end of a text file is \\ndenoted by placing one or more special characters, \\nknown as an end-of-file (EOF) marker, as padding after the last \\nline in a text file. In modern operating systems such as DOS, \\nMicrosoft Windows and Unix-like systems, \\ntext files do not contain any special EOF character, \\nbecause file systems on those operating systems \\nkeep track of the file size in bytes.\\n\\nSome operating systems, such as Multics, Unix-like systems, \\nCP/M, DOS, the classic Mac OS, and Windows, \\nstore text files as a sequence of bytes, \\nwith an end-of-line delimiter at the end of each line. \\nOther operating systems, such as OpenVMS and OS/360 \\nand its successors, have record-oriented filesystems, \\nin which text files are stored as a sequence either of \\nfixed-length records or of variable-length records with a \\nrecord-length value in the record header.\\n\\n\"Text file\" refers to a type of container, \\nwhile plain text refers to a type of content.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech = \"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech=f.read()\n",
    "\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='A text file (sometimes spelled textfile; \n",
      "an old alternative name is flat file) is a kind of computer file \n",
      "that is structured as a sequence of lines of electronic text.'\n",
      "page_content='A text file exists stored as data within a computer file system.'\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "text = text_splitter.create_documents([speech])\n",
    "print(text[0])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='A text file (sometimes spelled textfile; \\nan old alternative name is flat file) is a kind of computer file \\nthat is structured as a sequence of lines of electronic text. \\nA text file exists stored as data within a computer file system.\\n\\nIn operating systems such as CP/M, where the operating system does \\nnot keep track of the file size in bytes, the end of a text file is \\ndenoted by placing one or more special characters, \\nknown as an end-of-file (EOF) marker, as padding after the last \\nline in a text file. In modern operating systems such as DOS, \\nMicrosoft Windows and Unix-like systems, \\ntext files do not contain any special EOF character, \\nbecause file systems on those operating systems \\nkeep track of the file size in bytes.\\n\\nSome operating systems, such as Multics, Unix-like systems, \\nCP/M, DOS, the classic Mac OS, and Windows, \\nstore text files as a sequence of bytes, \\nwith an end-of-line delimiter at the end of each line. \\nOther operating systems, such as OpenVMS and OS/360 \\nand its successors, have record-oriented filesystems, \\nin which text files are stored as a sequence either of \\nfixed-length records or of variable-length records with a \\nrecord-length value in the record header.\\n\\n\"Text file\" refers to a type of container, \\nwhile plain text refers to a type of content.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Text Splitter\n",
    "- Separato = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 235, which is longer than the specified 100\n",
      "Created a chunk of size 499, which is longer than the specified 100\n",
      "Created a chunk of size 468, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='A text file (sometimes spelled textfile; \\nan old alternative name is flat file) is a kind of computer file \\nthat is structured as a sequence of lines of electronic text. \\nA text file exists stored as data within a computer file system.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='In operating systems such as CP/M, where the operating system does \\nnot keep track of the file size in bytes, the end of a text file is \\ndenoted by placing one or more special characters, \\nknown as an end-of-file (EOF) marker, as padding after the last \\nline in a text file. In modern operating systems such as DOS, \\nMicrosoft Windows and Unix-like systems, \\ntext files do not contain any special EOF character, \\nbecause file systems on those operating systems \\nkeep track of the file size in bytes.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Some operating systems, such as Multics, Unix-like systems, \\nCP/M, DOS, the classic Mac OS, and Windows, \\nstore text files as a sequence of bytes, \\nwith an end-of-line delimiter at the end of each line. \\nOther operating systems, such as OpenVMS and OS/360 \\nand its successors, have record-oriented filesystems, \\nin which text files are stored as a sequence either of \\nfixed-length records or of variable-length records with a \\nrecord-length value in the record header.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='\"Text file\" refers to a type of container, \\nwhile plain text refers to a type of content.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
