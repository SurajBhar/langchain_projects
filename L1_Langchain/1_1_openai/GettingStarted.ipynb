{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started with Lanchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "- Get setup with Lanchain, LangSmith, LangServe and Open AI\n",
    "- Use the most basic and common components of Langchain: prompt templates, models and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "# Langsmith tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to work with OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7f8a3ce94850> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f8a3ce96950> root_client=<openai.OpenAI object at 0x7f8a3a6d00a0> root_async_client=<openai.AsyncOpenAI object at 0x7f8a3ce948b0> model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # To create a chatbot\n",
    "# Create LLM\n",
    "# llm = ChatOpenAI(api_key=\"\") #Environment variable already set\n",
    "llm = ChatOpenAI()\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Famous Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7f8a3ceb1b10> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f8a3ceb2770> root_client=<openai.OpenAI object at 0x7f8a3ce97460> root_async_client=<openai.AsyncOpenAI object at 0x7f8a3ceb1ab0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='RAPTOR RAG typically refers to a type of event or theme day in schools or organizations where participants dress up as birds of prey, specifically raptors, or engage in activities related to these birds. RAG often stands for \"Raising and Giving,\" a fundraising initiative, so RAPTOR RAG could be a themed fundraising event involving costumes, educational activities, or community engagement centered around raptors. However, without more specific context, it\\'s difficult to provide a precise definition, as the term could be used in different ways by various groups. If you have a specific context or organization in mind, please provide more details for a more accurate explanation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 14, 'total_tokens': 144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9d50cd990b', 'finish_reason': 'stop', 'logprobs': None} id='run-07a92f33-bcd3-48a4-8fe7-0844e60818c4-0' usage_metadata={'input_tokens': 14, 'output_tokens': 130, 'total_tokens': 144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Get a response from LLM\n",
    "result = llm.invoke(\"What is RAPTOR RAG?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI engineer. Provide me answer for my questions.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Langchain Supports a variety of LLM Template prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI engineer. Provide me answer for my questions.\"), # Behave in a certain manner\n",
    "        (\"user\", \"{input}\") # User will provide soe input\n",
    "    ]\n",
    ")\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First the input will go through prompt and then through llm creating a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a tool developed by LangChain to improve the development, testing, and monitoring of applications that utilize large language models (LLMs). It provides a suite of features designed to enhance the efficiency and effectiveness of working with LLMs, including:\\n\\n1. **Testing and Experimentation**: Langsmith allows developers to run experiments with different prompts and model configurations to identify which setups work best for their applications. This helps in optimizing performance and fine-tuning models for specific tasks.\\n\\n2. **Monitoring and Evaluation**: It offers tools to monitor the performance of language models in real-time, providing insights into metrics like accuracy, latency, and cost. This is crucial for maintaining the quality and efficiency of LLM-powered applications.\\n\\n3. **Debugging and Troubleshooting**: Langsmith aids in debugging by providing detailed logs and diagnostics, helping developers quickly identify and fix issues that arise during the use of language models.\\n\\n4. **Collaboration and Sharing**: The platform supports collaboration among team members by allowing them to share experiments, results, and configurations, facilitating a more cohesive and informed development process.\\n\\n5. **Integration**: Langsmith is designed to integrate seamlessly with existing workflows and tools, making it easier for developers to incorporate it into their projects without significant disruptions.\\n\\nOverall, Langsmith serves as a comprehensive platform for enhancing the development lifecycle of applications using large language models, making it easier to build, deploy, and maintain high-quality AI-driven solutions.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 292, 'prompt_tokens': 33, 'total_tokens': 325, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_c7ca0ebaca', 'finish_reason': 'stop', 'logprobs': None} id='run-47a9e3f2-6879-43de-b6f5-433a4a2bd680-0' usage_metadata={'input_tokens': 33, 'output_tokens': 292, 'total_tokens': 325, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "response = chain.invoke({\"input\":\"What is the use of Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"father of cars\" is often considered to be Karl Benz, who is credited with inventing the first true automobile, the Benz Patent-Motorwagen, in 1885/1886.\n",
      "\n",
      "The \"father of deep learning\" is a title commonly attributed to multiple pioneers, but Geoffrey Hinton, Yann LeCun, and Yoshua Bengio are often recognized as key figures who have significantly contributed to the development and popularization of deep learning techniques.\n",
      "\n",
      "As for self-driving cars, the development of autonomous vehicles has been a collaborative effort over many years, with contributions from numerous researchers and companies. However, the DARPA Grand Challenge in the early 2000s was a significant milestone that spurred advancements in self-driving technology. Companies like Google (now Waymo) further accelerated the development and commercialization of self-driving cars. It is difficult to attribute the invention of self-driving cars to a single individual, as it has been a collaborative and evolutionary process involving many engineers and researchers.\n"
     ]
    }
   ],
   "source": [
    "# String Output Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Display my entire output\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "response = chain.invoke({\"input\":\"Who is the father of Cars and Deep Learning? Who invented Self-Driving Cars?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
