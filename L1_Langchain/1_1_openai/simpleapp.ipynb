{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gen AI App using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "# Langsmith tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Web Page data:\n",
    "- https://python.langchain.com/v0.1/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x7fe7bf97d690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.1/docs/use_cases/question_answering/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nQ&A with RAG | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentA newer LangChain version is out! Check out the latest version.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGQuickstartAdd chat historyStreamingReturning sourcesCitationsMoreExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraph🦜️🏓 LangServeSecurityThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).Use casesQ&A with RAGOn this pageQ&A with RAGOverview\\u200bOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.What is RAG?\\u200bRAG is a technique for augmenting LLM knowledge with additional data.LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally. Note: Here we focus on Q&A for unstructured data. Two RAG use cases which we cover elsewhere are:Q&A over SQL dataQ&A over code (e.g., Python)RAG Architecture\\u200bA typical RAG application has two main components:Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.The most common full sequence from raw data to answer looks like:Indexing\\u200bLoad: First we need to load our data. This is done with DocumentLoaders.Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.Retrieval and generation\\u200bRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved dataTable of contents\\u200bQuickstart: We recommend starting here. Many of the following guides assume you fully understand the architecture shown in the Quickstart.Returning sources: How to return the source documents used in a particular generation.Streaming: How to stream final answers as well as intermediate steps.Adding chat history: How to add chat history to a Q&A app.Hybrid search: How to do hybrid search.Per-user retrieval: How to do retrieval when each user has their own private data.Using agents: How to use agents for Q&A.Using local models: How to use local models for Q&A.Help us out by providing feedback on this documentation page:PreviousUse casesNextQuickstartOverviewWhat is RAG?RAG ArchitectureTable of contentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Architecture\n",
    "- A typical RAG application has two main components:\n",
    "    - Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "    - Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "- The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "## Indexing\n",
    " - Load: First we need to load our data. This is done with DocumentLoaders.\n",
    " - Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    " - Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "## Retrieval and generation\n",
    " - Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    " - Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk_size > Chunk_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content='Q&A with RAG | 🦜️🔗 LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content='Skip to main contentA newer LangChain version is out! Check out the latest version.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGQuickstartAdd chat historyStreamingReturning sourcesCitationsMoreExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraph🦜️🏓 LangServeSecurityThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).Use casesQ&A with RAGOn this pageQ&A with RAGOverview\\u200bOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content=\"(Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.What is RAG?\\u200bRAG is a technique for augmenting LLM knowledge with additional data.LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally. Note: Here we focus on Q&A for unstructured data. Two RAG use cases which we cover elsewhere are:Q&A over SQL dataQ&A\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content=\"over SQL dataQ&A over code (e.g., Python)RAG Architecture\\u200bA typical RAG application has two main components:Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.The most common full sequence from raw data to answer looks like:Indexing\\u200bLoad: First we need to load our data. This is done with DocumentLoaders.Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.Retrieval and generation\\u200bRetrieve: Given a user input, relevant splits are retrieved from\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content='are retrieved from storage using a Retriever.Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved dataTable of contents\\u200bQuickstart: We recommend starting here. Many of the following guides assume you fully understand the architecture shown in the Quickstart.Returning sources: How to return the source documents used in a particular generation.Streaming: How to stream final answers as well as intermediate steps.Adding chat history: How to add chat history to a Q&A app.Hybrid search: How to do hybrid search.Per-user retrieval: How to do retrieval when each user has their own private data.Using agents: How to use agents for Q&A.Using local models: How to use local models for Q&A.Help us out by providing feedback on this documentation page:PreviousUse casesNextQuickstartOverviewWhat is RAG?RAG ArchitectureTable of contentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7fe7c17f7df0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search\n",
    "- Query from a Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.What is RAG?\\u200bRAG is a technique for augmenting LLM knowledge with additional data.LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally. Note: Here we focus on Q&A for unstructured data. Two RAG use cases which we cover elsewhere are:Q&A over SQL dataQ&A\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is RAG?\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate your LLm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Chain | Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fe7c69ae950>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fe7c69f4400>, root_client=<openai.OpenAI object at 0x7fe7c61a47c0>, root_async_client=<openai.AsyncOpenAI object at 0x7fe7c69ae1a0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, please provide the question you would like answered based on the provided context.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"What are two main components of RAG application.\",\n",
    "    \"context\": [Document(page_content=\"A typical RAG application has two main components:Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\")]\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But, we should get the documents from the Retriever. \n",
    "- Retriever can dynamically select the most relevant documents from the vector store and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input => Retriever => Vectorstoredb => Relevant Docs\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain = create_retrieval_chain(retriever,document_chain) # Info from Vectore sore + Context Information from document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fe7c17f7df0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following question based on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fe7c69ae950>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fe7c69f4400>, root_client=<openai.OpenAI object at 0x7fe7c61a47c0>, root_async_client=<openai.AsyncOpenAI object at 0x7fe7c69ae1a0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, Retrieval Augmented Generation (RAG) is a technique used to enhance the capabilities of language models (LLMs) by supplementing their knowledge with additional data. While LLMs possess broad reasoning abilities, their knowledge is confined to the public data available up to the point of their training. RAG is particularly useful for applications that require reasoning over private data or data introduced after the model's training cutoff date.\\n\\nThe RAG process involves two main components:\\n\\n1. **Indexing**:\\n   - **Load**: The initial step is loading data through DocumentLoaders.\\n   - **Split**: Large documents are broken into smaller chunks using text splitters. This facilitates easier searching and ensures the data fits within the model's context window.\\n   - **Store**: These chunks are stored in a searchable format, often using a VectorStore and Embeddings model.\\n\\n2. **Retrieval and Generation**:\\n   - **Retrieve**: When a user inputs a query, relevant data chunks are retrieved from storage.\\n   - **Generate**: A language model, such as a ChatModel or LLM, generates an answer by incorporating the user query and the retrieved data into a prompt.\\n\\nRAG is particularly beneficial for Q&A applications, allowing them to provide precise answers based on both public and private or recent data.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retriever_chain.invoke({\"input\":\"What are two main components of RAG application?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content=\"over SQL dataQ&A over code (e.g., Python)RAG Architecture\\u200bA typical RAG application has two main components:Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.The most common full sequence from raw data to answer looks like:Indexing\\u200bLoad: First we need to load our data. This is done with DocumentLoaders.Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.Retrieval and generation\\u200bRetrieve: Given a user input, relevant splits are retrieved from\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content=\"(Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.What is RAG?\\u200bRAG is a technique for augmenting LLM knowledge with additional data.LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally. Note: Here we focus on Q&A for unstructured data. Two RAG use cases which we cover elsewhere are:Q&A over SQL dataQ&A\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content='are retrieved from storage using a Retriever.Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved dataTable of contents\\u200bQuickstart: We recommend starting here. Many of the following guides assume you fully understand the architecture shown in the Quickstart.Returning sources: How to return the source documents used in a particular generation.Streaming: How to stream final answers as well as intermediate steps.Adding chat history: How to add chat history to a Q&A app.Hybrid search: How to do hybrid search.Per-user retrieval: How to do retrieval when each user has their own private data.Using agents: How to use agents for Q&A.Using local models: How to use local models for Q&A.Help us out by providing feedback on this documentation page:PreviousUse casesNextQuickstartOverviewWhat is RAG?RAG ArchitectureTable of contentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/', 'title': 'Q&A with RAG | 🦜️🔗 LangChain', 'description': 'Overview', 'language': 'en'}, page_content='Q&A with RAG | 🦜️🔗 LangChain')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
